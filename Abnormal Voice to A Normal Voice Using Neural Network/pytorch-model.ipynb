{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchaudio\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2023-04-22T15:36:09.735387Z","iopub.execute_input":"2023-04-22T15:36:09.735870Z","iopub.status.idle":"2023-04-22T15:36:12.109130Z","shell.execute_reply.started":"2023-04-22T15:36:09.735823Z","shell.execute_reply":"2023-04-22T15:36:12.107903Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def avg_wer(wer_scores, combined_ref_len):\n    return float(sum(wer_scores)) / float(combined_ref_len)\n\n\ndef _levenshtein_distance(ref, hyp):\n    \"\"\"Levenshtein distance is a string metric for measuring the difference\n    between two sequences. Informally, the levenshtein disctance is defined as\n    the minimum number of single-character edits (substitutions, insertions or\n    deletions) required to change one word into the other. We can naturally\n    extend the edits to word level when calculate levenshtein disctance for\n    two sentences.\n    \"\"\"\n    m = len(ref)\n    n = len(hyp)\n\n    # special case\n    if ref == hyp:\n        return 0\n    if m == 0:\n        return n\n    if n == 0:\n        return m\n\n    if m < n:\n        ref, hyp = hyp, ref\n        m, n = n, m\n\n    # use O(min(m, n)) space\n    distance = np.zeros((2, n + 1), dtype=np.int32)\n\n    # initialize distance matrix\n    for j in range(0,n + 1):\n        distance[0][j] = j\n\n    # calculate levenshtein distance\n    for i in range(1, m + 1):\n        prev_row_idx = (i - 1) % 2\n        cur_row_idx = i % 2\n        distance[cur_row_idx][0] = i\n        for j in range(1, n + 1):\n            if ref[i - 1] == hyp[j - 1]:\n                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n            else:\n                s_num = distance[prev_row_idx][j - 1] + 1\n                i_num = distance[cur_row_idx][j - 1] + 1\n                d_num = distance[prev_row_idx][j] + 1\n                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n\n    return distance[m % 2][n]\n\n\ndef word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n    \"\"\"Compute the levenshtein distance between reference sequence and\n    hypothesis sequence in word-level.\n    :param reference: The reference sentence.\n    :type reference: basestring\n    :param hypothesis: The hypothesis sentence.\n    :type hypothesis: basestring\n    :param ignore_case: Whether case-sensitive or not.\n    :type ignore_case: bool\n    :param delimiter: Delimiter of input sentences.\n    :type delimiter: char\n    :return: Levenshtein distance and word number of reference sentence.\n    :rtype: list\n    \"\"\"\n    if ignore_case == True:\n        reference = reference.lower()\n        hypothesis = hypothesis.lower()\n\n    ref_words = reference.split(delimiter)\n    hyp_words = hypothesis.split(delimiter)\n\n    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n    return float(edit_distance), len(ref_words)\n\n\ndef char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n    \"\"\"Compute the levenshtein distance between reference sequence and\n    hypothesis sequence in char-level.\n    :param reference: The reference sentence.\n    :type reference: basestring\n    :param hypothesis: The hypothesis sentence.\n    :type hypothesis: basestring\n    :param ignore_case: Whether case-sensitive or not.\n    :type ignore_case: bool\n    :param remove_space: Whether remove internal space characters\n    :type remove_space: bool\n    :return: Levenshtein distance and length of reference sentence.\n    :rtype: list\n    \"\"\"\n    if ignore_case == True:\n        reference = reference.lower()\n        hypothesis = hypothesis.lower()\n\n    join_char = ' '\n    if remove_space == True:\n        join_char = ''\n\n    reference = join_char.join(filter(None, reference.split(' ')))\n    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n\n    edit_distance = _levenshtein_distance(reference, hypothesis)\n    return float(edit_distance), len(reference)\n\n\ndef wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n    \"\"\"Calculate word error rate (WER). WER compares reference text and\n    hypothesis text in word-level. WER is defined as:\n    .. math::\n        WER = (Sw + Dw + Iw) / Nw\n    where\n    .. code-block:: text\n        Sw is the number of words subsituted,\n        Dw is the number of words deleted,\n        Iw is the number of words inserted,\n        Nw is the number of words in the reference\n    We can use levenshtein distance to calculate WER. Please draw an attention\n    that empty items will be removed when splitting sentences by delimiter.\n    :param reference: The reference sentence.\n    :type reference: basestring\n    :param hypothesis: The hypothesis sentence.\n    :type hypothesis: basestring\n    :param ignore_case: Whether case-sensitive or not.\n    :type ignore_case: bool\n    :param delimiter: Delimiter of input sentences.\n    :type delimiter: char\n    :return: Word error rate.\n    :rtype: float\n    :raises ValueError: If word number of reference is zero.\n    \"\"\"\n    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n                                         delimiter)\n\n    if ref_len == 0:\n        raise ValueError(\"Reference's word number should be greater than 0.\")\n\n    wer = float(edit_distance) / ref_len\n    return wer\n\n\ndef cer(reference, hypothesis, ignore_case=False, remove_space=False):\n    \"\"\"Calculate charactor error rate (CER). CER compares reference text and\n    hypothesis text in char-level. CER is defined as:\n    .. math::\n        CER = (Sc + Dc + Ic) / Nc\n    where\n    .. code-block:: text\n        Sc is the number of characters substituted,\n        Dc is the number of characters deleted,\n        Ic is the number of characters inserted\n        Nc is the number of characters in the reference\n    We can use levenshtein distance to calculate CER. Chinese input should be\n    encoded to unicode. Please draw an attention that the leading and tailing\n    space characters will be truncated and multiple consecutive space\n    characters in a sentence will be replaced by one space character.\n    :param reference: The reference sentence.\n    :type reference: basestring\n    :param hypothesis: The hypothesis sentence.\n    :type hypothesis: basestring\n    :param ignore_case: Whether case-sensitive or not.\n    :type ignore_case: bool\n    :param remove_space: Whether remove internal space characters\n    :type remove_space: bool\n    :return: Character error rate.\n    :rtype: float\n    :raises ValueError: If the reference length is zero.\n    \"\"\"\n    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n                                         remove_space)\n\n    if ref_len == 0:\n        raise ValueError(\"Length of reference should be greater than 0.\")\n\n    cer = float(edit_distance) / ref_len\n    return cer\n\nclass TextTransform:\n    \"\"\"Maps characters to integers and vice versa\"\"\"\n    def __init__(self):\n        self.char_map = {\"а\": 0, \"б\": 1, \"в\": 2, \"г\": 3, \"д\": 4, \"е\": 5, \"ё\": 6, \"ж\": 7, \"з\": 8, \"и\": 9, \"й\": 10,\n                  \"к\": 11, \"л\": 12, \"м\": 13, \"н\": 14, \"о\": 15, \"п\": 16, \"р\": 17, \"с\": 18, \"т\": 19, \"у\": 20,\n                  \"ф\": 21, \"ч\": 22, \"ц\": 23, \"ш\": 24, \"щ\": 25, \"ъ\": 26, \"ы\": 27, \"ь\": 28, \"э\": 29, \"ю\": 30,\n                  \"я\": 31, \"х\": 32, \" \": 33}\n\n        self.index_map = {}\n        for key, value in self.char_map.items():\n            self.index_map[value] = key\n\n    def text_to_int(self, text):\n        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n        int_sequence = []\n        for c in text:\n            if c != '':\n                ch = self.char_map[c]\n            int_sequence.append(ch)\n        return int_sequence\n\n    def int_to_text(self, labels):\n        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n        string = []\n        for i in labels:\n            string.append(self.index_map[i])\n        return ''.join(string)\n\ntrain_audio_transforms = nn.Sequential(\n    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n    torchaudio.transforms.TimeMasking(time_mask_param=100)\n)\n\nvalid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n\ntext_transform = TextTransform()\n\ndef data_processing(data, data_type=\"train\"):\n    spectrograms = []\n    labels = []\n    input_lengths = []\n    label_lengths = []\n    for (waveform, utterance) in data:\n        if data_type == 'train':\n            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        elif data_type == 'valid':\n            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        else:\n            raise Exception('data_type should be train or valid')\n        spectrograms.append(spec)\n        label = torch.Tensor(text_transform.text_to_int(utterance))\n        labels.append(label)\n        input_lengths.append(spec.shape[0]//4)\n        label_lengths.append(len(label))\n    \n    spectrograms1 = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n            \n    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n\n    return spectrograms1, labels, input_lengths, label_lengths\n\n\ndef GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n    arg_maxes = torch.argmax(output, dim=2)\n    decodes = []\n    targets = []\n    for i, args in enumerate(arg_maxes):\n        decode = []\n        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n        for j, index in enumerate(args):\n            if index != blank_label:\n                if collapse_repeated and j != 0 and index == args[j -1]:\n                    continue\n                decode.append(index.item())\n        decodes.append(text_transform.int_to_text(decode))\n    return decodes, targets","metadata":{"execution":{"iopub.status.busy":"2023-04-22T16:03:36.787732Z","iopub.execute_input":"2023-04-22T16:03:36.788130Z","iopub.status.idle":"2023-04-22T16:03:36.823871Z","shell.execute_reply.started":"2023-04-22T16:03:36.788097Z","shell.execute_reply":"2023-04-22T16:03:36.822715Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"class CNNLayerNorm(nn.Module):\n    \"\"\"Layer normalization built for cnns input\"\"\"\n    def __init__(self, n_feats):\n        super(CNNLayerNorm, self).__init__()\n        self.layer_norm = nn.LayerNorm(n_feats)\n\n    def forward(self, x):\n        # x (batch, channel, feature, time)\n        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n        x = self.layer_norm(x)\n        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n\n\nclass ResidualCNN(nn.Module):\n    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n        except with layer norm instead of batch norm\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n        super(ResidualCNN, self).__init__()\n\n        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.layer_norm1 = CNNLayerNorm(n_feats)\n        self.layer_norm2 = CNNLayerNorm(n_feats)\n\n    def forward(self, x):\n        residual = x  # (batch, channel, feature, time)\n        x = self.layer_norm1(x)\n        x = F.gelu(x)\n        x = self.dropout1(x)\n        x = self.cnn1(x)\n        x = self.layer_norm2(x)\n        x = F.gelu(x)\n        x = self.dropout2(x)\n        x = self.cnn2(x)\n        x += residual\n        return x # (batch, channel, feature, time)\n\n\nclass BidirectionalGRU(nn.Module):\n\n    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n        super(BidirectionalGRU, self).__init__()\n\n        self.BiGRU = nn.GRU(\n            input_size=rnn_dim, hidden_size=hidden_size,\n            num_layers=1, batch_first=batch_first, bidirectional=True)\n        self.layer_norm = nn.LayerNorm(rnn_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.layer_norm(x)\n        x = F.gelu(x)\n        x, _ = self.BiGRU(x)\n        x = self.dropout(x)\n        return x\n\n\nclass SpeechRecognitionModel(nn.Module):\n    \n    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n        super(SpeechRecognitionModel, self).__init__()\n        n_feats = n_feats//2\n        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n\n        # n residual cnn layers with filter size of 32\n        self.rescnn_layers = nn.Sequential(*[\n            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n            for _ in range(n_cnn_layers)\n        ])\n        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n        self.birnn_layers = nn.Sequential(*[\n            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n            for i in range(n_rnn_layers)\n        ])\n        self.classifier = nn.Sequential(\n            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(rnn_dim, n_class)\n        )\n\n    def forward(self, x):\n        x = self.cnn(x)\n        x = self.rescnn_layers(x)\n        sizes = x.size()\n        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n        x = x.transpose(1, 2) # (batch, time, feature)\n        x = self.fully_connected(x)\n        x = self.birnn_layers(x)\n        x = self.classifier(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-04-22T16:01:59.318569Z","iopub.execute_input":"2023-04-22T16:01:59.318957Z","iopub.status.idle":"2023-04-22T16:01:59.337059Z","shell.execute_reply.started":"2023-04-22T16:01:59.318898Z","shell.execute_reply":"2023-04-22T16:01:59.335966Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport librosa\n\nfile = pd.read_excel('/kaggle/input/rus-speech/russian_speech.xlsx')\ny = [sentence for sentence in file['Русская речь']]\n\ndir_name = \"/kaggle/input/upd-speech/abnormal_voice/\"\nfiles_in_dir = os.listdir(dir_name)\n\nX = []\ni = 1\n\nfor e in range(1, 1001):\n    file_name = f'{e}.wav'\n    sampl = librosa.load(dir_name + file_name, sr=16000)[0]\n    sampl = sampl[np.newaxis, :]\n    X.append(torch.Tensor(sampl))\n    if i % 100 == 0:\n        print(i)\n    i += 1","metadata":{"execution":{"iopub.status.busy":"2023-04-22T16:02:00.230458Z","iopub.execute_input":"2023-04-22T16:02:00.231377Z","iopub.status.idle":"2023-04-22T16:02:02.073930Z","shell.execute_reply.started":"2023-04-22T16:02:00.231326Z","shell.execute_reply":"2023-04-22T16:02:02.072790Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n","output_type":"stream"}]},{"cell_type":"code","source":"X[0].shape","metadata":{"execution":{"iopub.status.busy":"2023-04-22T16:02:02.075903Z","iopub.execute_input":"2023-04-22T16:02:02.076285Z","iopub.status.idle":"2023-04-22T16:02:02.085788Z","shell.execute_reply.started":"2023-04-22T16:02:02.076249Z","shell.execute_reply":"2023-04-22T16:02:02.084448Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 79872])"},"metadata":{}}]},{"cell_type":"code","source":"char_map = {\"а\": 0, \"б\": 1, \"в\": 2, \"г\": 3, \"д\": 4, \"е\": 5, \"ё\": 6, \"ж\": 7, \"з\": 8, \"и\": 9, \"й\": 10,\n            \"к\": 11, \"л\": 12, \"м\": 13, \"н\": 14, \"о\": 15, \"п\": 16, \"р\": 17, \"с\": 18, \"т\": 19, \"у\": 20,\n            \"ф\": 21, \"ч\": 22, \"ц\": 23, \"ш\": 24, \"щ\": 25, \"ъ\": 26, \"ы\": 27, \"ь\": 28, \"э\": 29, \"ю\": 30,\n            \"я\": 31, \"х\": 32, \" \": 33}\n\ndef remove_characters(sentence):\n    sentence = sentence.lower()\n    sentence = ''.join(filter(lambda x: x in char_map, sentence))\n    return sentence\n\ny = list(map(remove_characters, y))","metadata":{"execution":{"iopub.status.busy":"2023-04-22T16:02:03.400961Z","iopub.execute_input":"2023-04-22T16:02:03.401883Z","iopub.status.idle":"2023-04-22T16:02:03.416583Z","shell.execute_reply.started":"2023-04-22T16:02:03.401832Z","shell.execute_reply":"2023-04-22T16:02:03.415481Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T16:02:03.579054Z","iopub.execute_input":"2023-04-22T16:02:03.579569Z","iopub.status.idle":"2023-04-22T16:02:03.586845Z","shell.execute_reply.started":"2023-04-22T16:02:03.579538Z","shell.execute_reply":"2023-04-22T16:02:03.585681Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass AudioDataset(Dataset):\n    def __init__(self, audio_list, text_list):\n        self.audio_list = audio_list\n        self.text_list = text_list\n        \n    def __len__(self):\n        return len(self.text_list)\n    \n    def __getitem__(self, index):\n        audio = self.audio_list[index]\n        text = self.text_list[index]\n        return audio, text","metadata":{"execution":{"iopub.status.busy":"2023-04-22T16:02:03.742203Z","iopub.execute_input":"2023-04-22T16:02:03.742887Z","iopub.status.idle":"2023-04-22T16:02:03.749657Z","shell.execute_reply.started":"2023-04-22T16:02:03.742853Z","shell.execute_reply":"2023-04-22T16:02:03.748380Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"class SpeechRecognitionModel1(nn.Module):\n    def __init__(self, num_classes):\n        super(SpeechRecognitionModel1, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n            nn.BatchNorm2d(64),\n            nn.GELU(),\n            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)),\n            nn.Conv2d(64, 128, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n            nn.BatchNorm2d(128),\n            nn.GELU(),\n            nn.Conv2d(128, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n            nn.BatchNorm2d(64),\n            nn.GELU(),\n            nn.Conv2d(64, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n            nn.GELU(),\n            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)),\n        )\n        \n        self.rnn = nn.GRU(input_size=2048, \n                    hidden_size=256, \n                    num_layers=1, \n                    batch_first=True, \n                    bidirectional=True)\n        \n        self.fc = nn.Sequential(\n            nn.Linear(512, 128),\n            nn.GELU(),\n            nn.Linear(128, num_classes),\n        )\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x.permute(0, 3, 1, 2)\n        x = x.view(x.size(0), x.size(1), -1)\n        x, _ = self.rnn(x)\n        x = self.fc(x)\n        x = self.softmax(x)\n        return x\n    \n\"\"\"nn.Linear(512, 128),\n            nn.GELU(),\n            nn.Dropout(0.35),\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-04-22T16:45:21.958436Z","iopub.execute_input":"2023-04-22T16:45:21.958850Z","iopub.status.idle":"2023-04-22T16:45:21.978050Z","shell.execute_reply.started":"2023-04-22T16:45:21.958815Z","shell.execute_reply":"2023-04-22T16:45:21.976009Z"},"trusted":true},"execution_count":95,"outputs":[{"execution_count":95,"output_type":"execute_result","data":{"text/plain":"'nn.Linear(512, 128),\\n            nn.GELU(),\\n            nn.Dropout(0.35),'"},"metadata":{}}]},{"cell_type":"code","source":"class IterMeter(object):\n    \"\"\"keeps track of total iterations\"\"\"\n    def __init__(self):\n        self.val = 0\n\n    def step(self):\n        self.val += 1\n\n    def get(self):\n        return self.val\n\n\ndef train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter):\n    model.train()\n    data_len = len(train_loader.dataset)\n    for batch_idx, _data in enumerate(train_loader):\n        spectrograms, labels, input_lengths, label_lengths = _data \n        spectrograms, labels = spectrograms.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        output = model(spectrograms)  # (batch, time, n_class)\n        output = F.log_softmax(output, dim=2)\n        output = output.transpose(0, 1) # (time, batch, n_class)\n\n        loss = criterion(output, labels, input_lengths, label_lengths)\n        loss.backward()\n\n        optimizer.step()\n        scheduler.step()\n        iter_meter.step()\n        if batch_idx % 10 == 0 or batch_idx == data_len:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(spectrograms), data_len,\n                100. * batch_idx / len(train_loader), loss.item()))\n\n\ndef test(model, device, test_loader, criterion, epoch, iter_meter):\n    print('\\nevaluating...')\n    model.eval()\n    test_loss = 0\n    test_cer, test_wer = [], []\n    with torch.no_grad():\n        for i, _data in enumerate(test_loader):\n            spectrograms, labels, input_lengths, label_lengths = _data \n            spectrograms, labels = spectrograms.to(device), labels.to(device)\n\n            output = model(spectrograms)  # (batch, time, n_class)\n            output = F.log_softmax(output, dim=2)\n            output = output.transpose(0, 1) # (time, batch, n_class)\n\n            loss = criterion(output, labels, input_lengths, label_lengths)\n            test_loss += loss.item() / len(test_loader)\n\n            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n            for j in range(len(decoded_preds)):\n                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n\n\n    avg_cer = sum(test_cer)/len(test_cer)\n    avg_wer = sum(test_wer)/len(test_wer)\n\n    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n\n\ndef main(learning_rate=5e-4, batch_size=20, epochs=10):\n\n    hparams = {\n        \"n_cnn_layers\": 2,\n        \"n_rnn_layers\": 2,\n        \"rnn_dim\": 256,\n        \"n_class\": 34,\n        \"n_feats\": 128,\n        \"stride\":2,\n        \"dropout\": 0.1,\n        \"learning_rate\": learning_rate,\n        \"batch_size\": batch_size,\n        \"epochs\": epochs\n    }\n\n    use_cuda = torch.cuda.is_available()\n    torch.manual_seed(7)\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    train_dataset = AudioDataset(X_train, y_train)\n    test_dataset = AudioDataset(X_test, y_test)\n\n    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n    train_loader = data.DataLoader(dataset=train_dataset,\n                                batch_size=hparams['batch_size'],\n                                shuffle=True,\n                                collate_fn=lambda x: data_processing(x, 'train'),\n                                **kwargs)\n    test_loader = data.DataLoader(dataset=test_dataset,\n                                batch_size=hparams['batch_size'],\n                                shuffle=False,\n                                collate_fn=lambda x: data_processing(x, 'valid'),\n                                **kwargs)\n\n    model = SpeechRecognitionModel1(34).to(device)\n    \"\"\"model = SpeechRecognitionModel(\n        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n        ).to(device)\"\"\"\n\n    print(model)\n    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n\n    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n    criterion = nn.CTCLoss(blank=28).to(device)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n                                            steps_per_epoch=int(len(train_loader)),\n                                            epochs=hparams['epochs'],\n                                            anneal_strategy='linear')\n    \n    iter_meter = IterMeter()\n    for epoch in range(1, epochs + 1):\n        train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\n        test(model, device, test_loader, criterion, epoch, iter_meter)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T16:02:12.699663Z","iopub.execute_input":"2023-04-22T16:02:12.700111Z","iopub.status.idle":"2023-04-22T16:02:12.723713Z","shell.execute_reply.started":"2023-04-22T16:02:12.700076Z","shell.execute_reply":"2023-04-22T16:02:12.722610Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.001\nbatch_size = 10\nepochs = 250\n\nmain(learning_rate, batch_size, epochs)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-04-22T16:45:25.816591Z","iopub.execute_input":"2023-04-22T16:45:25.817296Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"SpeechRecognitionModel1(\n  (conv): Sequential(\n    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): GELU(approximate='none')\n    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): GELU(approximate='none')\n    (7): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (9): GELU(approximate='none')\n    (10): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): GELU(approximate='none')\n    (12): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  )\n  (rnn): GRU(2048, 256, batch_first=True, bidirectional=True)\n  (fc): Sequential(\n    (0): Linear(in_features=512, out_features=128, bias=True)\n    (1): GELU(approximate='none')\n    (2): Linear(in_features=128, out_features=34, bias=True)\n  )\n  (softmax): LogSoftmax(dim=1)\n)\nNum Model Parameters 3797794\nTrain Epoch: 1 [0/900 (0%)]\tLoss: 12.265035\nTrain Epoch: 1 [100/900 (11%)]\tLoss: 10.244518\nTrain Epoch: 1 [200/900 (22%)]\tLoss: 11.821872\nTrain Epoch: 1 [300/900 (33%)]\tLoss: 10.533934\nTrain Epoch: 1 [400/900 (44%)]\tLoss: 10.265449\nTrain Epoch: 1 [500/900 (56%)]\tLoss: 11.833995\nTrain Epoch: 1 [600/900 (67%)]\tLoss: 11.082906\nTrain Epoch: 1 [700/900 (78%)]\tLoss: 10.221188\nTrain Epoch: 1 [800/900 (89%)]\tLoss: 9.064540\n\nevaluating...\nTest set: Average loss: 10.3092, Average CER: 0.969988 Average WER: 1.0000\n\nTrain Epoch: 2 [0/900 (0%)]\tLoss: 10.992615\nTrain Epoch: 2 [100/900 (11%)]\tLoss: 9.801940\nTrain Epoch: 2 [200/900 (22%)]\tLoss: 8.970467\nTrain Epoch: 2 [300/900 (33%)]\tLoss: 9.284268\nTrain Epoch: 2 [400/900 (44%)]\tLoss: 9.647020\nTrain Epoch: 2 [500/900 (56%)]\tLoss: 9.139844\nTrain Epoch: 2 [600/900 (67%)]\tLoss: 9.780423\nTrain Epoch: 2 [700/900 (78%)]\tLoss: 10.659213\nTrain Epoch: 2 [800/900 (89%)]\tLoss: 10.004518\n\nevaluating...\nTest set: Average loss: 6.9685, Average CER: 1.004104 Average WER: 1.0000\n\nTrain Epoch: 3 [0/900 (0%)]\tLoss: 7.725401\nTrain Epoch: 3 [100/900 (11%)]\tLoss: 6.403322\nTrain Epoch: 3 [200/900 (22%)]\tLoss: 8.779566\nTrain Epoch: 3 [300/900 (33%)]\tLoss: 4.879209\nTrain Epoch: 3 [400/900 (44%)]\tLoss: 5.869485\nTrain Epoch: 3 [500/900 (56%)]\tLoss: 4.790915\nTrain Epoch: 3 [600/900 (67%)]\tLoss: 5.565995\nTrain Epoch: 3 [700/900 (78%)]\tLoss: 4.063441\nTrain Epoch: 3 [800/900 (89%)]\tLoss: 4.167707\n\nevaluating...\nTest set: Average loss: 4.4105, Average CER: 0.959700 Average WER: 1.0000\n\nTrain Epoch: 4 [0/900 (0%)]\tLoss: 4.502885\nTrain Epoch: 4 [100/900 (11%)]\tLoss: 3.741678\nTrain Epoch: 4 [200/900 (22%)]\tLoss: 4.036948\nTrain Epoch: 4 [300/900 (33%)]\tLoss: 4.256277\nTrain Epoch: 4 [400/900 (44%)]\tLoss: 3.782094\nTrain Epoch: 4 [500/900 (56%)]\tLoss: 4.006182\nTrain Epoch: 4 [600/900 (67%)]\tLoss: 3.646770\nTrain Epoch: 4 [700/900 (78%)]\tLoss: 3.907647\nTrain Epoch: 4 [800/900 (89%)]\tLoss: 3.554085\n\nevaluating...\nTest set: Average loss: 3.7663, Average CER: 0.982116 Average WER: 1.0752\n\nTrain Epoch: 5 [0/900 (0%)]\tLoss: 3.796948\nTrain Epoch: 5 [100/900 (11%)]\tLoss: 3.826939\nTrain Epoch: 5 [200/900 (22%)]\tLoss: 3.491627\nTrain Epoch: 5 [300/900 (33%)]\tLoss: 3.664868\nTrain Epoch: 5 [400/900 (44%)]\tLoss: 3.596797\nTrain Epoch: 5 [500/900 (56%)]\tLoss: 3.175581\nTrain Epoch: 5 [600/900 (67%)]\tLoss: 3.347099\nTrain Epoch: 5 [700/900 (78%)]\tLoss: 3.341534\nTrain Epoch: 5 [800/900 (89%)]\tLoss: 3.162967\n\nevaluating...\nTest set: Average loss: 3.3431, Average CER: 0.981304 Average WER: 1.0000\n\nTrain Epoch: 6 [0/900 (0%)]\tLoss: 3.611168\nTrain Epoch: 6 [100/900 (11%)]\tLoss: 3.302932\nTrain Epoch: 6 [200/900 (22%)]\tLoss: 3.558591\nTrain Epoch: 6 [300/900 (33%)]\tLoss: 3.485109\nTrain Epoch: 6 [400/900 (44%)]\tLoss: 3.342442\nTrain Epoch: 6 [500/900 (56%)]\tLoss: 3.104277\nTrain Epoch: 6 [600/900 (67%)]\tLoss: 3.690970\nTrain Epoch: 6 [700/900 (78%)]\tLoss: 3.232396\nTrain Epoch: 6 [800/900 (89%)]\tLoss: 3.304963\n\nevaluating...\nTest set: Average loss: 3.2349, Average CER: 0.970931 Average WER: 1.0000\n\nTrain Epoch: 7 [0/900 (0%)]\tLoss: 3.245757\nTrain Epoch: 7 [100/900 (11%)]\tLoss: 3.341102\nTrain Epoch: 7 [200/900 (22%)]\tLoss: 3.068076\nTrain Epoch: 7 [300/900 (33%)]\tLoss: 3.152831\nTrain Epoch: 7 [400/900 (44%)]\tLoss: 3.063792\nTrain Epoch: 7 [500/900 (56%)]\tLoss: 3.081379\nTrain Epoch: 7 [600/900 (67%)]\tLoss: 3.315295\nTrain Epoch: 7 [700/900 (78%)]\tLoss: 3.256223\nTrain Epoch: 7 [800/900 (89%)]\tLoss: 3.284388\n\nevaluating...\nTest set: Average loss: 3.2191, Average CER: 0.964825 Average WER: 1.0000\n\nTrain Epoch: 8 [0/900 (0%)]\tLoss: 3.151129\nTrain Epoch: 8 [100/900 (11%)]\tLoss: 3.090006\nTrain Epoch: 8 [200/900 (22%)]\tLoss: 3.290111\nTrain Epoch: 8 [300/900 (33%)]\tLoss: 3.184175\nTrain Epoch: 8 [400/900 (44%)]\tLoss: 2.901906\nTrain Epoch: 8 [500/900 (56%)]\tLoss: 3.365335\nTrain Epoch: 8 [600/900 (67%)]\tLoss: 3.239249\nTrain Epoch: 8 [700/900 (78%)]\tLoss: 3.215264\nTrain Epoch: 8 [800/900 (89%)]\tLoss: 3.246654\n\nevaluating...\nTest set: Average loss: 3.1832, Average CER: 0.953121 Average WER: 1.0000\n\nTrain Epoch: 9 [0/900 (0%)]\tLoss: 3.129723\nTrain Epoch: 9 [100/900 (11%)]\tLoss: 3.032275\nTrain Epoch: 9 [200/900 (22%)]\tLoss: 2.898948\nTrain Epoch: 9 [300/900 (33%)]\tLoss: 3.056439\nTrain Epoch: 9 [400/900 (44%)]\tLoss: 3.061276\nTrain Epoch: 9 [500/900 (56%)]\tLoss: 3.220918\nTrain Epoch: 9 [600/900 (67%)]\tLoss: 3.261778\nTrain Epoch: 9 [700/900 (78%)]\tLoss: 3.088760\nTrain Epoch: 9 [800/900 (89%)]\tLoss: 3.064336\n\nevaluating...\nTest set: Average loss: 3.1641, Average CER: 0.936408 Average WER: 1.0000\n\nTrain Epoch: 10 [0/900 (0%)]\tLoss: 2.886799\nTrain Epoch: 10 [100/900 (11%)]\tLoss: 3.109587\nTrain Epoch: 10 [200/900 (22%)]\tLoss: 3.115734\nTrain Epoch: 10 [300/900 (33%)]\tLoss: 3.072966\nTrain Epoch: 10 [400/900 (44%)]\tLoss: 3.229708\nTrain Epoch: 10 [500/900 (56%)]\tLoss: 3.140054\nTrain Epoch: 10 [600/900 (67%)]\tLoss: 3.057136\nTrain Epoch: 10 [700/900 (78%)]\tLoss: 3.115516\nTrain Epoch: 10 [800/900 (89%)]\tLoss: 3.136168\n\nevaluating...\nTest set: Average loss: 3.1435, Average CER: 0.951363 Average WER: 1.0000\n\nTrain Epoch: 11 [0/900 (0%)]\tLoss: 3.202473\nTrain Epoch: 11 [100/900 (11%)]\tLoss: 3.190946\nTrain Epoch: 11 [200/900 (22%)]\tLoss: 3.171634\nTrain Epoch: 11 [300/900 (33%)]\tLoss: 2.997189\nTrain Epoch: 11 [400/900 (44%)]\tLoss: 3.066371\nTrain Epoch: 11 [500/900 (56%)]\tLoss: 3.081600\nTrain Epoch: 11 [600/900 (67%)]\tLoss: 3.071889\nTrain Epoch: 11 [700/900 (78%)]\tLoss: 3.178649\nTrain Epoch: 11 [800/900 (89%)]\tLoss: 2.915858\n\nevaluating...\nTest set: Average loss: 3.1374, Average CER: 0.932921 Average WER: 1.0000\n\nTrain Epoch: 12 [0/900 (0%)]\tLoss: 3.018284\nTrain Epoch: 12 [100/900 (11%)]\tLoss: 3.150153\nTrain Epoch: 12 [200/900 (22%)]\tLoss: 2.964222\nTrain Epoch: 12 [300/900 (33%)]\tLoss: 3.098699\nTrain Epoch: 12 [400/900 (44%)]\tLoss: 3.041979\nTrain Epoch: 12 [500/900 (56%)]\tLoss: 3.153472\nTrain Epoch: 12 [600/900 (67%)]\tLoss: 3.020569\nTrain Epoch: 12 [700/900 (78%)]\tLoss: 3.029241\nTrain Epoch: 12 [800/900 (89%)]\tLoss: 3.143673\n\nevaluating...\nTest set: Average loss: 3.1059, Average CER: 0.932819 Average WER: 1.0000\n\nTrain Epoch: 13 [0/900 (0%)]\tLoss: 3.000185\nTrain Epoch: 13 [100/900 (11%)]\tLoss: 3.059890\nTrain Epoch: 13 [200/900 (22%)]\tLoss: 3.053439\nTrain Epoch: 13 [300/900 (33%)]\tLoss: 3.124644\nTrain Epoch: 13 [400/900 (44%)]\tLoss: 3.103388\nTrain Epoch: 13 [500/900 (56%)]\tLoss: 2.948405\nTrain Epoch: 13 [600/900 (67%)]\tLoss: 3.108810\nTrain Epoch: 13 [700/900 (78%)]\tLoss: 3.164974\nTrain Epoch: 13 [800/900 (89%)]\tLoss: 2.902054\n\nevaluating...\nTest set: Average loss: 3.0671, Average CER: 0.930423 Average WER: 1.0000\n\nTrain Epoch: 14 [0/900 (0%)]\tLoss: 3.051732\nTrain Epoch: 14 [100/900 (11%)]\tLoss: 3.096223\nTrain Epoch: 14 [200/900 (22%)]\tLoss: 3.126546\nTrain Epoch: 14 [300/900 (33%)]\tLoss: 3.035084\nTrain Epoch: 14 [400/900 (44%)]\tLoss: 3.018119\nTrain Epoch: 14 [500/900 (56%)]\tLoss: 3.046373\nTrain Epoch: 14 [600/900 (67%)]\tLoss: 2.945817\nTrain Epoch: 14 [700/900 (78%)]\tLoss: 2.778517\nTrain Epoch: 14 [800/900 (89%)]\tLoss: 3.054949\n\nevaluating...\nTest set: Average loss: 3.0965, Average CER: 0.933503 Average WER: 1.0000\n\nTrain Epoch: 15 [0/900 (0%)]\tLoss: 3.221055\nTrain Epoch: 15 [100/900 (11%)]\tLoss: 3.082696\nTrain Epoch: 15 [200/900 (22%)]\tLoss: 2.943979\nTrain Epoch: 15 [300/900 (33%)]\tLoss: 3.126443\nTrain Epoch: 15 [400/900 (44%)]\tLoss: 2.977577\nTrain Epoch: 15 [500/900 (56%)]\tLoss: 3.119735\nTrain Epoch: 15 [600/900 (67%)]\tLoss: 3.021311\nTrain Epoch: 15 [700/900 (78%)]\tLoss: 2.922657\nTrain Epoch: 15 [800/900 (89%)]\tLoss: 2.960299\n\nevaluating...\nTest set: Average loss: 2.9955, Average CER: 0.936374 Average WER: 1.0000\n\nTrain Epoch: 16 [0/900 (0%)]\tLoss: 3.001853\nTrain Epoch: 16 [100/900 (11%)]\tLoss: 2.978594\nTrain Epoch: 16 [200/900 (22%)]\tLoss: 2.941176\nTrain Epoch: 16 [300/900 (33%)]\tLoss: 2.986175\nTrain Epoch: 16 [400/900 (44%)]\tLoss: 2.913072\nTrain Epoch: 16 [500/900 (56%)]\tLoss: 2.993183\nTrain Epoch: 16 [600/900 (67%)]\tLoss: 3.048349\nTrain Epoch: 16 [700/900 (78%)]\tLoss: 2.974499\nTrain Epoch: 16 [800/900 (89%)]\tLoss: 2.876100\n\nevaluating...\nTest set: Average loss: 2.9557, Average CER: 0.933325 Average WER: 1.0000\n\nTrain Epoch: 17 [0/900 (0%)]\tLoss: 2.935081\nTrain Epoch: 17 [100/900 (11%)]\tLoss: 3.090155\nTrain Epoch: 17 [200/900 (22%)]\tLoss: 3.009092\nTrain Epoch: 17 [300/900 (33%)]\tLoss: 2.810921\nTrain Epoch: 17 [400/900 (44%)]\tLoss: 2.906806\nTrain Epoch: 17 [500/900 (56%)]\tLoss: 2.921199\nTrain Epoch: 17 [600/900 (67%)]\tLoss: 3.108717\nTrain Epoch: 17 [700/900 (78%)]\tLoss: 2.822756\nTrain Epoch: 17 [800/900 (89%)]\tLoss: 3.090810\n\nevaluating...\nTest set: Average loss: 2.8918, Average CER: 0.938694 Average WER: 1.0000\n\nTrain Epoch: 18 [0/900 (0%)]\tLoss: 2.910877\nTrain Epoch: 18 [100/900 (11%)]\tLoss: 2.941802\nTrain Epoch: 18 [200/900 (22%)]\tLoss: 3.079222\nTrain Epoch: 18 [300/900 (33%)]\tLoss: 3.003516\nTrain Epoch: 18 [400/900 (44%)]\tLoss: 2.841828\nTrain Epoch: 18 [500/900 (56%)]\tLoss: 3.108747\nTrain Epoch: 18 [600/900 (67%)]\tLoss: 2.763873\nTrain Epoch: 18 [700/900 (78%)]\tLoss: 2.805729\nTrain Epoch: 18 [800/900 (89%)]\tLoss: 2.986123\n\nevaluating...\nTest set: Average loss: 2.8159, Average CER: 0.951750 Average WER: 1.0000\n\nTrain Epoch: 19 [0/900 (0%)]\tLoss: 2.771736\nTrain Epoch: 19 [100/900 (11%)]\tLoss: 2.929471\nTrain Epoch: 19 [200/900 (22%)]\tLoss: 2.804426\nTrain Epoch: 19 [300/900 (33%)]\tLoss: 2.995668\nTrain Epoch: 19 [400/900 (44%)]\tLoss: 2.941821\nTrain Epoch: 19 [500/900 (56%)]\tLoss: 2.815310\nTrain Epoch: 19 [600/900 (67%)]\tLoss: 3.005075\nTrain Epoch: 19 [700/900 (78%)]\tLoss: 2.880907\nTrain Epoch: 19 [800/900 (89%)]\tLoss: 3.004189\n\nevaluating...\nTest set: Average loss: 2.7209, Average CER: 0.942951 Average WER: 1.0000\n\nTrain Epoch: 20 [0/900 (0%)]\tLoss: 2.746949\nTrain Epoch: 20 [100/900 (11%)]\tLoss: 2.725300\nTrain Epoch: 20 [200/900 (22%)]\tLoss: 2.821354\nTrain Epoch: 20 [300/900 (33%)]\tLoss: 2.944612\nTrain Epoch: 20 [400/900 (44%)]\tLoss: 2.761673\nTrain Epoch: 20 [500/900 (56%)]\tLoss: 2.822039\nTrain Epoch: 20 [600/900 (67%)]\tLoss: 2.828492\nTrain Epoch: 20 [700/900 (78%)]\tLoss: 3.048596\nTrain Epoch: 20 [800/900 (89%)]\tLoss: 3.150243\n\nevaluating...\nTest set: Average loss: 2.6097, Average CER: 0.902813 Average WER: 1.0100\n\nTrain Epoch: 21 [0/900 (0%)]\tLoss: 2.608045\nTrain Epoch: 21 [100/900 (11%)]\tLoss: 2.662219\nTrain Epoch: 21 [200/900 (22%)]\tLoss: 3.210115\nTrain Epoch: 21 [300/900 (33%)]\tLoss: 2.813777\nTrain Epoch: 21 [400/900 (44%)]\tLoss: 2.630284\nTrain Epoch: 21 [500/900 (56%)]\tLoss: 2.662005\nTrain Epoch: 21 [600/900 (67%)]\tLoss: 2.432672\nTrain Epoch: 21 [700/900 (78%)]\tLoss: 2.565946\nTrain Epoch: 21 [800/900 (89%)]\tLoss: 2.578023\n\nevaluating...\nTest set: Average loss: 2.5096, Average CER: 0.823909 Average WER: 1.0113\n\nTrain Epoch: 22 [0/900 (0%)]\tLoss: 2.610991\nTrain Epoch: 22 [100/900 (11%)]\tLoss: 2.608662\nTrain Epoch: 22 [200/900 (22%)]\tLoss: 2.467092\nTrain Epoch: 22 [300/900 (33%)]\tLoss: 2.751757\nTrain Epoch: 22 [400/900 (44%)]\tLoss: 2.716553\nTrain Epoch: 22 [500/900 (56%)]\tLoss: 2.436863\nTrain Epoch: 22 [600/900 (67%)]\tLoss: 2.406029\nTrain Epoch: 22 [700/900 (78%)]\tLoss: 2.576356\nTrain Epoch: 22 [800/900 (89%)]\tLoss: 2.667389\n\nevaluating...\nTest set: Average loss: 2.4167, Average CER: 0.768875 Average WER: 1.0125\n\nTrain Epoch: 23 [0/900 (0%)]\tLoss: 2.631514\nTrain Epoch: 23 [100/900 (11%)]\tLoss: 2.565188\nTrain Epoch: 23 [200/900 (22%)]\tLoss: 2.484240\nTrain Epoch: 23 [300/900 (33%)]\tLoss: 2.466734\nTrain Epoch: 23 [400/900 (44%)]\tLoss: 2.482130\nTrain Epoch: 23 [500/900 (56%)]\tLoss: 2.745614\nTrain Epoch: 23 [600/900 (67%)]\tLoss: 2.382291\nTrain Epoch: 23 [700/900 (78%)]\tLoss: 2.286316\nTrain Epoch: 23 [800/900 (89%)]\tLoss: 2.378684\n\nevaluating...\nTest set: Average loss: 2.2553, Average CER: 0.709848 Average WER: 1.0300\n\nTrain Epoch: 24 [0/900 (0%)]\tLoss: 2.291166\nTrain Epoch: 24 [100/900 (11%)]\tLoss: 2.646219\nTrain Epoch: 24 [200/900 (22%)]\tLoss: 2.359732\nTrain Epoch: 24 [300/900 (33%)]\tLoss: 2.377091\nTrain Epoch: 24 [400/900 (44%)]\tLoss: 2.564656\nTrain Epoch: 24 [500/900 (56%)]\tLoss: 2.453891\nTrain Epoch: 24 [600/900 (67%)]\tLoss: 2.346116\nTrain Epoch: 24 [700/900 (78%)]\tLoss: 2.310172\nTrain Epoch: 24 [800/900 (89%)]\tLoss: 2.419559\n\nevaluating...\nTest set: Average loss: 2.1430, Average CER: 0.694197 Average WER: 1.0130\n\nTrain Epoch: 25 [0/900 (0%)]\tLoss: 2.102622\nTrain Epoch: 25 [100/900 (11%)]\tLoss: 2.329289\nTrain Epoch: 25 [200/900 (22%)]\tLoss: 2.364152\nTrain Epoch: 25 [300/900 (33%)]\tLoss: 2.386322\nTrain Epoch: 25 [400/900 (44%)]\tLoss: 2.363431\nTrain Epoch: 25 [500/900 (56%)]\tLoss: 2.588372\nTrain Epoch: 25 [600/900 (67%)]\tLoss: 2.261183\nTrain Epoch: 25 [700/900 (78%)]\tLoss: 2.313560\nTrain Epoch: 25 [800/900 (89%)]\tLoss: 2.066676\n\nevaluating...\nTest set: Average loss: 2.0470, Average CER: 0.678109 Average WER: 1.0130\n\nTrain Epoch: 26 [0/900 (0%)]\tLoss: 2.240425\nTrain Epoch: 26 [100/900 (11%)]\tLoss: 2.529582\nTrain Epoch: 26 [200/900 (22%)]\tLoss: 2.321407\nTrain Epoch: 26 [300/900 (33%)]\tLoss: 2.341588\nTrain Epoch: 26 [400/900 (44%)]\tLoss: 2.349994\nTrain Epoch: 26 [500/900 (56%)]\tLoss: 2.362286\nTrain Epoch: 26 [600/900 (67%)]\tLoss: 2.212876\nTrain Epoch: 26 [700/900 (78%)]\tLoss: 2.405272\nTrain Epoch: 26 [800/900 (89%)]\tLoss: 2.096692\n\nevaluating...\nTest set: Average loss: 1.9823, Average CER: 0.675699 Average WER: 1.0080\n\nTrain Epoch: 27 [0/900 (0%)]\tLoss: 2.353556\nTrain Epoch: 27 [100/900 (11%)]\tLoss: 2.394422\nTrain Epoch: 27 [200/900 (22%)]\tLoss: 2.248969\nTrain Epoch: 27 [300/900 (33%)]\tLoss: 2.485780\nTrain Epoch: 27 [400/900 (44%)]\tLoss: 2.159736\nTrain Epoch: 27 [500/900 (56%)]\tLoss: 2.207346\nTrain Epoch: 27 [600/900 (67%)]\tLoss: 2.240075\nTrain Epoch: 27 [700/900 (78%)]\tLoss: 2.239267\nTrain Epoch: 27 [800/900 (89%)]\tLoss: 1.988915\n\nevaluating...\nTest set: Average loss: 1.9062, Average CER: 0.633392 Average WER: 1.0550\n\nTrain Epoch: 28 [0/900 (0%)]\tLoss: 2.108279\nTrain Epoch: 28 [100/900 (11%)]\tLoss: 2.336700\nTrain Epoch: 28 [200/900 (22%)]\tLoss: 2.216344\nTrain Epoch: 28 [300/900 (33%)]\tLoss: 1.912202\nTrain Epoch: 28 [400/900 (44%)]\tLoss: 2.332389\nTrain Epoch: 28 [500/900 (56%)]\tLoss: 1.828599\nTrain Epoch: 28 [600/900 (67%)]\tLoss: 2.031823\nTrain Epoch: 28 [700/900 (78%)]\tLoss: 2.199732\nTrain Epoch: 28 [800/900 (89%)]\tLoss: 2.040156\n\nevaluating...\nTest set: Average loss: 1.8633, Average CER: 0.635010 Average WER: 1.0658\n\nTrain Epoch: 29 [0/900 (0%)]\tLoss: 2.109662\nTrain Epoch: 29 [100/900 (11%)]\tLoss: 2.093581\nTrain Epoch: 29 [200/900 (22%)]\tLoss: 2.090437\nTrain Epoch: 29 [300/900 (33%)]\tLoss: 2.198176\nTrain Epoch: 29 [400/900 (44%)]\tLoss: 2.083127\nTrain Epoch: 29 [500/900 (56%)]\tLoss: 2.055619\nTrain Epoch: 29 [600/900 (67%)]\tLoss: 1.820006\nTrain Epoch: 29 [700/900 (78%)]\tLoss: 2.288908\nTrain Epoch: 29 [800/900 (89%)]\tLoss: 1.917813\n\nevaluating...\nTest set: Average loss: 1.7580, Average CER: 0.632666 Average WER: 1.0200\n\nTrain Epoch: 30 [0/900 (0%)]\tLoss: 1.967250\nTrain Epoch: 30 [100/900 (11%)]\tLoss: 1.937811\nTrain Epoch: 30 [200/900 (22%)]\tLoss: 2.033335\nTrain Epoch: 30 [300/900 (33%)]\tLoss: 2.237518\nTrain Epoch: 30 [400/900 (44%)]\tLoss: 2.040182\nTrain Epoch: 30 [500/900 (56%)]\tLoss: 2.220152\nTrain Epoch: 30 [600/900 (67%)]\tLoss: 2.462484\nTrain Epoch: 30 [700/900 (78%)]\tLoss: 2.374565\nTrain Epoch: 30 [800/900 (89%)]\tLoss: 1.872848\n\nevaluating...\nTest set: Average loss: 1.7572, Average CER: 0.611215 Average WER: 1.0339\n\nTrain Epoch: 31 [0/900 (0%)]\tLoss: 2.196247\nTrain Epoch: 31 [100/900 (11%)]\tLoss: 2.219199\nTrain Epoch: 31 [200/900 (22%)]\tLoss: 1.588713\nTrain Epoch: 31 [300/900 (33%)]\tLoss: 2.002120\nTrain Epoch: 31 [400/900 (44%)]\tLoss: 2.092585\nTrain Epoch: 31 [500/900 (56%)]\tLoss: 1.917501\nTrain Epoch: 31 [600/900 (67%)]\tLoss: 1.873656\nTrain Epoch: 31 [700/900 (78%)]\tLoss: 1.815979\nTrain Epoch: 31 [800/900 (89%)]\tLoss: 1.858022\n\nevaluating...\nTest set: Average loss: 1.7488, Average CER: 0.590596 Average WER: 1.0759\n\nTrain Epoch: 32 [0/900 (0%)]\tLoss: 2.162105\nTrain Epoch: 32 [100/900 (11%)]\tLoss: 1.783811\nTrain Epoch: 32 [200/900 (22%)]\tLoss: 2.005081\nTrain Epoch: 32 [300/900 (33%)]\tLoss: 1.684342\nTrain Epoch: 32 [400/900 (44%)]\tLoss: 2.160098\nTrain Epoch: 32 [500/900 (56%)]\tLoss: 1.763211\nTrain Epoch: 32 [600/900 (67%)]\tLoss: 1.640723\nTrain Epoch: 32 [700/900 (78%)]\tLoss: 1.671304\nTrain Epoch: 32 [800/900 (89%)]\tLoss: 1.935285\n\nevaluating...\nTest set: Average loss: 1.6240, Average CER: 0.560049 Average WER: 1.0280\n\nTrain Epoch: 33 [0/900 (0%)]\tLoss: 1.910085\nTrain Epoch: 33 [100/900 (11%)]\tLoss: 1.870178\nTrain Epoch: 33 [200/900 (22%)]\tLoss: 2.221246\nTrain Epoch: 33 [300/900 (33%)]\tLoss: 1.993613\nTrain Epoch: 33 [400/900 (44%)]\tLoss: 1.795780\nTrain Epoch: 33 [500/900 (56%)]\tLoss: 1.985579\nTrain Epoch: 33 [600/900 (67%)]\tLoss: 1.790260\nTrain Epoch: 33 [700/900 (78%)]\tLoss: 1.947778\nTrain Epoch: 33 [800/900 (89%)]\tLoss: 1.884674\n\nevaluating...\nTest set: Average loss: 1.5809, Average CER: 0.533708 Average WER: 1.0150\n\nTrain Epoch: 34 [0/900 (0%)]\tLoss: 2.088810\nTrain Epoch: 34 [100/900 (11%)]\tLoss: 1.756961\nTrain Epoch: 34 [200/900 (22%)]\tLoss: 2.067070\nTrain Epoch: 34 [300/900 (33%)]\tLoss: 1.922287\nTrain Epoch: 34 [400/900 (44%)]\tLoss: 1.839278\nTrain Epoch: 34 [500/900 (56%)]\tLoss: 1.675933\nTrain Epoch: 34 [600/900 (67%)]\tLoss: 1.788797\nTrain Epoch: 34 [700/900 (78%)]\tLoss: 1.494745\nTrain Epoch: 34 [800/900 (89%)]\tLoss: 1.831713\n\nevaluating...\nTest set: Average loss: 1.5374, Average CER: 0.499652 Average WER: 1.0266\n\nTrain Epoch: 35 [0/900 (0%)]\tLoss: 1.390717\nTrain Epoch: 35 [100/900 (11%)]\tLoss: 1.730216\nTrain Epoch: 35 [200/900 (22%)]\tLoss: 1.904276\nTrain Epoch: 35 [300/900 (33%)]\tLoss: 1.752010\nTrain Epoch: 35 [400/900 (44%)]\tLoss: 1.592380\nTrain Epoch: 35 [500/900 (56%)]\tLoss: 1.857775\nTrain Epoch: 35 [600/900 (67%)]\tLoss: 1.553799\nTrain Epoch: 35 [700/900 (78%)]\tLoss: 1.963440\nTrain Epoch: 35 [800/900 (89%)]\tLoss: 1.839545\n\nevaluating...\nTest set: Average loss: 1.4923, Average CER: 0.500908 Average WER: 1.0185\n\nTrain Epoch: 36 [0/900 (0%)]\tLoss: 1.916065\nTrain Epoch: 36 [100/900 (11%)]\tLoss: 2.058777\nTrain Epoch: 36 [200/900 (22%)]\tLoss: 1.582274\nTrain Epoch: 36 [300/900 (33%)]\tLoss: 1.933758\nTrain Epoch: 36 [400/900 (44%)]\tLoss: 1.709020\nTrain Epoch: 36 [500/900 (56%)]\tLoss: 1.824163\nTrain Epoch: 36 [600/900 (67%)]\tLoss: 2.319005\nTrain Epoch: 36 [700/900 (78%)]\tLoss: 1.440336\nTrain Epoch: 36 [800/900 (89%)]\tLoss: 1.550661\n\nevaluating...\nTest set: Average loss: 1.4681, Average CER: 0.503204 Average WER: 0.9955\n\nTrain Epoch: 37 [0/900 (0%)]\tLoss: 1.754516\nTrain Epoch: 37 [100/900 (11%)]\tLoss: 1.652270\nTrain Epoch: 37 [200/900 (22%)]\tLoss: 1.864281\nTrain Epoch: 37 [300/900 (33%)]\tLoss: 1.580336\nTrain Epoch: 37 [400/900 (44%)]\tLoss: 1.585723\nTrain Epoch: 37 [500/900 (56%)]\tLoss: 2.072208\nTrain Epoch: 37 [600/900 (67%)]\tLoss: 1.752981\nTrain Epoch: 37 [700/900 (78%)]\tLoss: 1.912268\nTrain Epoch: 37 [800/900 (89%)]\tLoss: 1.698004\n\nevaluating...\nTest set: Average loss: 1.4915, Average CER: 0.483366 Average WER: 1.0130\n\nTrain Epoch: 38 [0/900 (0%)]\tLoss: 2.106539\nTrain Epoch: 38 [100/900 (11%)]\tLoss: 1.317045\nTrain Epoch: 38 [200/900 (22%)]\tLoss: 1.516330\nTrain Epoch: 38 [300/900 (33%)]\tLoss: 1.765861\nTrain Epoch: 38 [400/900 (44%)]\tLoss: 1.801247\nTrain Epoch: 38 [500/900 (56%)]\tLoss: 1.759953\nTrain Epoch: 38 [600/900 (67%)]\tLoss: 1.644308\nTrain Epoch: 38 [700/900 (78%)]\tLoss: 1.643094\nTrain Epoch: 38 [800/900 (89%)]\tLoss: 1.666919\n\nevaluating...\nTest set: Average loss: 1.5315, Average CER: 0.503211 Average WER: 1.0000\n\nTrain Epoch: 39 [0/900 (0%)]\tLoss: 1.688685\nTrain Epoch: 39 [100/900 (11%)]\tLoss: 1.753976\nTrain Epoch: 39 [200/900 (22%)]\tLoss: 1.579998\nTrain Epoch: 39 [300/900 (33%)]\tLoss: 1.417755\nTrain Epoch: 39 [400/900 (44%)]\tLoss: 1.467198\nTrain Epoch: 39 [500/900 (56%)]\tLoss: 1.521881\nTrain Epoch: 39 [600/900 (67%)]\tLoss: 1.503117\nTrain Epoch: 39 [700/900 (78%)]\tLoss: 1.997003\nTrain Epoch: 39 [800/900 (89%)]\tLoss: 1.471203\n\nevaluating...\nTest set: Average loss: 1.3593, Average CER: 0.451164 Average WER: 0.9867\n\nTrain Epoch: 40 [0/900 (0%)]\tLoss: 1.531756\nTrain Epoch: 40 [100/900 (11%)]\tLoss: 1.575238\nTrain Epoch: 40 [200/900 (22%)]\tLoss: 1.824357\nTrain Epoch: 40 [300/900 (33%)]\tLoss: 2.042999\nTrain Epoch: 40 [400/900 (44%)]\tLoss: 1.625505\nTrain Epoch: 40 [500/900 (56%)]\tLoss: 1.724047\nTrain Epoch: 40 [600/900 (67%)]\tLoss: 1.776281\nTrain Epoch: 40 [700/900 (78%)]\tLoss: 1.921481\nTrain Epoch: 40 [800/900 (89%)]\tLoss: 1.624811\n\nevaluating...\nTest set: Average loss: 1.4135, Average CER: 0.476226 Average WER: 1.0043\n\nTrain Epoch: 41 [0/900 (0%)]\tLoss: 1.633929\nTrain Epoch: 41 [100/900 (11%)]\tLoss: 1.710540\nTrain Epoch: 41 [200/900 (22%)]\tLoss: 1.554755\nTrain Epoch: 41 [300/900 (33%)]\tLoss: 1.687256\nTrain Epoch: 41 [400/900 (44%)]\tLoss: 1.759653\nTrain Epoch: 41 [500/900 (56%)]\tLoss: 1.570320\nTrain Epoch: 41 [600/900 (67%)]\tLoss: 1.551298\nTrain Epoch: 41 [700/900 (78%)]\tLoss: 1.688686\nTrain Epoch: 41 [800/900 (89%)]\tLoss: 1.370081\n\nevaluating...\nTest set: Average loss: 1.3228, Average CER: 0.434645 Average WER: 0.9926\n\nTrain Epoch: 42 [0/900 (0%)]\tLoss: 1.317203\nTrain Epoch: 42 [100/900 (11%)]\tLoss: 1.540430\nTrain Epoch: 42 [200/900 (22%)]\tLoss: 1.722732\nTrain Epoch: 42 [300/900 (33%)]\tLoss: 1.405532\nTrain Epoch: 42 [400/900 (44%)]\tLoss: 1.650701\nTrain Epoch: 42 [500/900 (56%)]\tLoss: 1.543269\nTrain Epoch: 42 [600/900 (67%)]\tLoss: 1.765677\nTrain Epoch: 42 [700/900 (78%)]\tLoss: 1.822602\nTrain Epoch: 42 [800/900 (89%)]\tLoss: 1.403384\n\nevaluating...\nTest set: Average loss: 1.3206, Average CER: 0.452359 Average WER: 0.9988\n\nTrain Epoch: 43 [0/900 (0%)]\tLoss: 1.732221\nTrain Epoch: 43 [100/900 (11%)]\tLoss: 1.272180\nTrain Epoch: 43 [200/900 (22%)]\tLoss: 1.502257\nTrain Epoch: 43 [300/900 (33%)]\tLoss: 1.729516\nTrain Epoch: 43 [400/900 (44%)]\tLoss: 1.414172\nTrain Epoch: 43 [500/900 (56%)]\tLoss: 1.547133\nTrain Epoch: 43 [600/900 (67%)]\tLoss: 1.297640\nTrain Epoch: 43 [700/900 (78%)]\tLoss: 1.627046\nTrain Epoch: 43 [800/900 (89%)]\tLoss: 1.948868\n\nevaluating...\nTest set: Average loss: 1.3721, Average CER: 0.468541 Average WER: 1.0211\n\nTrain Epoch: 44 [0/900 (0%)]\tLoss: 1.514993\nTrain Epoch: 44 [100/900 (11%)]\tLoss: 1.863253\nTrain Epoch: 44 [200/900 (22%)]\tLoss: 1.112131\nTrain Epoch: 44 [300/900 (33%)]\tLoss: 1.634426\nTrain Epoch: 44 [400/900 (44%)]\tLoss: 1.881111\nTrain Epoch: 44 [500/900 (56%)]\tLoss: 1.341874\nTrain Epoch: 44 [600/900 (67%)]\tLoss: 1.650381\nTrain Epoch: 44 [700/900 (78%)]\tLoss: 1.546451\nTrain Epoch: 44 [800/900 (89%)]\tLoss: 1.300454\n\nevaluating...\nTest set: Average loss: 1.2585, Average CER: 0.421897 Average WER: 0.9806\n\nTrain Epoch: 45 [0/900 (0%)]\tLoss: 1.107208\nTrain Epoch: 45 [100/900 (11%)]\tLoss: 1.489727\nTrain Epoch: 45 [200/900 (22%)]\tLoss: 1.597253\nTrain Epoch: 45 [300/900 (33%)]\tLoss: 1.486328\nTrain Epoch: 45 [400/900 (44%)]\tLoss: 1.161384\nTrain Epoch: 45 [500/900 (56%)]\tLoss: 1.566893\nTrain Epoch: 45 [600/900 (67%)]\tLoss: 1.460381\nTrain Epoch: 45 [700/900 (78%)]\tLoss: 1.678871\nTrain Epoch: 45 [800/900 (89%)]\tLoss: 1.405288\n\nevaluating...\nTest set: Average loss: 1.2923, Average CER: 0.437674 Average WER: 0.9953\n\nTrain Epoch: 46 [0/900 (0%)]\tLoss: 1.457556\nTrain Epoch: 46 [100/900 (11%)]\tLoss: 1.133775\nTrain Epoch: 46 [200/900 (22%)]\tLoss: 1.439649\nTrain Epoch: 46 [300/900 (33%)]\tLoss: 1.462561\nTrain Epoch: 46 [400/900 (44%)]\tLoss: 1.710590\nTrain Epoch: 46 [500/900 (56%)]\tLoss: 1.076447\nTrain Epoch: 46 [600/900 (67%)]\tLoss: 1.789453\nTrain Epoch: 46 [700/900 (78%)]\tLoss: 1.568888\nTrain Epoch: 46 [800/900 (89%)]\tLoss: 1.865966\n\nevaluating...\nTest set: Average loss: 1.3738, Average CER: 0.480459 Average WER: 0.9981\n\nTrain Epoch: 47 [0/900 (0%)]\tLoss: 1.228687\nTrain Epoch: 47 [100/900 (11%)]\tLoss: 1.307627\nTrain Epoch: 47 [200/900 (22%)]\tLoss: 1.482950\nTrain Epoch: 47 [300/900 (33%)]\tLoss: 1.400796\nTrain Epoch: 47 [400/900 (44%)]\tLoss: 1.347612\nTrain Epoch: 47 [500/900 (56%)]\tLoss: 1.598197\nTrain Epoch: 47 [600/900 (67%)]\tLoss: 1.867463\nTrain Epoch: 47 [700/900 (78%)]\tLoss: 1.331607\nTrain Epoch: 47 [800/900 (89%)]\tLoss: 1.433845\n\nevaluating...\nTest set: Average loss: 1.3904, Average CER: 0.438890 Average WER: 0.9965\n\nTrain Epoch: 48 [0/900 (0%)]\tLoss: 1.475392\nTrain Epoch: 48 [100/900 (11%)]\tLoss: 1.054130\nTrain Epoch: 48 [200/900 (22%)]\tLoss: 1.331267\nTrain Epoch: 48 [300/900 (33%)]\tLoss: 1.375809\nTrain Epoch: 48 [400/900 (44%)]\tLoss: 1.598564\nTrain Epoch: 48 [500/900 (56%)]\tLoss: 2.065625\nTrain Epoch: 48 [600/900 (67%)]\tLoss: 1.232936\nTrain Epoch: 48 [700/900 (78%)]\tLoss: 1.455502\nTrain Epoch: 48 [800/900 (89%)]\tLoss: 1.950575\n\nevaluating...\nTest set: Average loss: 1.2615, Average CER: 0.416165 Average WER: 0.9792\n\nTrain Epoch: 49 [0/900 (0%)]\tLoss: 1.284290\nTrain Epoch: 49 [100/900 (11%)]\tLoss: 1.326158\nTrain Epoch: 49 [200/900 (22%)]\tLoss: 1.482091\nTrain Epoch: 49 [300/900 (33%)]\tLoss: 1.218943\nTrain Epoch: 49 [400/900 (44%)]\tLoss: 1.402127\nTrain Epoch: 49 [500/900 (56%)]\tLoss: 1.397301\nTrain Epoch: 49 [600/900 (67%)]\tLoss: 1.320870\nTrain Epoch: 49 [700/900 (78%)]\tLoss: 1.747689\nTrain Epoch: 49 [800/900 (89%)]\tLoss: 1.222783\n\nevaluating...\nTest set: Average loss: 1.1893, Average CER: 0.413037 Average WER: 0.9694\n\nTrain Epoch: 50 [0/900 (0%)]\tLoss: 1.146069\nTrain Epoch: 50 [100/900 (11%)]\tLoss: 1.553010\nTrain Epoch: 50 [200/900 (22%)]\tLoss: 1.607005\nTrain Epoch: 50 [300/900 (33%)]\tLoss: 1.304571\nTrain Epoch: 50 [400/900 (44%)]\tLoss: 1.298211\nTrain Epoch: 50 [500/900 (56%)]\tLoss: 1.238926\nTrain Epoch: 50 [600/900 (67%)]\tLoss: 1.232546\nTrain Epoch: 50 [700/900 (78%)]\tLoss: 1.316372\nTrain Epoch: 50 [800/900 (89%)]\tLoss: 1.100294\n\nevaluating...\nTest set: Average loss: 1.2635, Average CER: 0.432648 Average WER: 0.9916\n\nTrain Epoch: 51 [0/900 (0%)]\tLoss: 1.484043\nTrain Epoch: 51 [100/900 (11%)]\tLoss: 1.213411\nTrain Epoch: 51 [200/900 (22%)]\tLoss: 1.193165\nTrain Epoch: 51 [300/900 (33%)]\tLoss: 1.470401\nTrain Epoch: 51 [400/900 (44%)]\tLoss: 1.271742\nTrain Epoch: 51 [500/900 (56%)]\tLoss: 1.080245\nTrain Epoch: 51 [600/900 (67%)]\tLoss: 1.188292\nTrain Epoch: 51 [700/900 (78%)]\tLoss: 1.690286\nTrain Epoch: 51 [800/900 (89%)]\tLoss: 1.268836\n\nevaluating...\nTest set: Average loss: 1.2032, Average CER: 0.399772 Average WER: 1.0269\n\nTrain Epoch: 52 [0/900 (0%)]\tLoss: 1.352790\nTrain Epoch: 52 [100/900 (11%)]\tLoss: 0.905117\nTrain Epoch: 52 [200/900 (22%)]\tLoss: 1.534792\nTrain Epoch: 52 [300/900 (33%)]\tLoss: 1.799215\nTrain Epoch: 52 [400/900 (44%)]\tLoss: 1.565438\nTrain Epoch: 52 [500/900 (56%)]\tLoss: 1.297954\nTrain Epoch: 52 [600/900 (67%)]\tLoss: 1.141541\nTrain Epoch: 52 [700/900 (78%)]\tLoss: 1.124598\nTrain Epoch: 52 [800/900 (89%)]\tLoss: 1.008712\n\nevaluating...\nTest set: Average loss: 1.7258, Average CER: 0.470636 Average WER: 1.0562\n\nTrain Epoch: 53 [0/900 (0%)]\tLoss: 1.155520\nTrain Epoch: 53 [100/900 (11%)]\tLoss: 1.110592\nTrain Epoch: 53 [200/900 (22%)]\tLoss: 1.380550\nTrain Epoch: 53 [300/900 (33%)]\tLoss: 1.757936\nTrain Epoch: 53 [400/900 (44%)]\tLoss: 1.301104\nTrain Epoch: 53 [500/900 (56%)]\tLoss: 0.972149\nTrain Epoch: 53 [600/900 (67%)]\tLoss: 1.673833\nTrain Epoch: 53 [700/900 (78%)]\tLoss: 1.519580\nTrain Epoch: 53 [800/900 (89%)]\tLoss: 1.226171\n\nevaluating...\nTest set: Average loss: 1.1810, Average CER: 0.400625 Average WER: 0.9781\n\nTrain Epoch: 54 [0/900 (0%)]\tLoss: 0.763562\nTrain Epoch: 54 [100/900 (11%)]\tLoss: 1.471031\nTrain Epoch: 54 [200/900 (22%)]\tLoss: 1.043593\nTrain Epoch: 54 [300/900 (33%)]\tLoss: 1.532991\nTrain Epoch: 54 [400/900 (44%)]\tLoss: 1.017541\nTrain Epoch: 54 [500/900 (56%)]\tLoss: 1.069088\nTrain Epoch: 54 [600/900 (67%)]\tLoss: 1.533902\nTrain Epoch: 54 [700/900 (78%)]\tLoss: 1.479963\nTrain Epoch: 54 [800/900 (89%)]\tLoss: 1.731120\n\nevaluating...\nTest set: Average loss: 1.1783, Average CER: 0.401097 Average WER: 1.0039\n\nTrain Epoch: 55 [0/900 (0%)]\tLoss: 1.112583\nTrain Epoch: 55 [100/900 (11%)]\tLoss: 1.342447\nTrain Epoch: 55 [200/900 (22%)]\tLoss: 1.115394\nTrain Epoch: 55 [300/900 (33%)]\tLoss: 1.049162\nTrain Epoch: 55 [400/900 (44%)]\tLoss: 1.240727\nTrain Epoch: 55 [500/900 (56%)]\tLoss: 0.977744\nTrain Epoch: 55 [600/900 (67%)]\tLoss: 0.908275\nTrain Epoch: 55 [700/900 (78%)]\tLoss: 1.119112\nTrain Epoch: 55 [800/900 (89%)]\tLoss: 1.548663\n\nevaluating...\nTest set: Average loss: 1.2288, Average CER: 0.407477 Average WER: 0.9972\n\nTrain Epoch: 56 [0/900 (0%)]\tLoss: 1.265797\nTrain Epoch: 56 [100/900 (11%)]\tLoss: 1.249886\nTrain Epoch: 56 [200/900 (22%)]\tLoss: 1.175366\nTrain Epoch: 56 [300/900 (33%)]\tLoss: 1.030118\nTrain Epoch: 56 [400/900 (44%)]\tLoss: 0.952911\nTrain Epoch: 56 [500/900 (56%)]\tLoss: 1.682769\nTrain Epoch: 56 [600/900 (67%)]\tLoss: 1.350101\nTrain Epoch: 56 [700/900 (78%)]\tLoss: 1.101409\nTrain Epoch: 56 [800/900 (89%)]\tLoss: 1.620572\n\nevaluating...\nTest set: Average loss: 1.1921, Average CER: 0.410532 Average WER: 0.9720\n\nTrain Epoch: 57 [0/900 (0%)]\tLoss: 1.265234\nTrain Epoch: 57 [100/900 (11%)]\tLoss: 1.271111\nTrain Epoch: 57 [200/900 (22%)]\tLoss: 1.104763\nTrain Epoch: 57 [300/900 (33%)]\tLoss: 0.954480\nTrain Epoch: 57 [400/900 (44%)]\tLoss: 1.189750\nTrain Epoch: 57 [500/900 (56%)]\tLoss: 1.389891\nTrain Epoch: 57 [600/900 (67%)]\tLoss: 1.188257\nTrain Epoch: 57 [700/900 (78%)]\tLoss: 1.285105\nTrain Epoch: 57 [800/900 (89%)]\tLoss: 1.513577\n\nevaluating...\nTest set: Average loss: 1.2494, Average CER: 0.418624 Average WER: 1.0125\n\nTrain Epoch: 58 [0/900 (0%)]\tLoss: 1.357547\nTrain Epoch: 58 [100/900 (11%)]\tLoss: 1.183542\nTrain Epoch: 58 [200/900 (22%)]\tLoss: 0.951040\nTrain Epoch: 58 [300/900 (33%)]\tLoss: 1.366977\nTrain Epoch: 58 [400/900 (44%)]\tLoss: 1.091257\nTrain Epoch: 58 [500/900 (56%)]\tLoss: 1.382639\nTrain Epoch: 58 [600/900 (67%)]\tLoss: 0.857783\nTrain Epoch: 58 [700/900 (78%)]\tLoss: 0.816988\nTrain Epoch: 58 [800/900 (89%)]\tLoss: 1.251335\n\nevaluating...\nTest set: Average loss: 1.1886, Average CER: 0.390984 Average WER: 0.9894\n\nTrain Epoch: 59 [0/900 (0%)]\tLoss: 1.348723\nTrain Epoch: 59 [100/900 (11%)]\tLoss: 1.371485\nTrain Epoch: 59 [200/900 (22%)]\tLoss: 1.319544\nTrain Epoch: 59 [300/900 (33%)]\tLoss: 0.952240\nTrain Epoch: 59 [400/900 (44%)]\tLoss: 0.897017\nTrain Epoch: 59 [500/900 (56%)]\tLoss: 1.101937\nTrain Epoch: 59 [600/900 (67%)]\tLoss: 0.941533\nTrain Epoch: 59 [700/900 (78%)]\tLoss: 1.106851\nTrain Epoch: 59 [800/900 (89%)]\tLoss: 1.101245\n\nevaluating...\nTest set: Average loss: 1.3822, Average CER: 0.464460 Average WER: 1.0188\n\nTrain Epoch: 60 [0/900 (0%)]\tLoss: 1.624721\nTrain Epoch: 60 [100/900 (11%)]\tLoss: 1.168710\nTrain Epoch: 60 [200/900 (22%)]\tLoss: 0.996585\nTrain Epoch: 60 [300/900 (33%)]\tLoss: 1.037576\nTrain Epoch: 60 [400/900 (44%)]\tLoss: 1.274178\nTrain Epoch: 60 [500/900 (56%)]\tLoss: 1.101150\nTrain Epoch: 60 [600/900 (67%)]\tLoss: 1.068669\nTrain Epoch: 60 [700/900 (78%)]\tLoss: 1.449985\nTrain Epoch: 60 [800/900 (89%)]\tLoss: 0.842761\n\nevaluating...\nTest set: Average loss: 1.9987, Average CER: 0.537014 Average WER: 1.3685\n\nTrain Epoch: 61 [0/900 (0%)]\tLoss: 1.227245\nTrain Epoch: 61 [100/900 (11%)]\tLoss: 1.054043\nTrain Epoch: 61 [200/900 (22%)]\tLoss: 1.204990\nTrain Epoch: 61 [300/900 (33%)]\tLoss: 1.080253\nTrain Epoch: 61 [400/900 (44%)]\tLoss: 1.576092\nTrain Epoch: 61 [500/900 (56%)]\tLoss: 0.892769\nTrain Epoch: 61 [600/900 (67%)]\tLoss: 1.378065\nTrain Epoch: 61 [700/900 (78%)]\tLoss: 1.333869\nTrain Epoch: 61 [800/900 (89%)]\tLoss: 1.550505\n\nevaluating...\nTest set: Average loss: 1.2322, Average CER: 0.405067 Average WER: 1.0139\n\nTrain Epoch: 62 [0/900 (0%)]\tLoss: 1.235753\nTrain Epoch: 62 [100/900 (11%)]\tLoss: 1.069341\nTrain Epoch: 62 [200/900 (22%)]\tLoss: 1.451875\nTrain Epoch: 62 [300/900 (33%)]\tLoss: 2.125185\nTrain Epoch: 62 [400/900 (44%)]\tLoss: 1.346889\nTrain Epoch: 62 [500/900 (56%)]\tLoss: 0.938645\nTrain Epoch: 62 [600/900 (67%)]\tLoss: 1.193987\nTrain Epoch: 62 [700/900 (78%)]\tLoss: 0.965129\nTrain Epoch: 62 [800/900 (89%)]\tLoss: 1.352288\n\nevaluating...\nTest set: Average loss: 1.2340, Average CER: 0.402984 Average WER: 0.9750\n\nTrain Epoch: 63 [0/900 (0%)]\tLoss: 1.083710\nTrain Epoch: 63 [100/900 (11%)]\tLoss: 1.159354\nTrain Epoch: 63 [200/900 (22%)]\tLoss: 1.124886\nTrain Epoch: 63 [300/900 (33%)]\tLoss: 1.134385\nTrain Epoch: 63 [400/900 (44%)]\tLoss: 0.936359\nTrain Epoch: 63 [500/900 (56%)]\tLoss: 1.208503\nTrain Epoch: 63 [600/900 (67%)]\tLoss: 1.605597\nTrain Epoch: 63 [700/900 (78%)]\tLoss: 1.179458\nTrain Epoch: 63 [800/900 (89%)]\tLoss: 1.443682\n\nevaluating...\nTest set: Average loss: 1.3873, Average CER: 0.413364 Average WER: 1.0155\n\nTrain Epoch: 64 [0/900 (0%)]\tLoss: 1.061034\nTrain Epoch: 64 [100/900 (11%)]\tLoss: 1.069804\nTrain Epoch: 64 [200/900 (22%)]\tLoss: 1.001573\nTrain Epoch: 64 [300/900 (33%)]\tLoss: 1.112454\nTrain Epoch: 64 [400/900 (44%)]\tLoss: 1.347985\nTrain Epoch: 64 [500/900 (56%)]\tLoss: 1.173460\nTrain Epoch: 64 [600/900 (67%)]\tLoss: 1.229845\nTrain Epoch: 64 [700/900 (78%)]\tLoss: 1.152527\nTrain Epoch: 64 [800/900 (89%)]\tLoss: 1.271095\n\nevaluating...\nTest set: Average loss: 1.4555, Average CER: 0.453219 Average WER: 1.0936\n\nTrain Epoch: 65 [0/900 (0%)]\tLoss: 1.477939\nTrain Epoch: 65 [100/900 (11%)]\tLoss: 1.126784\nTrain Epoch: 65 [200/900 (22%)]\tLoss: 1.065218\nTrain Epoch: 65 [300/900 (33%)]\tLoss: 0.939635\nTrain Epoch: 65 [400/900 (44%)]\tLoss: 1.223470\nTrain Epoch: 65 [500/900 (56%)]\tLoss: 0.970024\nTrain Epoch: 65 [600/900 (67%)]\tLoss: 0.994590\nTrain Epoch: 65 [700/900 (78%)]\tLoss: 2.165526\nTrain Epoch: 65 [800/900 (89%)]\tLoss: 0.671628\n\nevaluating...\nTest set: Average loss: 1.2167, Average CER: 0.444031 Average WER: 0.9604\n\nTrain Epoch: 66 [0/900 (0%)]\tLoss: 1.473999\nTrain Epoch: 66 [100/900 (11%)]\tLoss: 0.691069\nTrain Epoch: 66 [200/900 (22%)]\tLoss: 1.054640\nTrain Epoch: 66 [300/900 (33%)]\tLoss: 1.174435\nTrain Epoch: 66 [400/900 (44%)]\tLoss: 0.864154\nTrain Epoch: 66 [500/900 (56%)]\tLoss: 0.845725\nTrain Epoch: 66 [600/900 (67%)]\tLoss: 0.782875\nTrain Epoch: 66 [700/900 (78%)]\tLoss: 1.102994\nTrain Epoch: 66 [800/900 (89%)]\tLoss: 1.520479\n\nevaluating...\nTest set: Average loss: 1.2805, Average CER: 0.424832 Average WER: 0.9903\n\nTrain Epoch: 67 [0/900 (0%)]\tLoss: 0.924014\nTrain Epoch: 67 [100/900 (11%)]\tLoss: 0.970627\nTrain Epoch: 67 [200/900 (22%)]\tLoss: 1.104498\nTrain Epoch: 67 [300/900 (33%)]\tLoss: 1.063579\nTrain Epoch: 67 [400/900 (44%)]\tLoss: 1.343070\nTrain Epoch: 67 [500/900 (56%)]\tLoss: 1.106193\nTrain Epoch: 67 [600/900 (67%)]\tLoss: 1.518673\nTrain Epoch: 67 [700/900 (78%)]\tLoss: 1.022026\nTrain Epoch: 67 [800/900 (89%)]\tLoss: 0.814971\n\nevaluating...\nTest set: Average loss: 1.2959, Average CER: 0.408802 Average WER: 0.9839\n\nTrain Epoch: 68 [0/900 (0%)]\tLoss: 1.073084\nTrain Epoch: 68 [100/900 (11%)]\tLoss: 1.282732\nTrain Epoch: 68 [200/900 (22%)]\tLoss: 1.108121\nTrain Epoch: 68 [300/900 (33%)]\tLoss: 1.166482\nTrain Epoch: 68 [400/900 (44%)]\tLoss: 1.044248\nTrain Epoch: 68 [500/900 (56%)]\tLoss: 0.936904\nTrain Epoch: 68 [600/900 (67%)]\tLoss: 1.067589\nTrain Epoch: 68 [700/900 (78%)]\tLoss: 1.086594\nTrain Epoch: 68 [800/900 (89%)]\tLoss: 1.058046\n\nevaluating...\nTest set: Average loss: 5.3496, Average CER: 0.997291 Average WER: 2.8398\n\nTrain Epoch: 69 [0/900 (0%)]\tLoss: 1.747595\nTrain Epoch: 69 [100/900 (11%)]\tLoss: 1.317579\nTrain Epoch: 69 [200/900 (22%)]\tLoss: 0.894942\nTrain Epoch: 69 [300/900 (33%)]\tLoss: 0.936859\nTrain Epoch: 69 [400/900 (44%)]\tLoss: 1.300910\nTrain Epoch: 69 [500/900 (56%)]\tLoss: 1.368961\nTrain Epoch: 69 [600/900 (67%)]\tLoss: 1.218530\nTrain Epoch: 69 [700/900 (78%)]\tLoss: 1.184807\nTrain Epoch: 69 [800/900 (89%)]\tLoss: 0.849977\n\nevaluating...\nTest set: Average loss: 2.4053, Average CER: 0.626513 Average WER: 1.1525\n\nTrain Epoch: 70 [0/900 (0%)]\tLoss: 1.308257\nTrain Epoch: 70 [100/900 (11%)]\tLoss: 1.089220\nTrain Epoch: 70 [200/900 (22%)]\tLoss: 1.047602\nTrain Epoch: 70 [300/900 (33%)]\tLoss: 0.608076\nTrain Epoch: 70 [400/900 (44%)]\tLoss: 1.084463\nTrain Epoch: 70 [500/900 (56%)]\tLoss: 0.658263\nTrain Epoch: 70 [600/900 (67%)]\tLoss: 1.033349\nTrain Epoch: 70 [700/900 (78%)]\tLoss: 1.215292\nTrain Epoch: 70 [800/900 (89%)]\tLoss: 0.882183\n\nevaluating...\nTest set: Average loss: 1.2421, Average CER: 0.399613 Average WER: 0.9779\n\nTrain Epoch: 71 [0/900 (0%)]\tLoss: 0.966343\nTrain Epoch: 71 [100/900 (11%)]\tLoss: 1.323889\nTrain Epoch: 71 [200/900 (22%)]\tLoss: 0.921159\nTrain Epoch: 71 [300/900 (33%)]\tLoss: 0.877191\nTrain Epoch: 71 [400/900 (44%)]\tLoss: 1.131194\nTrain Epoch: 71 [500/900 (56%)]\tLoss: 1.427375\nTrain Epoch: 71 [600/900 (67%)]\tLoss: 0.747873\nTrain Epoch: 71 [700/900 (78%)]\tLoss: 1.445028\nTrain Epoch: 71 [800/900 (89%)]\tLoss: 0.919640\n\nevaluating...\nTest set: Average loss: 3.9702, Average CER: 0.634441 Average WER: 1.4141\n\nTrain Epoch: 72 [0/900 (0%)]\tLoss: 0.601131\nTrain Epoch: 72 [100/900 (11%)]\tLoss: 0.814670\nTrain Epoch: 72 [200/900 (22%)]\tLoss: 0.640469\nTrain Epoch: 72 [300/900 (33%)]\tLoss: 1.429281\nTrain Epoch: 72 [400/900 (44%)]\tLoss: 1.231449\nTrain Epoch: 72 [500/900 (56%)]\tLoss: 0.638213\nTrain Epoch: 72 [600/900 (67%)]\tLoss: 1.004336\nTrain Epoch: 72 [700/900 (78%)]\tLoss: 0.905483\nTrain Epoch: 72 [800/900 (89%)]\tLoss: 1.271759\n\nevaluating...\nTest set: Average loss: 1.5640, Average CER: 0.479393 Average WER: 1.1830\n\nTrain Epoch: 73 [0/900 (0%)]\tLoss: 0.853826\nTrain Epoch: 73 [100/900 (11%)]\tLoss: 0.892940\nTrain Epoch: 73 [200/900 (22%)]\tLoss: 0.811434\nTrain Epoch: 73 [300/900 (33%)]\tLoss: 1.051618\nTrain Epoch: 73 [400/900 (44%)]\tLoss: 1.062401\nTrain Epoch: 73 [500/900 (56%)]\tLoss: 0.723729\nTrain Epoch: 73 [600/900 (67%)]\tLoss: 0.935519\nTrain Epoch: 73 [700/900 (78%)]\tLoss: 0.726007\nTrain Epoch: 73 [800/900 (89%)]\tLoss: 0.854894\n\nevaluating...\nTest set: Average loss: 1.2634, Average CER: 0.453904 Average WER: 0.9832\n\nTrain Epoch: 74 [0/900 (0%)]\tLoss: 0.769540\nTrain Epoch: 74 [100/900 (11%)]\tLoss: 0.874025\nTrain Epoch: 74 [200/900 (22%)]\tLoss: 0.981487\nTrain Epoch: 74 [300/900 (33%)]\tLoss: 1.044871\nTrain Epoch: 74 [400/900 (44%)]\tLoss: 0.864935\nTrain Epoch: 74 [500/900 (56%)]\tLoss: 1.296017\nTrain Epoch: 74 [600/900 (67%)]\tLoss: 0.887107\nTrain Epoch: 74 [700/900 (78%)]\tLoss: 1.015552\nTrain Epoch: 74 [800/900 (89%)]\tLoss: 1.004553\n\nevaluating...\nTest set: Average loss: 1.3110, Average CER: 0.409590 Average WER: 1.0575\n\nTrain Epoch: 75 [0/900 (0%)]\tLoss: 0.860148\nTrain Epoch: 75 [100/900 (11%)]\tLoss: 0.937051\nTrain Epoch: 75 [200/900 (22%)]\tLoss: 1.238938\nTrain Epoch: 75 [300/900 (33%)]\tLoss: 0.781489\nTrain Epoch: 75 [400/900 (44%)]\tLoss: 1.052446\nTrain Epoch: 75 [500/900 (56%)]\tLoss: 0.975196\nTrain Epoch: 75 [600/900 (67%)]\tLoss: 0.998201\nTrain Epoch: 75 [700/900 (78%)]\tLoss: 1.306786\nTrain Epoch: 75 [800/900 (89%)]\tLoss: 1.344807\n\nevaluating...\nTest set: Average loss: 1.4621, Average CER: 0.428101 Average WER: 1.0420\n\nTrain Epoch: 76 [0/900 (0%)]\tLoss: 0.787812\nTrain Epoch: 76 [100/900 (11%)]\tLoss: 0.846726\nTrain Epoch: 76 [200/900 (22%)]\tLoss: 1.260051\nTrain Epoch: 76 [300/900 (33%)]\tLoss: 0.759485\nTrain Epoch: 76 [400/900 (44%)]\tLoss: 1.098270\nTrain Epoch: 76 [500/900 (56%)]\tLoss: 0.979449\nTrain Epoch: 76 [600/900 (67%)]\tLoss: 0.851974\nTrain Epoch: 76 [700/900 (78%)]\tLoss: 0.960798\nTrain Epoch: 76 [800/900 (89%)]\tLoss: 0.883852\n\nevaluating...\nTest set: Average loss: 1.2256, Average CER: 0.415441 Average WER: 0.9908\n\nTrain Epoch: 77 [0/900 (0%)]\tLoss: 1.327292\nTrain Epoch: 77 [100/900 (11%)]\tLoss: 0.785746\nTrain Epoch: 77 [200/900 (22%)]\tLoss: 1.277256\nTrain Epoch: 77 [300/900 (33%)]\tLoss: 0.941661\nTrain Epoch: 77 [400/900 (44%)]\tLoss: 1.048439\nTrain Epoch: 77 [500/900 (56%)]\tLoss: 0.872276\nTrain Epoch: 77 [600/900 (67%)]\tLoss: 1.168471\nTrain Epoch: 77 [700/900 (78%)]\tLoss: 1.093051\nTrain Epoch: 77 [800/900 (89%)]\tLoss: 0.923384\n\nevaluating...\nTest set: Average loss: 1.2454, Average CER: 0.383399 Average WER: 0.9961\n\nTrain Epoch: 78 [0/900 (0%)]\tLoss: 1.233149\nTrain Epoch: 78 [100/900 (11%)]\tLoss: 0.852931\nTrain Epoch: 78 [200/900 (22%)]\tLoss: 1.313658\nTrain Epoch: 78 [300/900 (33%)]\tLoss: 0.899297\nTrain Epoch: 78 [400/900 (44%)]\tLoss: 0.943677\nTrain Epoch: 78 [500/900 (56%)]\tLoss: 0.853710\nTrain Epoch: 78 [600/900 (67%)]\tLoss: 0.669194\nTrain Epoch: 78 [700/900 (78%)]\tLoss: 0.922533\nTrain Epoch: 78 [800/900 (89%)]\tLoss: 1.062617\n\nevaluating...\nTest set: Average loss: 1.2997, Average CER: 0.420496 Average WER: 1.0219\n\nTrain Epoch: 79 [0/900 (0%)]\tLoss: 0.938635\nTrain Epoch: 79 [100/900 (11%)]\tLoss: 1.263950\nTrain Epoch: 79 [200/900 (22%)]\tLoss: 0.701300\nTrain Epoch: 79 [300/900 (33%)]\tLoss: 1.069888\nTrain Epoch: 79 [400/900 (44%)]\tLoss: 0.826848\nTrain Epoch: 79 [500/900 (56%)]\tLoss: 0.954243\nTrain Epoch: 79 [600/900 (67%)]\tLoss: 0.879752\nTrain Epoch: 79 [700/900 (78%)]\tLoss: 1.329785\nTrain Epoch: 79 [800/900 (89%)]\tLoss: 0.831389\n\nevaluating...\nTest set: Average loss: 1.2940, Average CER: 0.395561 Average WER: 1.0362\n\nTrain Epoch: 80 [0/900 (0%)]\tLoss: 1.081804\nTrain Epoch: 80 [100/900 (11%)]\tLoss: 0.536700\nTrain Epoch: 80 [200/900 (22%)]\tLoss: 0.653555\nTrain Epoch: 80 [300/900 (33%)]\tLoss: 0.837178\nTrain Epoch: 80 [400/900 (44%)]\tLoss: 0.936910\nTrain Epoch: 80 [500/900 (56%)]\tLoss: 0.907897\nTrain Epoch: 80 [600/900 (67%)]\tLoss: 0.959795\nTrain Epoch: 80 [700/900 (78%)]\tLoss: 0.712738\nTrain Epoch: 80 [800/900 (89%)]\tLoss: 0.848739\n\nevaluating...\nTest set: Average loss: 1.3165, Average CER: 0.402608 Average WER: 0.9938\n\nTrain Epoch: 81 [0/900 (0%)]\tLoss: 1.570715\nTrain Epoch: 81 [100/900 (11%)]\tLoss: 0.571869\nTrain Epoch: 81 [200/900 (22%)]\tLoss: 1.040450\nTrain Epoch: 81 [300/900 (33%)]\tLoss: 0.999265\nTrain Epoch: 81 [400/900 (44%)]\tLoss: 0.557211\nTrain Epoch: 81 [500/900 (56%)]\tLoss: 0.790824\nTrain Epoch: 81 [600/900 (67%)]\tLoss: 1.370510\nTrain Epoch: 81 [700/900 (78%)]\tLoss: 0.757442\nTrain Epoch: 81 [800/900 (89%)]\tLoss: 0.751406\n\nevaluating...\nTest set: Average loss: 1.6370, Average CER: 0.488638 Average WER: 1.1082\n\nTrain Epoch: 82 [0/900 (0%)]\tLoss: 1.153054\nTrain Epoch: 82 [100/900 (11%)]\tLoss: 0.793503\nTrain Epoch: 82 [200/900 (22%)]\tLoss: 0.905900\nTrain Epoch: 82 [300/900 (33%)]\tLoss: 1.121682\nTrain Epoch: 82 [400/900 (44%)]\tLoss: 0.770862\nTrain Epoch: 82 [500/900 (56%)]\tLoss: 0.983387\nTrain Epoch: 82 [600/900 (67%)]\tLoss: 1.339888\nTrain Epoch: 82 [700/900 (78%)]\tLoss: 0.843518\nTrain Epoch: 82 [800/900 (89%)]\tLoss: 0.980535\n\nevaluating...\nTest set: Average loss: 1.3823, Average CER: 0.412513 Average WER: 1.0163\n\nTrain Epoch: 83 [0/900 (0%)]\tLoss: 0.614526\nTrain Epoch: 83 [100/900 (11%)]\tLoss: 0.842972\nTrain Epoch: 83 [200/900 (22%)]\tLoss: 0.799703\nTrain Epoch: 83 [300/900 (33%)]\tLoss: 0.705535\nTrain Epoch: 83 [400/900 (44%)]\tLoss: 1.203232\nTrain Epoch: 83 [500/900 (56%)]\tLoss: 1.122957\nTrain Epoch: 83 [600/900 (67%)]\tLoss: 0.768445\nTrain Epoch: 83 [700/900 (78%)]\tLoss: 0.811372\nTrain Epoch: 83 [800/900 (89%)]\tLoss: 0.772009\n\nevaluating...\nTest set: Average loss: 1.3597, Average CER: 0.410203 Average WER: 1.0746\n\nTrain Epoch: 84 [0/900 (0%)]\tLoss: 0.853496\nTrain Epoch: 84 [100/900 (11%)]\tLoss: 0.737002\nTrain Epoch: 84 [200/900 (22%)]\tLoss: 1.346192\nTrain Epoch: 84 [300/900 (33%)]\tLoss: 0.766521\nTrain Epoch: 84 [400/900 (44%)]\tLoss: 0.894798\nTrain Epoch: 84 [500/900 (56%)]\tLoss: 0.535962\nTrain Epoch: 84 [600/900 (67%)]\tLoss: 0.555185\nTrain Epoch: 84 [700/900 (78%)]\tLoss: 1.024196\nTrain Epoch: 84 [800/900 (89%)]\tLoss: 0.609094\n\nevaluating...\nTest set: Average loss: 1.2982, Average CER: 0.402796 Average WER: 1.0279\n\nTrain Epoch: 85 [0/900 (0%)]\tLoss: 0.825907\nTrain Epoch: 85 [100/900 (11%)]\tLoss: 0.726969\nTrain Epoch: 85 [200/900 (22%)]\tLoss: 0.678564\nTrain Epoch: 85 [300/900 (33%)]\tLoss: 0.978951\nTrain Epoch: 85 [400/900 (44%)]\tLoss: 0.988832\nTrain Epoch: 85 [500/900 (56%)]\tLoss: 0.911962\nTrain Epoch: 85 [600/900 (67%)]\tLoss: 0.851835\nTrain Epoch: 85 [700/900 (78%)]\tLoss: 1.109952\nTrain Epoch: 85 [800/900 (89%)]\tLoss: 1.069236\n\nevaluating...\nTest set: Average loss: 1.3257, Average CER: 0.405531 Average WER: 1.0444\n\nTrain Epoch: 86 [0/900 (0%)]\tLoss: 0.708472\nTrain Epoch: 86 [100/900 (11%)]\tLoss: 0.893804\nTrain Epoch: 86 [200/900 (22%)]\tLoss: 0.531663\nTrain Epoch: 86 [300/900 (33%)]\tLoss: 1.005206\nTrain Epoch: 86 [400/900 (44%)]\tLoss: 0.914171\nTrain Epoch: 86 [500/900 (56%)]\tLoss: 1.248437\nTrain Epoch: 86 [600/900 (67%)]\tLoss: 0.811403\nTrain Epoch: 86 [700/900 (78%)]\tLoss: 0.583125\nTrain Epoch: 86 [800/900 (89%)]\tLoss: 0.744491\n\nevaluating...\nTest set: Average loss: 1.4134, Average CER: 0.413374 Average WER: 1.0070\n\nTrain Epoch: 87 [0/900 (0%)]\tLoss: 1.475010\nTrain Epoch: 87 [100/900 (11%)]\tLoss: 0.564661\nTrain Epoch: 87 [200/900 (22%)]\tLoss: 0.786857\nTrain Epoch: 87 [300/900 (33%)]\tLoss: 0.761147\nTrain Epoch: 87 [400/900 (44%)]\tLoss: 0.808252\nTrain Epoch: 87 [500/900 (56%)]\tLoss: 0.892402\nTrain Epoch: 87 [600/900 (67%)]\tLoss: 0.816011\nTrain Epoch: 87 [700/900 (78%)]\tLoss: 0.947820\nTrain Epoch: 87 [800/900 (89%)]\tLoss: 0.745488\n\nevaluating...\nTest set: Average loss: 2.0399, Average CER: 0.553405 Average WER: 1.1704\n\nTrain Epoch: 88 [0/900 (0%)]\tLoss: 1.044998\nTrain Epoch: 88 [100/900 (11%)]\tLoss: 0.770936\nTrain Epoch: 88 [200/900 (22%)]\tLoss: 1.034652\nTrain Epoch: 88 [300/900 (33%)]\tLoss: 0.793143\nTrain Epoch: 88 [400/900 (44%)]\tLoss: 0.588065\nTrain Epoch: 88 [500/900 (56%)]\tLoss: 1.025825\nTrain Epoch: 88 [600/900 (67%)]\tLoss: 0.909695\nTrain Epoch: 88 [700/900 (78%)]\tLoss: 0.741560\nTrain Epoch: 88 [800/900 (89%)]\tLoss: 1.155248\n\nevaluating...\nTest set: Average loss: 1.3501, Average CER: 0.395062 Average WER: 1.0311\n\nTrain Epoch: 89 [0/900 (0%)]\tLoss: 1.059784\nTrain Epoch: 89 [100/900 (11%)]\tLoss: 1.059235\nTrain Epoch: 89 [200/900 (22%)]\tLoss: 0.753352\nTrain Epoch: 89 [300/900 (33%)]\tLoss: 0.725590\nTrain Epoch: 89 [400/900 (44%)]\tLoss: 0.752449\nTrain Epoch: 89 [500/900 (56%)]\tLoss: 0.663540\nTrain Epoch: 89 [600/900 (67%)]\tLoss: 1.349541\nTrain Epoch: 89 [700/900 (78%)]\tLoss: 0.730145\nTrain Epoch: 89 [800/900 (89%)]\tLoss: 0.836052\n\nevaluating...\nTest set: Average loss: 1.3164, Average CER: 0.393186 Average WER: 1.0269\n\nTrain Epoch: 90 [0/900 (0%)]\tLoss: 0.942578\nTrain Epoch: 90 [100/900 (11%)]\tLoss: 0.930072\nTrain Epoch: 90 [200/900 (22%)]\tLoss: 0.712065\nTrain Epoch: 90 [300/900 (33%)]\tLoss: 1.120054\nTrain Epoch: 90 [400/900 (44%)]\tLoss: 0.721006\nTrain Epoch: 90 [500/900 (56%)]\tLoss: 0.973104\nTrain Epoch: 90 [600/900 (67%)]\tLoss: 0.982894\nTrain Epoch: 90 [700/900 (78%)]\tLoss: 0.708680\nTrain Epoch: 90 [800/900 (89%)]\tLoss: 0.528039\n\nevaluating...\nTest set: Average loss: 1.2619, Average CER: 0.379942 Average WER: 0.9730\n\nTrain Epoch: 91 [0/900 (0%)]\tLoss: 0.456524\nTrain Epoch: 91 [100/900 (11%)]\tLoss: 0.880009\nTrain Epoch: 91 [200/900 (22%)]\tLoss: 0.707780\nTrain Epoch: 91 [300/900 (33%)]\tLoss: 0.717909\nTrain Epoch: 91 [400/900 (44%)]\tLoss: 0.885920\nTrain Epoch: 91 [500/900 (56%)]\tLoss: 0.612270\nTrain Epoch: 91 [600/900 (67%)]\tLoss: 0.606835\nTrain Epoch: 91 [700/900 (78%)]\tLoss: 1.045942\nTrain Epoch: 91 [800/900 (89%)]\tLoss: 0.976008\n\nevaluating...\nTest set: Average loss: 1.3510, Average CER: 0.393884 Average WER: 1.0219\n\nTrain Epoch: 92 [0/900 (0%)]\tLoss: 0.895954\nTrain Epoch: 92 [100/900 (11%)]\tLoss: 0.906852\nTrain Epoch: 92 [200/900 (22%)]\tLoss: 0.567153\nTrain Epoch: 92 [300/900 (33%)]\tLoss: 0.692880\nTrain Epoch: 92 [400/900 (44%)]\tLoss: 0.643882\nTrain Epoch: 92 [500/900 (56%)]\tLoss: 0.904253\nTrain Epoch: 92 [600/900 (67%)]\tLoss: 0.770847\nTrain Epoch: 92 [700/900 (78%)]\tLoss: 0.835325\nTrain Epoch: 92 [800/900 (89%)]\tLoss: 0.712172\n\nevaluating...\nTest set: Average loss: 2.1141, Average CER: 0.554449 Average WER: 1.0754\n\nTrain Epoch: 93 [0/900 (0%)]\tLoss: 0.926638\nTrain Epoch: 93 [100/900 (11%)]\tLoss: 0.880600\nTrain Epoch: 93 [200/900 (22%)]\tLoss: 1.152972\nTrain Epoch: 93 [300/900 (33%)]\tLoss: 0.804346\nTrain Epoch: 93 [400/900 (44%)]\tLoss: 0.628073\nTrain Epoch: 93 [500/900 (56%)]\tLoss: 1.010371\nTrain Epoch: 93 [600/900 (67%)]\tLoss: 0.817418\nTrain Epoch: 93 [700/900 (78%)]\tLoss: 0.959035\nTrain Epoch: 93 [800/900 (89%)]\tLoss: 0.620958\n\nevaluating...\nTest set: Average loss: 1.4197, Average CER: 0.418814 Average WER: 1.0116\n\nTrain Epoch: 94 [0/900 (0%)]\tLoss: 1.059978\nTrain Epoch: 94 [100/900 (11%)]\tLoss: 0.872992\nTrain Epoch: 94 [200/900 (22%)]\tLoss: 0.691430\nTrain Epoch: 94 [300/900 (33%)]\tLoss: 0.892771\nTrain Epoch: 94 [400/900 (44%)]\tLoss: 0.575934\nTrain Epoch: 94 [500/900 (56%)]\tLoss: 0.785601\nTrain Epoch: 94 [600/900 (67%)]\tLoss: 0.556567\nTrain Epoch: 94 [700/900 (78%)]\tLoss: 0.669540\nTrain Epoch: 94 [800/900 (89%)]\tLoss: 0.507939\n\nevaluating...\nTest set: Average loss: 1.3611, Average CER: 0.383793 Average WER: 0.9845\n\nTrain Epoch: 95 [0/900 (0%)]\tLoss: 0.721959\nTrain Epoch: 95 [100/900 (11%)]\tLoss: 0.487509\nTrain Epoch: 95 [200/900 (22%)]\tLoss: 0.760898\nTrain Epoch: 95 [300/900 (33%)]\tLoss: 0.910528\nTrain Epoch: 95 [400/900 (44%)]\tLoss: 0.755159\nTrain Epoch: 95 [500/900 (56%)]\tLoss: 0.387654\nTrain Epoch: 95 [600/900 (67%)]\tLoss: 0.961613\nTrain Epoch: 95 [700/900 (78%)]\tLoss: 1.173818\nTrain Epoch: 95 [800/900 (89%)]\tLoss: 1.012960\n\nevaluating...\nTest set: Average loss: 1.5582, Average CER: 0.481986 Average WER: 1.1001\n\nTrain Epoch: 96 [0/900 (0%)]\tLoss: 0.632735\nTrain Epoch: 96 [100/900 (11%)]\tLoss: 0.897097\nTrain Epoch: 96 [200/900 (22%)]\tLoss: 0.886500\nTrain Epoch: 96 [300/900 (33%)]\tLoss: 0.673228\nTrain Epoch: 96 [400/900 (44%)]\tLoss: 1.250131\nTrain Epoch: 96 [500/900 (56%)]\tLoss: 0.800995\nTrain Epoch: 96 [600/900 (67%)]\tLoss: 0.636494\nTrain Epoch: 96 [700/900 (78%)]\tLoss: 0.619771\nTrain Epoch: 96 [800/900 (89%)]\tLoss: 0.792947\n\nevaluating...\nTest set: Average loss: 1.3445, Average CER: 0.385356 Average WER: 1.0017\n\nTrain Epoch: 97 [0/900 (0%)]\tLoss: 0.591543\nTrain Epoch: 97 [100/900 (11%)]\tLoss: 0.865737\nTrain Epoch: 97 [200/900 (22%)]\tLoss: 0.485373\nTrain Epoch: 97 [300/900 (33%)]\tLoss: 0.866054\nTrain Epoch: 97 [400/900 (44%)]\tLoss: 1.143053\nTrain Epoch: 97 [500/900 (56%)]\tLoss: 0.683634\nTrain Epoch: 97 [600/900 (67%)]\tLoss: 1.087814\nTrain Epoch: 97 [700/900 (78%)]\tLoss: 0.839989\nTrain Epoch: 97 [800/900 (89%)]\tLoss: 0.699910\n\nevaluating...\nTest set: Average loss: 1.3532, Average CER: 0.436820 Average WER: 0.9845\n\nTrain Epoch: 98 [0/900 (0%)]\tLoss: 0.773053\nTrain Epoch: 98 [100/900 (11%)]\tLoss: 0.821437\nTrain Epoch: 98 [200/900 (22%)]\tLoss: 0.969368\nTrain Epoch: 98 [300/900 (33%)]\tLoss: 0.580391\nTrain Epoch: 98 [400/900 (44%)]\tLoss: 0.815431\nTrain Epoch: 98 [500/900 (56%)]\tLoss: 0.854268\nTrain Epoch: 98 [600/900 (67%)]\tLoss: 0.799509\nTrain Epoch: 98 [700/900 (78%)]\tLoss: 1.101131\nTrain Epoch: 98 [800/900 (89%)]\tLoss: 0.996903\n\nevaluating...\nTest set: Average loss: 1.3552, Average CER: 0.382118 Average WER: 0.9930\n\nTrain Epoch: 99 [0/900 (0%)]\tLoss: 1.126696\nTrain Epoch: 99 [100/900 (11%)]\tLoss: 0.606105\nTrain Epoch: 99 [200/900 (22%)]\tLoss: 1.068051\nTrain Epoch: 99 [300/900 (33%)]\tLoss: 0.856731\nTrain Epoch: 99 [400/900 (44%)]\tLoss: 0.710282\nTrain Epoch: 99 [500/900 (56%)]\tLoss: 0.822399\nTrain Epoch: 99 [600/900 (67%)]\tLoss: 1.605337\nTrain Epoch: 99 [700/900 (78%)]\tLoss: 0.692865\nTrain Epoch: 99 [800/900 (89%)]\tLoss: 0.809825\n\nevaluating...\nTest set: Average loss: 1.4547, Average CER: 0.416664 Average WER: 1.0639\n\nTrain Epoch: 100 [0/900 (0%)]\tLoss: 0.850309\nTrain Epoch: 100 [100/900 (11%)]\tLoss: 0.643456\nTrain Epoch: 100 [200/900 (22%)]\tLoss: 0.735172\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}