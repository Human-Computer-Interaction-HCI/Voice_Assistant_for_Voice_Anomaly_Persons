{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "t8hxdLWb_Rpd"
   },
   "outputs": [],
   "source": [
    "data = '/home/boris/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual Phoneme Recognition/data/raw_kaggle/Speeches.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Ry4Dpt7o_Rpg"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import jiwer\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.dataset import SpeechDataset, collate_spl\n",
    "from utils.transforms import XLSRTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3ushwkP9_Rph"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/boris/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual Phoneme Recognition/models/phonemizer were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at /home/boris/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual Phoneme Recognition/models/phonemizer and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "xls_r_transformer = XLSRTransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqDkG6V2_Rph"
   },
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kthEq-rH_Rpj"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7ET-LsWA_Rpj"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Число</th>\n",
       "      <th>Русская речь</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Как пройти до корпуса?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Где взять направление?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Бумага есть</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>анальгин</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>вата</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Число            Русская речь\n",
       "0      1  Как пройти до корпуса?\n",
       "1      2  Где взять направление?\n",
       "2      3             Бумага есть\n",
       "3      4                анальгин\n",
       "4      5                    вата"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "J8gq9Hcc_Rpk"
   },
   "outputs": [],
   "source": [
    "train_dataset = SpeechDataset(df, 0, 1500)\n",
    "val_dataset = SpeechDataset(df, 1500, 1700)\n",
    "test_dataset = SpeechDataset(df, 1600, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "mvuhyqdB_Rpk"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, BATCH_SIZE, True, collate_fn=collate_spl)\n",
    "val_dataloader = DataLoader(val_dataset, BATCH_SIZE, True, collate_fn=collate_spl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "z6KvoXMl_Rpm"
   },
   "outputs": [],
   "source": [
    "abc = '?абвгдеёжзийклмнопрстуфхшщчцьыъэюя'\n",
    "\n",
    "def show_batch(yam, epoch, bn, labels):\n",
    "    if yam.sum()<1: return\n",
    "    labels = labels.argmax(-1)\n",
    "    etalon_s=''\n",
    "    for i in range(labels.shape[0]):\n",
    "        for j in range(labels.shape[1]):\n",
    "            if (ci := labels[i,j])>0:\n",
    "                etalon_s+=abc[ci]\n",
    "        etalon_s+='|'\n",
    "    s=''\n",
    "    for i in yam:\n",
    "        f=False\n",
    "        for l in i:\n",
    "            if li:=l.item():\n",
    "                f=True\n",
    "                s+=abc[li]\n",
    "        if f:\n",
    "            s+='|'\n",
    "    c=-1\n",
    "    if len(s)>0:\n",
    "        c=jiwer.cer(etalon_s, s)\n",
    "        print(f'{epoch}.{bn}:', s, etalon_s, 'cer: ', c)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "8R-PBVjR_Rpm"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "DFmU3KAm_Rpn"
   },
   "outputs": [],
   "source": [
    "def get_y_lengths(y):\n",
    "    return torch.count_nonzero(y, -1)\n",
    "\n",
    "def padded_stack(list_of_tensors, maxlen=16):\n",
    "    # print([i.shape for i in list_of_tensors])\n",
    "    maxlen = max(x.shape[0] for x in list_of_tensors)\n",
    "    output = torch.zeros((len(list_of_tensors), maxlen, list_of_tensors[0].shape[-1]))\n",
    "    # print(output.shape)\n",
    "    for i, t in enumerate(list_of_tensors):\n",
    "        output[i, :min(maxlen, t.shape[0]), :] = t[:maxlen, :]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "pY2PpGyA_Rpo"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "TYhN5oMi_Rpk"
   },
   "outputs": [],
   "source": [
    "class SimplePerceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(392,34)\n",
    "        self.act = nn.LogSoftmax(-1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X, _ =self.rnn(X)\n",
    "        X = self.act(X)\n",
    "        return X\n",
    "\n",
    "perceptron = SimplePerceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "luK_RIVG_Rpl"
   },
   "outputs": [],
   "source": [
    "loss = nn.CTCLoss(\n",
    "    zero_infinity=True,\n",
    "    blank=0, )\n",
    "optimizer = torch.optim.AdamW(perceptron.parameters(), lr=0.0001,# weight_decay=0.1\n",
    "                              )\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "A2_X9pEs_Rpo"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a931b51c9c47fbbc950a36a6b07a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cfa01566003418da59667d67146a1da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0: ёрётёбёввтёёрёввётёрхо|ёвбётёёбббёрврвбёеееее| вкакихморях|римскийогоньпотух| cer:  1.4\n",
      "0.10: ввёвровввввёрррхффффффффффффффф|хрввррвёётётовёёхёрврвтвюёроёрр| способсказать|космоснеперестаетнасудивлять| cer:  1.3488372093023255\n",
      "0.20: вёвтвёвввёёоёоротрррвоёрвхрвррввё|рвхвбхвёрттвввввёввёворрёёеееееее| книгасопровождающаяваснавсюжизнь|шатенкиибрюнеткипохожи| cer:  1.0714285714285714\n",
      "0.30: вёввтётррёхвоёёр|ввввввттоёввёвех| разработкаэвм|расширения| cer:  1.2\n",
      "0.40: ворввтвтввёохоорвффффффф|ровввввтвтвёвхврвёввввтв| явзялсебявруки|естьялюблювязаниеисамбо| cer:  1.1538461538461537\n",
      "0.50: вввртохвввфффффффффффффффф|врвтрвёрврртррвтотввхрввёх| наборданных|видеонаютубе| cer:  1.96\n",
      "0.60: втотоотёвовёёрвврхо|воорёёвхёрврввеееее| повторитепожалуйста|первоеправило| cer:  0.9117647058823529\n",
      "0.70: вооррёврввёвёвёёцётёёхрвррвввворвх|вттвтрёвёрхрхееееееееееецееееееее| рыбалкавеселеееслистобойестькот|красивыйцветок| cer:  1.297872340425532\n",
      "0.80: рёвёюёроввёвтрров|твоовввотооввввхв| безмятежнаягавань|красивыеромашки| cer:  0.9411764705882353\n",
      "0.90: врвввёюрёрёоёёввовттёвоовт|вхёрохтовврровёрхеееееееее| наушникизастрялинапроводах|объезднаядорога| cer:  1.1162790697674418\n",
      "0.100: рёвтрёрвёвёврвёр|рорроврёвёрртёхр| холодныйвоздух|увидимсяпозже| cer:  1.0689655172413792\n",
      "0.110: хвхвооврввёрвфффффффффффф|вврвровввввовёёорёхотрврё| содержание|мывдвоёмгуляемсчитаемшаги| cer:  1.2432432432432432\n",
      "0.120: рвхотрроввтововвроюфффффффффффффффф|рёвтоввохвввввввёввтввврвтввввроввв| автобусбыстроедет|зеленыелугараскинулисьдогоризонта| cer:  1.2115384615384615\n",
      "0.130: ёррврвффффффффффффффффффффффффффффффффффффффф|врвврёввввовврврвбввввовёввхвёовввтёвввёвтвво| намдобавитьвашеимякномеру|новаяинтереснаяиградлякомпьютера| cer:  1.4576271186440677\n",
      "0.140: оюёвёрррфффффффффффффффффффффф|тёвооооротёоотовврвроввовоовов| пятница|первыйвзглядбываетобманчивым| cer:  1.5405405405405406\n",
      "0.150: ввртхрвёврвровхотрёвтоёёввр|ввввоввёрёрввввохёовеееееее| макдональдсвсетакизакрылся|смешныевидеона| cer:  1.1904761904761905\n",
      "0.160: оррёёёёвоввхр|ёхёвврртовррт| казниегипетские|холодныйвзгляд| cer:  0.9032258064516129\n",
      "0.170: тотёорвоввворврввввёрвоввювррввровтвво|ёввврёрвёвввввовёвёвохееееееееееееееее| природавдохновляетнатворчество|венскиевафлисклубникой| cer:  1.2037037037037037\n",
      "0.180: ооввтёввовтоввввввввоврёврвёвррввоввврв|ётотврроттрввхоовреееееееееееееееееееее| чистыйвоздухнеобходимдляздоровья|хорошеезаканчивается| cer:  1.2037037037037037\n",
      "0.190: рохррвёоёвоврввоовтооффф|ртёхртоёвхввврёрхвхвовёв| вершинаайсберга|аудиторияр| cer:  1.6296296296296295\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/boris/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual Phoneme Recognition/notebooks/3sem/simple rnn.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/boris/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual%20Phoneme%20Recognition/notebooks/3sem/simple%20rnn.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m bn, ((wf_batch, wf_lengths), (labels, labels_lengths)) \u001b[39min\u001b[39;00m (t2\u001b[39m:=\u001b[39mtqdm(\u001b[39menumerate\u001b[39m(train_dataloader), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(train_dataloader), leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/boris/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual%20Phoneme%20Recognition/notebooks/3sem/simple%20rnn.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# clear_output()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/boris/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual%20Phoneme%20Recognition/notebooks/3sem/simple%20rnn.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m#display(\"ji\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/boris/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual%20Phoneme%20Recognition/notebooks/3sem/simple%20rnn.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/boris/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual%20Phoneme%20Recognition/notebooks/3sem/simple%20rnn.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         tensor_list\u001b[39m=\u001b[39m[xls_r_transformer(wf_batch[i], label\u001b[39m=\u001b[39;49mlabels[i]) \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(wf_batch\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m])]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/boris/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual%20Phoneme%20Recognition/notebooks/3sem/simple%20rnn.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         phonemes \u001b[39m=\u001b[39m padded_stack(tensor_list)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/boris/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual%20Phoneme%20Recognition/notebooks/3sem/simple%20rnn.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;32m/home/boris/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual Phoneme Recognition/notebooks/3sem/simple rnn.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/boris/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual%20Phoneme%20Recognition/notebooks/3sem/simple%20rnn.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m bn, ((wf_batch, wf_lengths), (labels, labels_lengths)) \u001b[39min\u001b[39;00m (t2\u001b[39m:=\u001b[39mtqdm(\u001b[39menumerate\u001b[39m(train_dataloader), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(train_dataloader), leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/boris/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual%20Phoneme%20Recognition/notebooks/3sem/simple%20rnn.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# clear_output()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/boris/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual%20Phoneme%20Recognition/notebooks/3sem/simple%20rnn.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m#display(\"ji\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/boris/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual%20Phoneme%20Recognition/notebooks/3sem/simple%20rnn.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/boris/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual%20Phoneme%20Recognition/notebooks/3sem/simple%20rnn.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         tensor_list\u001b[39m=\u001b[39m[xls_r_transformer(wf_batch[i], label\u001b[39m=\u001b[39;49mlabels[i]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(wf_batch\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/boris/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual%20Phoneme%20Recognition/notebooks/3sem/simple%20rnn.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         phonemes \u001b[39m=\u001b[39m padded_stack(tensor_list)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/boris/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual%20Phoneme%20Recognition/notebooks/3sem/simple%20rnn.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual Phoneme Recognition/notebooks/3sem/utils/transforms.py:44\u001b[0m, in \u001b[0;36mXLSRTransformer.__call__\u001b[0;34m(self, wf, sr, label)\u001b[0m\n\u001b[1;32m     42\u001b[0m tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessor(wf, sampling_rate\u001b[39m=\u001b[39msr, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39minput_values\n\u001b[1;32m     43\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 44\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(tokens)\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     46\u001b[0m ixs \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(logits\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]) \u001b[39mif\u001b[39;00m logits[:, i, :]\u001b[39m.\u001b[39margmax()\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m]\n\u001b[1;32m     47\u001b[0m logits \u001b[39m=\u001b[39m logits[\u001b[39m0\u001b[39m, ixs, :]\n",
      "File \u001b[0;32m~/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual Phoneme Recognition/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual Phoneme Recognition/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual Phoneme Recognition/venv/lib/python3.11/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1962\u001b[0m, in \u001b[0;36mWav2Vec2ForCTC.forward\u001b[0;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[1;32m   1952\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1953\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1954\u001b[0m \u001b[39m    Labels for connectionist temporal classification. Note that `target_length` has to be smaller or equal to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1957\u001b[0m \u001b[39m    config.vocab_size - 1]`.\u001b[39;00m\n\u001b[1;32m   1958\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1960\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1962\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwav2vec2(\n\u001b[1;32m   1963\u001b[0m     input_values,\n\u001b[1;32m   1964\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1965\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1966\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1967\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1968\u001b[0m )\n\u001b[1;32m   1970\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1971\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n",
      "File \u001b[0;32m~/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual Phoneme Recognition/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual Phoneme Recognition/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual Phoneme Recognition/venv/lib/python3.11/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1547\u001b[0m, in \u001b[0;36mWav2Vec2Model.forward\u001b[0;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1542\u001b[0m output_hidden_states \u001b[39m=\u001b[39m (\n\u001b[1;32m   1543\u001b[0m     output_hidden_states \u001b[39mif\u001b[39;00m output_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39moutput_hidden_states\n\u001b[1;32m   1544\u001b[0m )\n\u001b[1;32m   1545\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1547\u001b[0m extract_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_extractor(input_values)\n\u001b[1;32m   1548\u001b[0m extract_features \u001b[39m=\u001b[39m extract_features\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m   1550\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1551\u001b[0m     \u001b[39m# compute reduced attention_mask corresponding to feature vectors\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual Phoneme Recognition/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual Phoneme Recognition/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual Phoneme Recognition/venv/lib/python3.11/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:459\u001b[0m, in \u001b[0;36mWav2Vec2FeatureEncoder.forward\u001b[0;34m(self, input_values)\u001b[0m\n\u001b[1;32m    454\u001b[0m         hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    455\u001b[0m             conv_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    456\u001b[0m             hidden_states,\n\u001b[1;32m    457\u001b[0m         )\n\u001b[1;32m    458\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m         hidden_states \u001b[39m=\u001b[39m conv_layer(hidden_states)\n\u001b[1;32m    461\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual Phoneme Recognition/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual Phoneme Recognition/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual Phoneme Recognition/venv/lib/python3.11/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:334\u001b[0m, in \u001b[0;36mWav2Vec2LayerNormConvLayer.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[0;32m--> 334\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(hidden_states)\n\u001b[1;32m    336\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    337\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n",
      "File \u001b[0;32m~/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual Phoneme Recognition/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual Phoneme Recognition/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual Phoneme Recognition/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual Phoneme Recognition/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    307\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in (t1:=tqdm(range(100))):\n",
    "    elv=0.0\n",
    "    elc=0\n",
    "    blv = 0.0\n",
    "    blc = 0\n",
    "    minlv=float(\"inf\")\n",
    "    cer = 0\n",
    "    cer_cnt = 0\n",
    "    for bn, ((wf_batch, wf_lengths), (labels, labels_lengths)) in (t2:=tqdm(enumerate(train_dataloader), total=len(train_dataloader), leave=False)):\n",
    "        # clear_output()\n",
    "        #display(\"ji\")\n",
    "        try:\n",
    "            tensor_list=[xls_r_transformer(wf_batch[i], label=labels[i]) for i in range(wf_batch.shape[0])]\n",
    "            phonemes = padded_stack(tensor_list)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "        # phonemes: batch  x len x classes\n",
    "        # print(phonemes.shape)\n",
    "        y = perceptron(phonemes)\n",
    "        \n",
    "        yam = y[:,:].argmax(-1)\n",
    "        yl = get_y_lengths(yam)\n",
    "        yam = y[:,:].argmax(-1)\n",
    "\n",
    "        # print(y.shape, labels.shape)\n",
    "\n",
    "        lv = loss(\n",
    "            y.permute(1,0,2),\n",
    "            labels.argmax(-1),\n",
    "            yl,\n",
    "            labels_lengths\n",
    "        )\n",
    "        # plotlosses.update({'loss': lv.item()})\n",
    "        # plotlosses.send()\n",
    "        # print(lv.item())\n",
    "        if not (math.isnan(lv.item()) or math.isinf(lv.item())):\n",
    "            blv+=lv.exp().item()\n",
    "        blc+=BATCH_SIZE\n",
    "        # writer.add_scalar('loss', lv.item(), step)\n",
    "        t2.set_postfix_str(f'loss: {blv/blc}/{elv/elc if elc>0 else \"inf\"}')\n",
    "        # clear_output()\n",
    "\n",
    "        lv.backward()\n",
    "        if bn%10==0:\n",
    "            cer += show_batch(yam, epoch, bn, labels)\n",
    "            cer_cnt += BATCH_SIZE\n",
    "            elv+=blv\n",
    "            elc+=blc\n",
    "            optimizer.step()\n",
    "            while (lv1 := loss(\n",
    "            y.permute(1,0,2),\n",
    "            labels.argmax(-1),\n",
    "            yl,\n",
    "            labels_lengths\n",
    "        ).item())<lv.item() and not (math.isnan(lv.item()) or math.isnan(lv1)):\n",
    "                print('>', end='')\n",
    "                optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            blv=0\n",
    "            blc=0\n",
    "        # scheduler.step()\n",
    "    t1.set_postfix_str(f'loss: {elv/elc} cer: {cer/cer_cnt}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3v4TiOL_Rpp"
   },
   "outputs": [],
   "source": [
    "torch.save(perceptron, '240104.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yam.sum()>-0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
