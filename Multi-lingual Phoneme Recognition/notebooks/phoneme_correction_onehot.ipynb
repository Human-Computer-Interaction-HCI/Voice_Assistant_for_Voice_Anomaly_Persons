{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/boris/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual Phoneme Recognition/notebooks',\n",
       " '/home/boris/miniconda3/envs/voice-speaker/lib/python310.zip',\n",
       " '/home/boris/miniconda3/envs/voice-speaker/lib/python3.10',\n",
       " '/home/boris/miniconda3/envs/voice-speaker/lib/python3.10/lib-dynload',\n",
       " '',\n",
       " '/home/boris/.local/lib/python3.10/site-packages',\n",
       " '/home/boris/miniconda3/envs/voice-speaker/lib/python3.10/site-packages',\n",
       " '/tmp/tmp57_ujoaa',\n",
       " '../src']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='val.log',\n",
    "                    filemode='a',\n",
    "                    format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "                    datefmt='%H:%M:%S',\n",
    "                    level=logging.DEBUG)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "import pytorch_lightning as pl\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"/home/boris/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual Phoneme Recognition/models/processor/vocab.json\")\n",
    "phonemes: dict[str, int] = json.load(f)\n",
    "f.close()\n",
    "\n",
    "phonemes_reverse ={j:i for i, j in phonemes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size': 4,\n",
    "    'phonemes': 392\n",
    "}\n",
    "logger.info(str(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/boris/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual Phoneme Recognition/data/датасет3.csv').values.tolist()\n",
    "df = pd.read_csv('/home/boris/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual Phoneme Recognition/data/xls-r_dataset.csv').values.tolist()\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.1)\n",
    "train_df, val_df = train_test_split(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_df, params[\"batch_size\"], True)\n",
    "val_loader = DataLoader(val_df, params[\"batch_size\"], True)\n",
    "test_loader = DataLoader(test_df, params[\"batch_size\"], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let see what is the type of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([192, 632,  73,  41]),\n",
       " ('Варить кофе с солью',\n",
       "  'Смерть это необратимый финал нашей жизни',\n",
       "  'Матёрый',\n",
       "  'отмечать'),\n",
       " ('v a ɡ r i t k o f e s t o n ʒ o',\n",
       "  's y n e k r i t ɛ t e n e a v u k k r a t i l m i f i n ɑ n e f e ʒ e z ə n i',\n",
       "  'm aː t o k k r i x',\n",
       "  'a t ʌ m i ʃ ɛ l tʃ')]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, we've `ids: list[int], texts: list['str'], phonemes: list[str]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = \"?абвгдеёжзийклмнопрстуфхшщчцьыъэюя\"\n",
    "def vectorize_str(labels: tuple[str]):\n",
    "    lengths = torch.LongTensor(size=(len(labels),))\n",
    "    letters = torch.zeros(size=(len(labels),  max(map(len, labels)), len(abc)), dtype=torch.double)\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        lengths[i] = len(label)\n",
    "        j=0\n",
    "        for c in label.lower():\n",
    "            if not c in abc:\n",
    "                lengths[i]-=1\n",
    "            else:\n",
    "                letters[i,j, abc.index(c)]=1\n",
    "                # letters[i,j]= abc.index(c)\n",
    "                j+=1\n",
    "    \n",
    "    return letters, lengths\n",
    "def decode_str(X, pred=True):\n",
    "    r = []\n",
    "    for i in range(X.shape[0]):\n",
    "        s = []\n",
    "        s1 = []\n",
    "        for j in range(X.shape[1]):\n",
    "            if (ix:=X[i,j].argmax()) >= 1:\n",
    "                s.append(abc[ix])\n",
    "            s1.append(abc[X[i,j,1:].argmax()+1])\n",
    "        if True:\n",
    "            r.append(''.join(s)+'/'+''.join(s1))\n",
    "        else:\n",
    "            r.append(''.join(s))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['сколькоможнотакделать/сколькоможнотакделатьаааааааааааааааааааааа',\n",
       " 'лазурноеморе/лазурноемореааааааааааааааааааааааааааааааа',\n",
       " 'зубнаяпаста/зубнаяпастааааааааааааааааааааааааааааааааа',\n",
       " 'нонапоездепочтиникогданебываетаварий/нонапоездепочтиникогданебываетаварийааааааа']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = vectorize_str(batch[1])[0]\n",
    "decode_str(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_phonemes(labels: tuple[str]):\n",
    "    lengths = torch.LongTensor(size=(len(labels),))\n",
    "    letters = torch.zeros(\n",
    "        size=(\n",
    "            len(labels),\n",
    "            max(map(lambda x: len(x.split()), labels)),\n",
    "            params['phonemes']\n",
    "        ),\n",
    "        dtype=torch.float\n",
    "    )\n",
    "    # letters = torch.zeros(size=(len(labels),  max(map(len, labels))), dtype=float)\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        lengths[i] = len(label.split())\n",
    "        for j, c in enumerate(label.split()):\n",
    "            letters[i,j, phonemes[c]]=1\n",
    "    \n",
    "    return letters, lengths\n",
    "def decode_phonemes(X):\n",
    "    r = []\n",
    "    for i in range(X.shape[0]):\n",
    "        s = []\n",
    "        for j in range(X.shape[1]):\n",
    "            if X[i,j].argmax() >= 0:\n",
    "                s.append(phonemes_reverse[int(X[i,j].argmax())])\n",
    "        r.append(''.join(s))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('s k r o l k a m o ʒ ɯ n a t a k n t e l ɯ t',\n",
       " 'l a z o k r o n n a ɡ e m o k r i',\n",
       " 'z o v o n aː j a p a s t a',\n",
       " 'n o n a p o e z i d e p e tʃ t i n i k a ɡ d a n e v o v a e t a v a k r i x')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['skrolkamoʒɯnatakntelɯt<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " 'lazokronnaɡemokri<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " 'zovonaːjapasta<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " 'nonapoezidepetʃtinikaɡdanevovaetavakrix']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_phonemes(vectorize_phonemes(batch[2])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test of preprocess correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self, *args) -> None:\n",
    "        super().__init__()\n",
    "        self.shape = args\n",
    "    def forward(self, x):\n",
    "        return x.reshape(*self.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1DCorrector(pl.LightningModule):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(\n",
    "            params['phonemes'],\n",
    "            20,\n",
    "            3,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(20, 40, batch_first=True)\n",
    "        self.linear = nn.Linear(40, len(abc))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: batch x len x phonemes\n",
    "        x = x.permute(0,2,1)\n",
    "        # x = x.reshape(x.shape[0], 1, x.shape[1], x.shape[2])# x: batch x 1 x phonhemes x len\n",
    "        x = self.conv(x)\n",
    "        x = x.permute(0,2, 1)\n",
    "        x, (h, c) = self.lstm(x)\n",
    "        x = self.linear(x)\n",
    "        return F.log_softmax(x, -1)\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        labels, lengths = vectorize_str(train_batch[1])\n",
    "        phonemes_vec, _ = vectorize_phonemes(train_batch[2])\n",
    "        \n",
    "        prediction = self(phonemes_vec.to(device))\n",
    "        \n",
    "        ilengths = lengths.clone()\n",
    "        for i in range(prediction.shape[0]):\n",
    "            ilengths[i] = prediction.shape[1]\n",
    "        \n",
    "        lv = F.ctc_loss(prediction.permute(1, 0, 2), labels.argmax(dim=-1), ilengths, lengths, zero_infinity=True)\n",
    "        \n",
    "        if batch_idx==0:\n",
    "            logging.info(\"train \\\"%s\\\" recognized as \\\"%s\\\"\", '|'.join(decode_str(labels, False)), '|'.join(decode_str(prediction)))\n",
    "        \n",
    "        self.log('train_loss', lv, True, batch_size=len(train_batch))\n",
    "        return lv\n",
    "    def validation_step(self, train_batch, batch_idx):\n",
    "        labels, lengths = vectorize_str(train_batch[1])\n",
    "        phonemes_vec, _ = vectorize_phonemes(train_batch[2])\n",
    "        \n",
    "        prediction = self(phonemes_vec.to(device))\n",
    "        \n",
    "        ilengths = lengths.clone()\n",
    "        for i in range(prediction.shape[0]):\n",
    "            ilengths[i] = prediction.shape[1]\n",
    "        \n",
    "        lv = F.ctc_loss(prediction.permute(1, 0, 2), labels.argmax(dim=-1), ilengths, lengths, zero_infinity=True)\n",
    "\n",
    "        if batch_idx==0:\n",
    "            logging.info(\"val \\\"%s\\\" recognized as \\\"%s\\\"\", '|'.join(decode_str(labels, False)), '|'.join(decode_str(prediction)))\n",
    "        \n",
    "        self.log('val_loss', lv, True, batch_size=len(train_batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conv1DCorrector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 20, 392])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = vectorize_phonemes(next(iter(train_loader))[2])[0]\n",
    "db.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================================\n",
      "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
      "===================================================================================================================\n",
      "Conv1DCorrector                          [4, 20, 392]              [4, 20, 34]               --\n",
      "├─Conv1d: 1-1                            [4, 392, 20]              [4, 20, 20]               23,540\n",
      "├─LSTM: 1-2                              [4, 20, 20]               [4, 20, 40]               9,920\n",
      "├─Linear: 1-3                            [4, 20, 40]               [4, 20, 34]               1,394\n",
      "===================================================================================================================\n",
      "Total params: 34,854\n",
      "Trainable params: 34,854\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 2.68\n",
      "===================================================================================================================\n",
      "Input size (MB): 0.13\n",
      "Forward/backward pass size (MB): 0.06\n",
      "Params size (MB): 0.14\n",
      "Estimated Total Size (MB): 0.33\n",
      "===================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boris/miniconda3/envs/voice-speaker/lib/python3.10/site-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  action_fn=lambda data: sys.getsizeof(data.storage()),\n",
      "/home/boris/miniconda3/envs/voice-speaker/lib/python3.10/site-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return super().__sizeof__() + self.nbytes()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
       "===================================================================================================================\n",
       "Conv1DCorrector                          [4, 20, 392]              [4, 20, 34]               --\n",
       "├─Conv1d: 1-1                            [4, 392, 20]              [4, 20, 20]               23,540\n",
       "├─LSTM: 1-2                              [4, 20, 20]               [4, 20, 40]               9,920\n",
       "├─Linear: 1-3                            [4, 20, 40]               [4, 20, 34]               1,394\n",
       "===================================================================================================================\n",
       "Total params: 34,854\n",
       "Trainable params: 34,854\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 2.68\n",
       "===================================================================================================================\n",
       "Input size (MB): 0.13\n",
       "Forward/backward pass size (MB): 0.06\n",
       "Params size (MB): 0.14\n",
       "Estimated Total Size (MB): 0.33\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_data=db, verbose=1, col_names=['input_size', 'output_size', 'num_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([237, 494, 176, 217]),\n",
       " ('Умничка', 'Производство', 'Дайте волку банан', 'Маленькие женщины'),\n",
       " ('o m n i tʃ k a',\n",
       "  'd e j t e r ə n ɛ θ t',\n",
       "  'd aɪ t e v o l k o v a n a n',\n",
       "  'm a l ɪ n v k eː j ʒ n ʃ e n ɪ')]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boris/miniconda3/envs/voice-speaker/lib/python3.10/site-packages/pytorch_lightning/core/module.py:410: UserWarning: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "model.validation_step(b, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=400, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name   | Type   | Params\n",
      "----------------------------------\n",
      "0 | conv   | Conv1d | 23.5 K\n",
      "1 | lstm   | LSTM   | 9.9 K \n",
      "2 | linear | Linear | 1.4 K \n",
      "----------------------------------\n",
      "34.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "34.9 K    Total params\n",
      "0.139     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|██████████| 129/129 [00:03<00:00, 37.51it/s, v_num=13, train_loss=1.570, val_loss=3.230]"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, test_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lightning.ai/docs/pytorch/stable/common/checkpointing_basic.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b876e42842332973d62de1b5c496584337e64d1e51e29a8a7f1efa2c60f9a9f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
