{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/boris/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual Phoneme Recognition/notebooks',\n",
       " '/usr/lib/python310.zip',\n",
       " '/usr/lib/python3.10',\n",
       " '/usr/lib/python3.10/lib-dynload',\n",
       " '',\n",
       " '/home/boris/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/venv/lib/python3.10/site-packages',\n",
       " '/tmp/tmprzh1u8oy',\n",
       " '../src',\n",
       " '../src']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "import pytorch_lightning as pl\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"/home/boris/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/Multi-lingual Phoneme Recognition/models/processor/vocab.json\")\n",
    "phonemes: dict[str, int] = json.load(f)\n",
    "f.close()\n",
    "\n",
    "phonemes_reverse ={j:i for i, j in phonemes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size': 4,\n",
    "    'phonemes': 392\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/boris/Projects/Project-On-Voice-Assistant/data/xls-r_dataset.csv').values.tolist()\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.1)\n",
    "train_df, val_df = train_test_split(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_df, params[\"batch_size\"], True)\n",
    "val_loader = DataLoader(val_df, params[\"batch_size\"], True)\n",
    "test_loader = DataLoader(test_df, params[\"batch_size\"], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let see what is the type of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 44, 243, 130, 380]),\n",
       " ('я очень высокий',\n",
       "  'Централизованный объект',\n",
       "  'Вода начала пузыриться',\n",
       "  'Закажем пиццу?'),\n",
       " ('i a o l tʃ e n i v e s o k i',\n",
       "  'ts e n k r e n d e z o v a n i a m b e j e k t',\n",
       "  'd a n e tʃ i l a p o z e p i ts a',\n",
       "  'n e o d a v l e t v a k r i t i l n i')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, we've `ids: list[int], texts: list['str'], phonemes: list[str]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = \"? абвгдеёжзийклмнопрстуфхшщчцьыъэюя\"\n",
    "def vectorize_str(labels: tuple[str]):\n",
    "    lengths = torch.LongTensor(size=(len(labels),))\n",
    "    letters = torch.zeros(size=(len(labels),  max(map(len, labels)), len(abc)), dtype=torch.double)\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        lengths[i] = len(label)\n",
    "        j=0\n",
    "        for c in label.lower():\n",
    "            if not c in abc:\n",
    "                lengths[i]-=1\n",
    "            else:\n",
    "                letters[i,j, abc.index(c)]=1\n",
    "                # letters[i,j]= abc.index(c)\n",
    "                j+=1\n",
    "    \n",
    "    return letters, lengths\n",
    "def decode_str(X):\n",
    "    r = []\n",
    "    for i in range(X.shape[0]):\n",
    "        s = []\n",
    "        s1 = []\n",
    "        for j in range(X.shape[1]):\n",
    "            if X[i,j].argmax() >= 0:\n",
    "                s.append(abc[X[i,j].argmax()])\n",
    "            s1.append(abc[X[i,j,1:].argmax()+1])\n",
    "        r.append(''.join(s)+'/'+''.join(s1))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['я очень высокий????????/я очень высокий        ',\n",
       " 'централизованный объект/централизованный объект',\n",
       " 'вода начала пузыриться?/вода начала пузыриться ',\n",
       " 'закажем пиццу??????????/закажем пиццу          ']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = vectorize_str(batch[1])[0]\n",
    "decode_str(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_phonemes(labels: tuple[str]):\n",
    "    lengths = torch.LongTensor(size=(len(labels),))\n",
    "    letters = torch.zeros(\n",
    "        size=(\n",
    "            len(labels),\n",
    "            max(map(lambda x: len(x.split()), labels)),\n",
    "            params['phonemes']\n",
    "        ),\n",
    "        dtype=torch.float\n",
    "    )\n",
    "    # letters = torch.zeros(size=(len(labels),  max(map(len, labels))), dtype=float)\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        lengths[i] = len(label.split())\n",
    "        for j, c in enumerate(label.split()):\n",
    "            letters[i,j, phonemes[c]]=1\n",
    "    \n",
    "    return letters, lengths\n",
    "def decode_phonemes(X):\n",
    "    r = []\n",
    "    for i in range(X.shape[0]):\n",
    "        s = []\n",
    "        for j in range(X.shape[1]):\n",
    "            if X[i,j].argmax() >= 0:\n",
    "                s.append(phonemes_reverse[int(X[i,j].argmax())])\n",
    "        r.append(''.join(s))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('i a o l tʃ e n i v e s o k i',\n",
       " 'ts e n k r e n d e z o v a n i a m b e j e k t',\n",
       " 'd a n e tʃ i l a p o z e p i ts a',\n",
       " 'n e o d a v l e t v a k r i t i l n i')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iaoltʃenivesoki<pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " 'tsenkrendezovaniambejekt',\n",
       " 'danetʃilapozepitsa<pad><pad><pad><pad><pad><pad><pad>',\n",
       " 'neodavletvakritilni<pad><pad><pad><pad>']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_phonemes(vectorize_phonemes(batch[2])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test of preprocess correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self, *args) -> None:\n",
    "        super().__init__()\n",
    "        self.shape = args\n",
    "    def forward(self, x):\n",
    "        return x.reshape(*self.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1DCorrector(pl.LightningModule):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            1,\n",
    "            33,\n",
    "            (params['phonemes'], 3),\n",
    "            padding=(0, 1)\n",
    "        )\n",
    "        self.lstm = nn.LSTM(33, 16)\n",
    "        self.linear = nn.Linear(16, len(abc))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: batch x len x phonemes\n",
    "        x = x.permute(0,2,1)\n",
    "        x = x.reshape(x.shape[0], 1, x.shape[1], x.shape[2])# x: batch x 1 x phonhemes x len\n",
    "        x = self.conv(x)\n",
    "        # x : batch x channels x 1 x len\n",
    "        x = x[:,:,0,:].permute(0, 2, 1)\n",
    "        x, (h, c) = self.lstm(x)\n",
    "        x = self.linear(x)\n",
    "        return F.log_softmax(x)\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        labels, lengths = vectorize_str(train_batch[1])\n",
    "        phonemes_vec, _ = vectorize_phonemes(train_batch[2])\n",
    "        \n",
    "        prediction = self(phonemes_vec.to(device))\n",
    "        \n",
    "        ilengths = lengths.clone()\n",
    "        for i in range(prediction.shape[0]):\n",
    "            ilengths[i] = prediction.shape[1]\n",
    "        \n",
    "        lv = F.ctc_loss(prediction.permute(1, 0, 2), labels.argmax(dim=-1), ilengths, lengths, zero_infinity=True)\n",
    "        \n",
    "        self.log('train_loss', lv, True)\n",
    "        return lv\n",
    "    def validation_step(self, train_batch, batch_idx):\n",
    "        labels, lengths = vectorize_str(train_batch[1])\n",
    "        phonemes_vec, _ = vectorize_phonemes(train_batch[2])\n",
    "        \n",
    "        prediction = self(phonemes_vec.to(device))\n",
    "        \n",
    "        ilengths = lengths.clone()\n",
    "        for i in range(prediction.shape[0]):\n",
    "            ilengths[i] = prediction.shape[1]\n",
    "        \n",
    "        lv = F.ctc_loss(prediction.permute(1, 0, 2), labels.argmax(dim=-1), ilengths, lengths, zero_infinity=True)\n",
    "        \n",
    "        self.log('val_loss', lv, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conv1DCorrector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 29, 392])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = vectorize_phonemes(next(iter(train_loader))[2])[0]\n",
    "db.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Conv1DCorrector                          [4, 29, 35]               --\n",
      "├─Conv2d: 1-1                            [4, 33, 1, 29]            38,841\n",
      "│    └─weight                                                      ├─38,808\n",
      "│    └─bias                                                        └─33\n",
      "├─LSTM: 1-2                              [1, 29, 16]               3,264\n",
      "│    └─weight_ih_l0                                                ├─2,112\n",
      "│    └─weight_hh_l0                                                ├─1,024\n",
      "│    └─bias_ih_l0                                                  ├─64\n",
      "│    └─bias_hh_l0                                                  └─64\n",
      "├─Linear: 1-3                            [4, 29, 35]               595\n",
      "│    └─weight                                                      ├─560\n",
      "│    └─bias                                                        └─35\n",
      "==========================================================================================\n",
      "Total params: 42,700\n",
      "Trainable params: 42,700\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 4.60\n",
      "==========================================================================================\n",
      "Input size (MB): 0.18\n",
      "Forward/backward pass size (MB): 0.07\n",
      "Params size (MB): 0.17\n",
      "Estimated Total Size (MB): 0.42\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30034/674004973.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Conv1DCorrector                          [4, 29, 35]               --\n",
       "├─Conv2d: 1-1                            [4, 33, 1, 29]            38,841\n",
       "│    └─weight                                                      ├─38,808\n",
       "│    └─bias                                                        └─33\n",
       "├─LSTM: 1-2                              [1, 29, 16]               3,264\n",
       "│    └─weight_ih_l0                                                ├─2,112\n",
       "│    └─weight_hh_l0                                                ├─1,024\n",
       "│    └─bias_ih_l0                                                  ├─64\n",
       "│    └─bias_hh_l0                                                  └─64\n",
       "├─Linear: 1-3                            [4, 29, 35]               595\n",
       "│    └─weight                                                      ├─560\n",
       "│    └─bias                                                        └─35\n",
       "==========================================================================================\n",
       "Total params: 42,700\n",
       "Trainable params: 42,700\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 4.60\n",
       "==========================================================================================\n",
       "Input size (MB): 0.18\n",
       "Forward/backward pass size (MB): 0.07\n",
       "Params size (MB): 0.17\n",
       "Estimated Total Size (MB): 0.42\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_data=db, batch_dim=0, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([569, 474, 205, 541]),\n",
       " ('Туалетная вода',\n",
       "  'Подпишите мою петицию',\n",
       "  'Тебе стоит собраться',\n",
       "  'Чистый переулок'),\n",
       " ('ɑ l e t u d n a j a v a n d a',\n",
       "  'n a k k r aː j a t u k ɔ k r a d s y n ɡ a l a v oː',\n",
       "  'd ɨ n v ø d oː d θ ʌ v ʌ ɡ r a θ a',\n",
       "  's e m e r f u n o d a ʃ i l ɑ s')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30034/674004973.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.0004, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.training_step(b, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type   | Params\n",
      "----------------------------------\n",
      "0 | conv   | Conv2d | 38.8 K\n",
      "1 | lstm   | LSTM   | 3.3 K \n",
      "2 | linear | Linear | 595   \n",
      "----------------------------------\n",
      "42.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "42.7 K    Total params\n",
      "0.171     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30034/674004973.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 129/129 [00:01<00:00, 74.79it/s, v_num=2, train_loss=0.000, val_loss=1.830]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boris/Projects/Voice_Assistant_for_Voice_Anomaly_Persons/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=40)\n",
    "trainer.fit(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30034/674004973.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "phonemes_vec, _ = vectorize_phonemes(b[2])\n",
    "prediction = model(phonemes_vec.to(device))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 25, 35])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "фйтюфюйюйтй?й?т??????????/фйтюфюйюйтйтййтооооооооеа\n",
      "еъфе?о?еъфе???ьфффффффффъ/еъфеоооеъфеоооьфффффффффъ\n",
      "????о?ф????о?ф?о?о???????/оооооофоооооофооооооооооо\n",
      "тооо?т?жооэффээоо????????/тоооотожооэффээооооооооое\n"
     ]
    }
   ],
   "source": [
    "print(*decode_str(prediction.permute(0,1,2)), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '../models/corr1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
