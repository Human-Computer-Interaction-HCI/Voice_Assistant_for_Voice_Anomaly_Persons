{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchaudio\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2023-04-26T16:40:54.715895Z","iopub.execute_input":"2023-04-26T16:40:54.717002Z","iopub.status.idle":"2023-04-26T16:40:55.943044Z","shell.execute_reply.started":"2023-04-26T16:40:54.716960Z","shell.execute_reply":"2023-04-26T16:40:55.941679Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def avg_wer(wer_scores, combined_ref_len):\n    return float(sum(wer_scores)) / float(combined_ref_len)\n\n\ndef _levenshtein_distance(ref, hyp):\n    \"\"\"Levenshtein distance is a string metric for measuring the difference\n    between two sequences. Informally, the levenshtein disctance is defined as\n    the minimum number of single-character edits (substitutions, insertions or\n    deletions) required to change one word into the other. We can naturally\n    extend the edits to word level when calculate levenshtein disctance for\n    two sentences.\n    \"\"\"\n    m = len(ref)\n    n = len(hyp)\n\n    # special case\n    if ref == hyp:\n        return 0\n    if m == 0:\n        return n\n    if n == 0:\n        return m\n\n    if m < n:\n        ref, hyp = hyp, ref\n        m, n = n, m\n\n    # use O(min(m, n)) space\n    distance = np.zeros((2, n + 1), dtype=np.int32)\n\n    # initialize distance matrix\n    for j in range(0,n + 1):\n        distance[0][j] = j\n\n    # calculate levenshtein distance\n    for i in range(1, m + 1):\n        prev_row_idx = (i - 1) % 2\n        cur_row_idx = i % 2\n        distance[cur_row_idx][0] = i\n        for j in range(1, n + 1):\n            if ref[i - 1] == hyp[j - 1]:\n                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n            else:\n                s_num = distance[prev_row_idx][j - 1] + 1\n                i_num = distance[cur_row_idx][j - 1] + 1\n                d_num = distance[prev_row_idx][j] + 1\n                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n\n    return distance[m % 2][n]\n\n\ndef word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n    \"\"\"Compute the levenshtein distance between reference sequence and\n    hypothesis sequence in word-level.\n    :param reference: The reference sentence.\n    :type reference: basestring\n    :param hypothesis: The hypothesis sentence.\n    :type hypothesis: basestring\n    :param ignore_case: Whether case-sensitive or not.\n    :type ignore_case: bool\n    :param delimiter: Delimiter of input sentences.\n    :type delimiter: char\n    :return: Levenshtein distance and word number of reference sentence.\n    :rtype: list\n    \"\"\"\n    if ignore_case == True:\n        reference = reference.lower()\n        hypothesis = hypothesis.lower()\n\n    ref_words = reference.split(delimiter)\n    hyp_words = hypothesis.split(delimiter)\n\n    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n    return float(edit_distance), len(ref_words)\n\n\ndef char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n    \"\"\"Compute the levenshtein distance between reference sequence and\n    hypothesis sequence in char-level.\n    :param reference: The reference sentence.\n    :type reference: basestring\n    :param hypothesis: The hypothesis sentence.\n    :type hypothesis: basestring\n    :param ignore_case: Whether case-sensitive or not.\n    :type ignore_case: bool\n    :param remove_space: Whether remove internal space characters\n    :type remove_space: bool\n    :return: Levenshtein distance and length of reference sentence.\n    :rtype: list\n    \"\"\"\n    if ignore_case == True:\n        reference = reference.lower()\n        hypothesis = hypothesis.lower()\n\n    join_char = ' '\n    if remove_space == True:\n        join_char = ''\n\n    reference = join_char.join(filter(None, reference.split(' ')))\n    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n\n    edit_distance = _levenshtein_distance(reference, hypothesis)\n    return float(edit_distance), len(reference)\n\n\ndef wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n    \"\"\"Calculate word error rate (WER). WER compares reference text and\n    hypothesis text in word-level. WER is defined as:\n    .. math::\n        WER = (Sw + Dw + Iw) / Nw\n    where\n    .. code-block:: text\n        Sw is the number of words subsituted,\n        Dw is the number of words deleted,\n        Iw is the number of words inserted,\n        Nw is the number of words in the reference\n    We can use levenshtein distance to calculate WER. Please draw an attention\n    that empty items will be removed when splitting sentences by delimiter.\n    :param reference: The reference sentence.\n    :type reference: basestring\n    :param hypothesis: The hypothesis sentence.\n    :type hypothesis: basestring\n    :param ignore_case: Whether case-sensitive or not.\n    :type ignore_case: bool\n    :param delimiter: Delimiter of input sentences.\n    :type delimiter: char\n    :return: Word error rate.\n    :rtype: float\n    :raises ValueError: If word number of reference is zero.\n    \"\"\"\n    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n                                         delimiter)\n\n    if ref_len == 0:\n        raise ValueError(\"Reference's word number should be greater than 0.\")\n\n    wer = float(edit_distance) / ref_len\n    return wer\n\n\ndef cer(reference, hypothesis, ignore_case=False, remove_space=False):\n    \"\"\"Calculate charactor error rate (CER). CER compares reference text and\n    hypothesis text in char-level. CER is defined as:\n    .. math::\n        CER = (Sc + Dc + Ic) / Nc\n    where\n    .. code-block:: text\n        Sc is the number of characters substituted,\n        Dc is the number of characters deleted,\n        Ic is the number of characters inserted\n        Nc is the number of characters in the reference\n    We can use levenshtein distance to calculate CER. Chinese input should be\n    encoded to unicode. Please draw an attention that the leading and tailing\n    space characters will be truncated and multiple consecutive space\n    characters in a sentence will be replaced by one space character.\n    :param reference: The reference sentence.\n    :type reference: basestring\n    :param hypothesis: The hypothesis sentence.\n    :type hypothesis: basestring\n    :param ignore_case: Whether case-sensitive or not.\n    :type ignore_case: bool\n    :param remove_space: Whether remove internal space characters\n    :type remove_space: bool\n    :return: Character error rate.\n    :rtype: float\n    :raises ValueError: If the reference length is zero.\n    \"\"\"\n    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n                                         remove_space)\n\n    if ref_len == 0:\n        raise ValueError(\"Length of reference should be greater than 0.\")\n\n    cer = float(edit_distance) / ref_len\n    return cer\n\nclass TextTransform:\n    \"\"\"Maps characters to integers and vice versa\"\"\"\n    def __init__(self):\n        self.char_map = {\"а\": 0, \"б\": 1, \"в\": 2, \"г\": 3, \"д\": 4, \"е\": 5, \"ё\": 6, \"ж\": 7, \"з\": 8, \"и\": 9, \"й\": 10,\n                  \"к\": 11, \"л\": 12, \"м\": 13, \"н\": 14, \"о\": 15, \"п\": 16, \"р\": 17, \"с\": 18, \"т\": 19, \"у\": 20,\n                  \"ф\": 21, \"ч\": 22, \"ц\": 23, \"ш\": 24, \"щ\": 25, \"ъ\": 26, \"ы\": 27, \"ь\": 28, \"э\": 29, \"ю\": 30,\n                  \"я\": 31, \"х\": 32, \" \": 33}\n\n        self.index_map = {}\n        for key, value in self.char_map.items():\n            self.index_map[value] = key\n\n    def text_to_int(self, text):\n        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n        int_sequence = []\n        for c in text:\n            if c != '':\n                ch = self.char_map[c]\n            int_sequence.append(ch)\n        return int_sequence\n\n    def int_to_text(self, labels):\n        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n        string = []\n        for i in labels:\n            string.append(self.index_map[i])\n        return ''.join(string)\n\ntrain_audio_transforms = nn.Sequential(\n    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n    torchaudio.transforms.TimeMasking(time_mask_param=100)\n)\n\nvalid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n\ntext_transform = TextTransform()\n\ndef data_processing(data, data_type=\"train\"):\n    spectrograms = []\n    labels = []\n    input_lengths = []\n    label_lengths = []\n    for (waveform, utterance) in data:\n        if data_type == 'train':\n            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        elif data_type == 'valid':\n            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        else:\n            raise Exception('data_type should be train or valid')\n        spectrograms.append(spec)\n        label = torch.Tensor(text_transform.text_to_int(utterance))\n        labels.append(label)\n        input_lengths.append(spec.shape[0]//4)\n        label_lengths.append(len(label))\n    \n    spectrograms1 = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n            \n    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n\n    return spectrograms1, labels, input_lengths, label_lengths\n\n\ndef GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n    arg_maxes = torch.argmax(output, dim=2)\n    decodes = []\n    targets = []\n    for i, args in enumerate(arg_maxes):\n        decode = []\n        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n        for j, index in enumerate(args):\n            if index != blank_label:\n                if collapse_repeated and j != 0 and index == args[j -1]:\n                    continue\n                decode.append(index.item())\n        decodes.append(text_transform.int_to_text(decode))\n    return decodes, targets","metadata":{"execution":{"iopub.status.busy":"2023-04-26T16:40:55.945606Z","iopub.execute_input":"2023-04-26T16:40:55.946287Z","iopub.status.idle":"2023-04-26T16:40:56.063366Z","shell.execute_reply.started":"2023-04-26T16:40:55.946253Z","shell.execute_reply":"2023-04-26T16:40:56.062419Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchaudio/functional/functional.py:572: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n  \"At least one mel filterbank has all zero values. \"\n","output_type":"stream"}]},{"cell_type":"code","source":"class CNNLayerNorm(nn.Module):\n    \"\"\"Layer normalization built for cnns input\"\"\"\n    def __init__(self, n_feats):\n        super(CNNLayerNorm, self).__init__()\n        self.layer_norm = nn.LayerNorm(n_feats)\n\n    def forward(self, x):\n        # x (batch, channel, feature, time)\n        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n        x = self.layer_norm(x)\n        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n\n\nclass ResidualCNN(nn.Module):\n    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n        except with layer norm instead of batch norm\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n        super(ResidualCNN, self).__init__()\n\n        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.layer_norm1 = CNNLayerNorm(n_feats)\n        self.layer_norm2 = CNNLayerNorm(n_feats)\n\n    def forward(self, x):\n        residual = x  # (batch, channel, feature, time)\n        x = self.layer_norm1(x)\n        x = F.gelu(x)\n        x = self.dropout1(x)\n        x = self.cnn1(x)\n        x = self.layer_norm2(x)\n        x = F.gelu(x)\n        x = self.dropout2(x)\n        x = self.cnn2(x)\n        x += residual\n        return x # (batch, channel, feature, time)\n\n\nclass BidirectionalGRU(nn.Module):\n\n    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n        super(BidirectionalGRU, self).__init__()\n\n        self.BiGRU = nn.GRU(\n            input_size=rnn_dim, hidden_size=hidden_size,\n            num_layers=1, batch_first=batch_first, bidirectional=True)\n        self.layer_norm = nn.LayerNorm(rnn_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.layer_norm(x)\n        x = F.gelu(x)\n        x, _ = self.BiGRU(x)\n        x = self.dropout(x)\n        return x\n\n\nclass SpeechRecognitionModel(nn.Module):\n    \n    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n        super(SpeechRecognitionModel, self).__init__()\n        n_feats = n_feats//2\n        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n\n        # n residual cnn layers with filter size of 32\n        self.rescnn_layers = nn.Sequential(*[\n            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n            for _ in range(n_cnn_layers)\n        ])\n        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n        self.birnn_layers = nn.Sequential(*[\n            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n            for i in range(n_rnn_layers)\n        ])\n        self.classifier = nn.Sequential(\n            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(rnn_dim, n_class)\n        )\n\n    def forward(self, x):\n        x = self.cnn(x)\n        x = self.rescnn_layers(x)\n        sizes = x.size()\n        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n        x = x.transpose(1, 2) # (batch, time, feature)\n        x = self.fully_connected(x)\n        x = self.birnn_layers(x)\n        x = self.classifier(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-04-26T16:40:56.064966Z","iopub.execute_input":"2023-04-26T16:40:56.065344Z","iopub.status.idle":"2023-04-26T16:40:56.085600Z","shell.execute_reply.started":"2023-04-26T16:40:56.065305Z","shell.execute_reply":"2023-04-26T16:40:56.084457Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport librosa\n\nfile = pd.read_excel('/kaggle/input/rus-speech/russian_speech (1).xlsx')\ny = [sentence for sentence in file['Русская речь']]\n\ndir_name = \"/kaggle/input/upd-speech/abnormal_voice/\"\nfiles_in_dir = os.listdir(dir_name)\n\nX = []\ni = 1\n\nfor e in range(1, 1001):\n    file_name = f'{e}.wav'\n    sampl = librosa.load(dir_name + file_name, sr=16000)[0]\n    sampl = sampl[np.newaxis, :]\n    X.append(torch.Tensor(sampl))\n    if i % 100 == 0:\n        print(i)\n    i += 1","metadata":{"execution":{"iopub.status.busy":"2023-04-26T16:40:56.089717Z","iopub.execute_input":"2023-04-26T16:40:56.090248Z","iopub.status.idle":"2023-04-26T16:41:13.719395Z","shell.execute_reply.started":"2023-04-26T16:40:56.090220Z","shell.execute_reply":"2023-04-26T16:41:13.718215Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n","output_type":"stream"}]},{"cell_type":"code","source":"X[0].shape","metadata":{"execution":{"iopub.status.busy":"2023-04-26T16:41:13.721242Z","iopub.execute_input":"2023-04-26T16:41:13.721965Z","iopub.status.idle":"2023-04-26T16:41:13.730477Z","shell.execute_reply.started":"2023-04-26T16:41:13.721924Z","shell.execute_reply":"2023-04-26T16:41:13.729476Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 79872])"},"metadata":{}}]},{"cell_type":"code","source":"char_map = {\"а\": 0, \"б\": 1, \"в\": 2, \"г\": 3, \"д\": 4, \"е\": 5, \"ё\": 6, \"ж\": 7, \"з\": 8, \"и\": 9, \"й\": 10,\n            \"к\": 11, \"л\": 12, \"м\": 13, \"н\": 14, \"о\": 15, \"п\": 16, \"р\": 17, \"с\": 18, \"т\": 19, \"у\": 20,\n            \"ф\": 21, \"ч\": 22, \"ц\": 23, \"ш\": 24, \"щ\": 25, \"ъ\": 26, \"ы\": 27, \"ь\": 28, \"э\": 29, \"ю\": 30,\n            \"я\": 31, \"х\": 32, \" \": 33}\n\ndef remove_characters(sentence):\n    sentence = sentence.lower()\n    sentence = ''.join(filter(lambda x: x in char_map, sentence))\n    return sentence\n\ny = list(map(remove_characters, y))","metadata":{"execution":{"iopub.status.busy":"2023-04-26T16:41:13.731945Z","iopub.execute_input":"2023-04-26T16:41:13.732917Z","iopub.status.idle":"2023-04-26T16:41:13.756769Z","shell.execute_reply.started":"2023-04-26T16:41:13.732867Z","shell.execute_reply":"2023-04-26T16:41:13.755856Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)","metadata":{"execution":{"iopub.status.busy":"2023-04-26T16:41:13.758132Z","iopub.execute_input":"2023-04-26T16:41:13.758457Z","iopub.status.idle":"2023-04-26T16:41:13.862058Z","shell.execute_reply.started":"2023-04-26T16:41:13.758422Z","shell.execute_reply":"2023-04-26T16:41:13.861129Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass AudioDataset(Dataset):\n    def __init__(self, audio_list, text_list):\n        self.audio_list = audio_list\n        self.text_list = text_list\n        \n    def __len__(self):\n        return len(self.text_list)\n    \n    def __getitem__(self, index):\n        audio = self.audio_list[index]\n        text = self.text_list[index]\n        return audio, text","metadata":{"execution":{"iopub.status.busy":"2023-04-26T16:41:13.865106Z","iopub.execute_input":"2023-04-26T16:41:13.865379Z","iopub.status.idle":"2023-04-26T16:41:13.871851Z","shell.execute_reply.started":"2023-04-26T16:41:13.865348Z","shell.execute_reply":"2023-04-26T16:41:13.870704Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\"\"\"class SpeechRecognitionModel1(nn.Module):\n    def __init__(self, num_classes):\n        super(SpeechRecognitionModel1, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n            nn.BatchNorm2d(64),\n            nn.GELU(),\n            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)),\n            nn.Conv2d(64, 128, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n            nn.BatchNorm2d(128),\n            nn.GELU(),\n            nn.Conv2d(128, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n            nn.BatchNorm2d(64),\n            nn.GELU(),\n            nn.Conv2d(64, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n            nn.GELU(),\n            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)),\n            nn.Conv2d(64, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n            nn.GELU(),\n            nn.Conv2d(64, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n            nn.GELU(),\n        )\n        \n        self.rnn = nn.GRU(input_size=2048, \n                    hidden_size=256, \n                    num_layers=1, \n                    batch_first=True, \n                    bidirectional=True)\n        \n        self.fc = nn.Sequential(\n            nn.Linear(512, num_classes),\n        )\n        self.softmax = nn.LogSoftmax(dim=2)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x.permute(0, 3, 1, 2)\n        x = x.view(x.size(0), x.size(1), -1)\n        x, _ = self.rnn(x)\n        x = self.fc(x)\n        x = self.softmax(x)\n        return x\n    \"\"\"\n\"\"\"nn.Linear(512, 128),\n            nn.GELU(),\n            nn.Dropout(0.35),\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-04-26T16:41:13.875463Z","iopub.execute_input":"2023-04-26T16:41:13.875820Z","iopub.status.idle":"2023-04-26T16:41:13.884516Z","shell.execute_reply.started":"2023-04-26T16:41:13.875739Z","shell.execute_reply":"2023-04-26T16:41:13.883389Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'nn.Linear(512, 128),\\n            nn.GELU(),\\n            nn.Dropout(0.35),'"},"metadata":{}}]},{"cell_type":"code","source":"class SpeechRecognitionModel1(nn.Module):\n    def __init__(self, num_classes):\n        super(SpeechRecognitionModel1, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n            nn.BatchNorm2d(32),\n            nn.GELU(),\n            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)),\n            nn.Conv2d(32, 32, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n            nn.BatchNorm2d(32),\n            nn.GELU(),\n            nn.Conv2d(32, 32, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n            nn.BatchNorm2d(32),\n            nn.GELU(),\n            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)),\n        )\n        \n        self.rnn_1 = nn.LSTM(input_size=1024, \n                    hidden_size=128, \n                    num_layers=1, \n                    batch_first=True, \n                    bidirectional=True)\n        \n        self.dropout = nn.Dropout(0.3)\n        \n        self.rnn_2 = nn.GRU(input_size=256, \n                    hidden_size=256, \n                    num_layers=1, \n                    batch_first=True, \n                    bidirectional=True)\n        \n        self.fc = nn.Sequential(\n            nn.Linear(512, num_classes),\n        )\n        self.softmax = nn.LogSoftmax(dim=2)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x.permute(0, 3, 1, 2)\n        x = x.view(x.size(0), x.size(1), -1)\n        x, _ = self.rnn_1(x)\n        x = self.dropout(x)\n        x, _ = self.rnn_2(x)\n        x = self.dropout(x)\n        x = self.fc(x)\n        x = self.softmax(x)\n        return x\n    \n\"\"\"nn.Linear(512, 128),\n            nn.GELU(),\n            nn.Dropout(0.35),\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-04-26T16:41:13.886451Z","iopub.execute_input":"2023-04-26T16:41:13.886816Z","iopub.status.idle":"2023-04-26T16:41:13.906977Z","shell.execute_reply.started":"2023-04-26T16:41:13.886781Z","shell.execute_reply":"2023-04-26T16:41:13.905810Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'nn.Linear(512, 128),\\n            nn.GELU(),\\n            nn.Dropout(0.35),'"},"metadata":{}}]},{"cell_type":"code","source":"class IterMeter(object):\n    \"\"\"keeps track of total iterations\"\"\"\n    def __init__(self):\n        self.val = 0\n\n    def step(self):\n        self.val += 1\n\n    def get(self):\n        return self.val\n\n\ndef train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter):\n    model.train()\n    data_len = len(train_loader.dataset)\n    for batch_idx, _data in enumerate(train_loader):\n        spectrograms, labels, input_lengths, label_lengths = _data \n        spectrograms, labels = spectrograms.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        output = model(spectrograms)  # (batch, time, n_class)\n        #output = F.log_softmax(output, dim=2)\n        output = output.transpose(0, 1) # (time, batch, n_class)\n\n        loss = criterion(output, labels, input_lengths, label_lengths)\n        loss.backward()\n\n        optimizer.step()\n        scheduler.step()\n        iter_meter.step()\n        if batch_idx % 10 == 0 or batch_idx == data_len:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(spectrograms), data_len,\n                100. * batch_idx / len(train_loader), loss.item()))\n\n\ndef test(model, device, test_loader, criterion, epoch, iter_meter):\n    print('\\nevaluating...')\n    model.eval()\n    test_loss = 0\n    test_cer, test_wer = [], []\n    with torch.no_grad():\n        for i, _data in enumerate(test_loader):\n            spectrograms, labels, input_lengths, label_lengths = _data \n            spectrograms, labels = spectrograms.to(device), labels.to(device)\n\n            output = model(spectrograms)  # (batch, time, n_class)\n            #output = F.log_softmax(output, dim=2)\n            output = output.transpose(0, 1) # (time, batch, n_class)\n\n            loss = criterion(output, labels, input_lengths, label_lengths)\n            test_loss += loss.item() / len(test_loader)\n\n            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n            for j in range(len(decoded_preds)):\n                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n\n\n    avg_cer = sum(test_cer)/len(test_cer)\n    avg_wer = sum(test_wer)/len(test_wer)\n\n    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n\n\ndef main(learning_rate=5e-4, batch_size=20, epochs=10):\n\n    hparams = {\n        \"n_cnn_layers\": 2,\n        \"n_rnn_layers\": 2,\n        \"rnn_dim\": 256,\n        \"n_class\": 34,\n        \"n_feats\": 128,\n        \"stride\":2,\n        \"dropout\": 0.1,\n        \"learning_rate\": learning_rate,\n        \"batch_size\": batch_size,\n        \"epochs\": epochs\n    }\n\n    use_cuda = torch.cuda.is_available()\n    torch.manual_seed(7)\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    train_dataset = AudioDataset(X_train, y_train)\n    test_dataset = AudioDataset(X_test, y_test)\n\n    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n    train_loader = data.DataLoader(dataset=train_dataset,\n                                batch_size=hparams['batch_size'],\n                                shuffle=True,\n                                collate_fn=lambda x: data_processing(x, 'train'),\n                                **kwargs)\n    test_loader = data.DataLoader(dataset=test_dataset,\n                                batch_size=hparams['batch_size'],\n                                shuffle=False,\n                                collate_fn=lambda x: data_processing(x, 'valid'),\n                                **kwargs)\n\n    model = SpeechRecognitionModel1(34).to(device)\n\n    print(model)\n    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n\n    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n    criterion = nn.CTCLoss(blank=28).to(device)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n                                            steps_per_epoch=int(len(train_loader)),\n                                            epochs=hparams['epochs'],\n                                            anneal_strategy='linear')\n    \n    iter_meter = IterMeter()\n    for epoch in range(1, epochs + 1):\n        train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\n        test(model, device, test_loader, criterion, epoch, iter_meter)\n        \n    torch.save(model, '/kaggle/working/model.pt')","metadata":{"execution":{"iopub.status.busy":"2023-04-26T16:41:13.908921Z","iopub.execute_input":"2023-04-26T16:41:13.909400Z","iopub.status.idle":"2023-04-26T16:41:13.931438Z","shell.execute_reply.started":"2023-04-26T16:41:13.909364Z","shell.execute_reply":"2023-04-26T16:41:13.930389Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def predict(model, file_name, device):\n    model.eval()\n    spectro = []\n    valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n    \n    sampl = librosa.load(file_name, sr=16000)[0]\n    sampl = sampl[np.newaxis, :]\n    sampl = torch.Tensor(sampl)\n    spectr = valid_audio_transforms(sampl).squeeze(0)\n    spectrogram_tensor = spectr.unsqueeze(0).unsqueeze(0)\n    \n    \n    #model.eval()\n    with torch.no_grad():\n        spectrogram_tensor.to(device)\n        output = model(spectrogram_tensor)  # (batch, time, n_class)\n        #output = F.log_softmax(output, dim=2)\n        #output = output.transpose(0, 1) # (time, batch, n_class)\n        \n        arg_maxes = torch.argmax(output, dim=2)\n        decodes = []\n        for i, args in enumerate(arg_maxes):\n            decode = []\n            for j, index in enumerate(args):\n                if index != 28:\n                    if True and j != 0 and index == args[j -1]:\n                        continue\n                    decode.append(index.item())\n            decodes.append(text_transform.int_to_text(decode))\n\n    return decodes[0]","metadata":{"execution":{"iopub.status.busy":"2023-04-26T16:41:13.933147Z","iopub.execute_input":"2023-04-26T16:41:13.933495Z","iopub.status.idle":"2023-04-26T16:41:13.943779Z","shell.execute_reply.started":"2023-04-26T16:41:13.933461Z","shell.execute_reply":"2023-04-26T16:41:13.942802Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.0005\nbatch_size = 10\nepochs = 90\n\nmain(learning_rate, batch_size, epochs)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-04-26T16:41:13.945294Z","iopub.execute_input":"2023-04-26T16:41:13.945743Z","iopub.status.idle":"2023-04-26T16:48:00.401044Z","shell.execute_reply.started":"2023-04-26T16:41:13.945707Z","shell.execute_reply":"2023-04-26T16:48:00.399375Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"SpeechRecognitionModel1(\n  (conv): Sequential(\n    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): GELU(approximate='none')\n    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): GELU(approximate='none')\n    (7): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (9): GELU(approximate='none')\n    (10): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  )\n  (rnn_1): LSTM(1024, 128, batch_first=True, bidirectional=True)\n  (dropout): Dropout(p=0.3, inplace=False)\n  (rnn_2): GRU(256, 256, batch_first=True, bidirectional=True)\n  (fc): Sequential(\n    (0): Linear(in_features=512, out_features=34, bias=True)\n  )\n  (softmax): LogSoftmax(dim=2)\n)\nNum Model Parameters 2007650\nTrain Epoch: 1 [0/900 (0%)]\tLoss: 12.144131\nTrain Epoch: 1 [100/900 (11%)]\tLoss: 10.540736\nTrain Epoch: 1 [200/900 (22%)]\tLoss: 9.710093\nTrain Epoch: 1 [300/900 (33%)]\tLoss: 11.780849\nTrain Epoch: 1 [400/900 (44%)]\tLoss: 8.995000\nTrain Epoch: 1 [500/900 (56%)]\tLoss: 8.825212\nTrain Epoch: 1 [600/900 (67%)]\tLoss: 7.416411\nTrain Epoch: 1 [700/900 (78%)]\tLoss: 5.609946\nTrain Epoch: 1 [800/900 (89%)]\tLoss: 6.350759\n\nevaluating...\nTest set: Average loss: 3.5799, Average CER: 1.000000 Average WER: 0.9874\n\nTrain Epoch: 2 [0/900 (0%)]\tLoss: 3.687578\nTrain Epoch: 2 [100/900 (11%)]\tLoss: 3.753239\nTrain Epoch: 2 [200/900 (22%)]\tLoss: 3.733908\nTrain Epoch: 2 [300/900 (33%)]\tLoss: 3.425626\nTrain Epoch: 2 [400/900 (44%)]\tLoss: 3.560130\nTrain Epoch: 2 [500/900 (56%)]\tLoss: 3.339781\nTrain Epoch: 2 [600/900 (67%)]\tLoss: 3.500274\nTrain Epoch: 2 [700/900 (78%)]\tLoss: 3.167075\nTrain Epoch: 2 [800/900 (89%)]\tLoss: 3.251608\n\nevaluating...\nTest set: Average loss: 3.2900, Average CER: 1.000000 Average WER: 0.9874\n\nTrain Epoch: 3 [0/900 (0%)]\tLoss: 3.237594\nTrain Epoch: 3 [100/900 (11%)]\tLoss: 3.278447\nTrain Epoch: 3 [200/900 (22%)]\tLoss: 3.185313\nTrain Epoch: 3 [300/900 (33%)]\tLoss: 3.078689\nTrain Epoch: 3 [400/900 (44%)]\tLoss: 3.110842\nTrain Epoch: 3 [500/900 (56%)]\tLoss: 3.236173\nTrain Epoch: 3 [600/900 (67%)]\tLoss: 3.206577\nTrain Epoch: 3 [700/900 (78%)]\tLoss: 3.096081\nTrain Epoch: 3 [800/900 (89%)]\tLoss: 3.042829\n\nevaluating...\nTest set: Average loss: 3.1468, Average CER: 1.000000 Average WER: 0.9874\n\nTrain Epoch: 4 [0/900 (0%)]\tLoss: 3.260983\nTrain Epoch: 4 [100/900 (11%)]\tLoss: 3.200203\nTrain Epoch: 4 [200/900 (22%)]\tLoss: 3.288373\nTrain Epoch: 4 [300/900 (33%)]\tLoss: 3.162089\nTrain Epoch: 4 [400/900 (44%)]\tLoss: 3.177754\nTrain Epoch: 4 [500/900 (56%)]\tLoss: 2.914747\nTrain Epoch: 4 [600/900 (67%)]\tLoss: 3.108591\nTrain Epoch: 4 [700/900 (78%)]\tLoss: 3.017960\nTrain Epoch: 4 [800/900 (89%)]\tLoss: 3.250409\n\nevaluating...\nTest set: Average loss: 3.1276, Average CER: 1.000000 Average WER: 0.9874\n\nTrain Epoch: 5 [0/900 (0%)]\tLoss: 3.179666\nTrain Epoch: 5 [100/900 (11%)]\tLoss: 3.103897\nTrain Epoch: 5 [200/900 (22%)]\tLoss: 3.031028\nTrain Epoch: 5 [300/900 (33%)]\tLoss: 3.020064\nTrain Epoch: 5 [400/900 (44%)]\tLoss: 3.156178\nTrain Epoch: 5 [500/900 (56%)]\tLoss: 3.028203\nTrain Epoch: 5 [600/900 (67%)]\tLoss: 3.051250\nTrain Epoch: 5 [700/900 (78%)]\tLoss: 3.161592\nTrain Epoch: 5 [800/900 (89%)]\tLoss: 3.160919\n\nevaluating...\nTest set: Average loss: 3.0977, Average CER: 1.000000 Average WER: 0.9874\n\nTrain Epoch: 6 [0/900 (0%)]\tLoss: 3.176154\nTrain Epoch: 6 [100/900 (11%)]\tLoss: 3.250854\nTrain Epoch: 6 [200/900 (22%)]\tLoss: 3.067202\nTrain Epoch: 6 [300/900 (33%)]\tLoss: 3.057124\nTrain Epoch: 6 [400/900 (44%)]\tLoss: 3.127597\nTrain Epoch: 6 [500/900 (56%)]\tLoss: 3.075967\nTrain Epoch: 6 [600/900 (67%)]\tLoss: 3.199713\nTrain Epoch: 6 [700/900 (78%)]\tLoss: 3.057626\nTrain Epoch: 6 [800/900 (89%)]\tLoss: 3.036631\n\nevaluating...\nTest set: Average loss: 3.0759, Average CER: 1.000000 Average WER: 0.9874\n\nTrain Epoch: 7 [0/900 (0%)]\tLoss: 3.145959\nTrain Epoch: 7 [100/900 (11%)]\tLoss: 3.270927\nTrain Epoch: 7 [200/900 (22%)]\tLoss: 3.100106\nTrain Epoch: 7 [300/900 (33%)]\tLoss: 3.092284\nTrain Epoch: 7 [400/900 (44%)]\tLoss: 3.165104\nTrain Epoch: 7 [500/900 (56%)]\tLoss: 3.095162\nTrain Epoch: 7 [600/900 (67%)]\tLoss: 3.065698\nTrain Epoch: 7 [700/900 (78%)]\tLoss: 3.135576\nTrain Epoch: 7 [800/900 (89%)]\tLoss: 3.082971\n\nevaluating...\nTest set: Average loss: 3.0389, Average CER: 1.000000 Average WER: 0.9874\n\nTrain Epoch: 8 [0/900 (0%)]\tLoss: 3.178641\nTrain Epoch: 8 [100/900 (11%)]\tLoss: 3.064715\nTrain Epoch: 8 [200/900 (22%)]\tLoss: 2.985725\nTrain Epoch: 8 [300/900 (33%)]\tLoss: 3.055797\nTrain Epoch: 8 [400/900 (44%)]\tLoss: 2.982682\nTrain Epoch: 8 [500/900 (56%)]\tLoss: 2.916649\nTrain Epoch: 8 [600/900 (67%)]\tLoss: 2.883811\nTrain Epoch: 8 [700/900 (78%)]\tLoss: 2.986488\nTrain Epoch: 8 [800/900 (89%)]\tLoss: 3.050986\n\nevaluating...\nTest set: Average loss: 3.0014, Average CER: 1.000000 Average WER: 0.9874\n\nTrain Epoch: 9 [0/900 (0%)]\tLoss: 3.260900\nTrain Epoch: 9 [100/900 (11%)]\tLoss: 3.109835\nTrain Epoch: 9 [200/900 (22%)]\tLoss: 3.045377\nTrain Epoch: 9 [300/900 (33%)]\tLoss: 3.017136\nTrain Epoch: 9 [400/900 (44%)]\tLoss: 3.081427\nTrain Epoch: 9 [500/900 (56%)]\tLoss: 3.181409\nTrain Epoch: 9 [600/900 (67%)]\tLoss: 3.074780\nTrain Epoch: 9 [700/900 (78%)]\tLoss: 3.002320\nTrain Epoch: 9 [800/900 (89%)]\tLoss: 2.841599\n\nevaluating...\nTest set: Average loss: 2.9429, Average CER: 1.000000 Average WER: 0.9874\n\nTrain Epoch: 10 [0/900 (0%)]\tLoss: 3.095570\nTrain Epoch: 10 [100/900 (11%)]\tLoss: 3.101183\nTrain Epoch: 10 [200/900 (22%)]\tLoss: 3.082800\nTrain Epoch: 10 [300/900 (33%)]\tLoss: 3.095859\nTrain Epoch: 10 [400/900 (44%)]\tLoss: 2.818727\nTrain Epoch: 10 [500/900 (56%)]\tLoss: 2.936987\nTrain Epoch: 10 [600/900 (67%)]\tLoss: 3.021893\nTrain Epoch: 10 [700/900 (78%)]\tLoss: 2.840192\nTrain Epoch: 10 [800/900 (89%)]\tLoss: 2.947720\n\nevaluating...\nTest set: Average loss: 2.8573, Average CER: 1.000000 Average WER: 0.9874\n\nTrain Epoch: 11 [0/900 (0%)]\tLoss: 2.973801\nTrain Epoch: 11 [100/900 (11%)]\tLoss: 2.968144\nTrain Epoch: 11 [200/900 (22%)]\tLoss: 2.951910\nTrain Epoch: 11 [300/900 (33%)]\tLoss: 2.965942\nTrain Epoch: 11 [400/900 (44%)]\tLoss: 2.777064\nTrain Epoch: 11 [500/900 (56%)]\tLoss: 3.059692\nTrain Epoch: 11 [600/900 (67%)]\tLoss: 2.854475\nTrain Epoch: 11 [700/900 (78%)]\tLoss: 3.040049\nTrain Epoch: 11 [800/900 (89%)]\tLoss: 2.967410\n\nevaluating...\nTest set: Average loss: 2.7224, Average CER: 0.986207 Average WER: 0.9923\n\nTrain Epoch: 12 [0/900 (0%)]\tLoss: 2.798879\nTrain Epoch: 12 [100/900 (11%)]\tLoss: 2.921168\nTrain Epoch: 12 [200/900 (22%)]\tLoss: 2.645371\nTrain Epoch: 12 [300/900 (33%)]\tLoss: 2.757503\nTrain Epoch: 12 [400/900 (44%)]\tLoss: 2.827107\nTrain Epoch: 12 [500/900 (56%)]\tLoss: 3.041464\nTrain Epoch: 12 [600/900 (67%)]\tLoss: 3.092680\nTrain Epoch: 12 [700/900 (78%)]\tLoss: 2.798975\nTrain Epoch: 12 [800/900 (89%)]\tLoss: 2.773329\n\nevaluating...\nTest set: Average loss: 2.5827, Average CER: 0.902225 Average WER: 0.9980\n\nTrain Epoch: 13 [0/900 (0%)]\tLoss: 2.787568\nTrain Epoch: 13 [100/900 (11%)]\tLoss: 2.800033\nTrain Epoch: 13 [200/900 (22%)]\tLoss: 2.870836\nTrain Epoch: 13 [300/900 (33%)]\tLoss: 2.756358\nTrain Epoch: 13 [400/900 (44%)]\tLoss: 2.689439\nTrain Epoch: 13 [500/900 (56%)]\tLoss: 2.856939\nTrain Epoch: 13 [600/900 (67%)]\tLoss: 2.776143\nTrain Epoch: 13 [700/900 (78%)]\tLoss: 2.750968\nTrain Epoch: 13 [800/900 (89%)]\tLoss: 2.652879\n\nevaluating...\nTest set: Average loss: 2.4200, Average CER: 0.788966 Average WER: 1.0000\n\nTrain Epoch: 14 [0/900 (0%)]\tLoss: 2.633570\nTrain Epoch: 14 [100/900 (11%)]\tLoss: 2.544800\nTrain Epoch: 14 [200/900 (22%)]\tLoss: 2.582352\nTrain Epoch: 14 [300/900 (33%)]\tLoss: 2.705674\nTrain Epoch: 14 [400/900 (44%)]\tLoss: 2.573128\nTrain Epoch: 14 [500/900 (56%)]\tLoss: 2.565179\nTrain Epoch: 14 [600/900 (67%)]\tLoss: 2.666746\nTrain Epoch: 14 [700/900 (78%)]\tLoss: 2.438398\nTrain Epoch: 14 [800/900 (89%)]\tLoss: 2.545409\n\nevaluating...\nTest set: Average loss: 2.2606, Average CER: 0.674906 Average WER: 1.0286\n\nTrain Epoch: 15 [0/900 (0%)]\tLoss: 2.537024\nTrain Epoch: 15 [100/900 (11%)]\tLoss: 2.475774\nTrain Epoch: 15 [200/900 (22%)]\tLoss: 2.714464\nTrain Epoch: 15 [300/900 (33%)]\tLoss: 2.720454\nTrain Epoch: 15 [400/900 (44%)]\tLoss: 2.598650\nTrain Epoch: 15 [500/900 (56%)]\tLoss: 2.763407\nTrain Epoch: 15 [600/900 (67%)]\tLoss: 2.552839\nTrain Epoch: 15 [700/900 (78%)]\tLoss: 2.569804\nTrain Epoch: 15 [800/900 (89%)]\tLoss: 2.621334\n\nevaluating...\nTest set: Average loss: 2.1043, Average CER: 0.637847 Average WER: 1.0025\n\nTrain Epoch: 16 [0/900 (0%)]\tLoss: 2.538396\nTrain Epoch: 16 [100/900 (11%)]\tLoss: 2.522269\nTrain Epoch: 16 [200/900 (22%)]\tLoss: 2.374165\nTrain Epoch: 16 [300/900 (33%)]\tLoss: 2.268822\nTrain Epoch: 16 [400/900 (44%)]\tLoss: 2.430693\nTrain Epoch: 16 [500/900 (56%)]\tLoss: 2.457065\nTrain Epoch: 16 [600/900 (67%)]\tLoss: 2.354683\nTrain Epoch: 16 [700/900 (78%)]\tLoss: 2.457106\nTrain Epoch: 16 [800/900 (89%)]\tLoss: 2.357141\n\nevaluating...\nTest set: Average loss: 1.9217, Average CER: 0.582405 Average WER: 1.0200\n\nTrain Epoch: 17 [0/900 (0%)]\tLoss: 1.912991\nTrain Epoch: 17 [100/900 (11%)]\tLoss: 2.208433\nTrain Epoch: 17 [200/900 (22%)]\tLoss: 2.103534\nTrain Epoch: 17 [300/900 (33%)]\tLoss: 2.441187\nTrain Epoch: 17 [400/900 (44%)]\tLoss: 2.501872\nTrain Epoch: 17 [500/900 (56%)]\tLoss: 2.133454\nTrain Epoch: 17 [600/900 (67%)]\tLoss: 2.090738\nTrain Epoch: 17 [700/900 (78%)]\tLoss: 2.312529\nTrain Epoch: 17 [800/900 (89%)]\tLoss: 2.138307\n\nevaluating...\nTest set: Average loss: 1.8220, Average CER: 0.576025 Average WER: 1.0150\n\nTrain Epoch: 18 [0/900 (0%)]\tLoss: 2.001846\nTrain Epoch: 18 [100/900 (11%)]\tLoss: 2.261444\nTrain Epoch: 18 [200/900 (22%)]\tLoss: 2.106265\nTrain Epoch: 18 [300/900 (33%)]\tLoss: 2.061776\nTrain Epoch: 18 [400/900 (44%)]\tLoss: 2.367770\nTrain Epoch: 18 [500/900 (56%)]\tLoss: 2.305993\nTrain Epoch: 18 [600/900 (67%)]\tLoss: 2.238322\nTrain Epoch: 18 [700/900 (78%)]\tLoss: 2.085728\nTrain Epoch: 18 [800/900 (89%)]\tLoss: 2.072873\n\nevaluating...\nTest set: Average loss: 1.7309, Average CER: 0.534674 Average WER: 1.0083\n\nTrain Epoch: 19 [0/900 (0%)]\tLoss: 1.851777\nTrain Epoch: 19 [100/900 (11%)]\tLoss: 2.139895\nTrain Epoch: 19 [200/900 (22%)]\tLoss: 2.250212\nTrain Epoch: 19 [300/900 (33%)]\tLoss: 1.766258\nTrain Epoch: 19 [400/900 (44%)]\tLoss: 2.148584\nTrain Epoch: 19 [500/900 (56%)]\tLoss: 2.147421\nTrain Epoch: 19 [600/900 (67%)]\tLoss: 2.262553\nTrain Epoch: 19 [700/900 (78%)]\tLoss: 2.116026\nTrain Epoch: 19 [800/900 (89%)]\tLoss: 2.030098\n\nevaluating...\nTest set: Average loss: 1.7028, Average CER: 0.536362 Average WER: 1.0136\n\nTrain Epoch: 20 [0/900 (0%)]\tLoss: 2.125029\nTrain Epoch: 20 [100/900 (11%)]\tLoss: 2.276651\nTrain Epoch: 20 [200/900 (22%)]\tLoss: 2.440467\nTrain Epoch: 20 [300/900 (33%)]\tLoss: 2.164857\nTrain Epoch: 20 [400/900 (44%)]\tLoss: 2.011874\nTrain Epoch: 20 [500/900 (56%)]\tLoss: 2.413667\nTrain Epoch: 20 [600/900 (67%)]\tLoss: 2.051757\nTrain Epoch: 20 [700/900 (78%)]\tLoss: 2.076931\nTrain Epoch: 20 [800/900 (89%)]\tLoss: 1.889872\n\nevaluating...\nTest set: Average loss: 1.6108, Average CER: 0.492124 Average WER: 1.0125\n\nTrain Epoch: 21 [0/900 (0%)]\tLoss: 1.685535\nTrain Epoch: 21 [100/900 (11%)]\tLoss: 1.906868\nTrain Epoch: 21 [200/900 (22%)]\tLoss: 1.948424\nTrain Epoch: 21 [300/900 (33%)]\tLoss: 1.847893\nTrain Epoch: 21 [400/900 (44%)]\tLoss: 1.970652\nTrain Epoch: 21 [500/900 (56%)]\tLoss: 2.007228\nTrain Epoch: 21 [600/900 (67%)]\tLoss: 2.246033\nTrain Epoch: 21 [700/900 (78%)]\tLoss: 2.164017\nTrain Epoch: 21 [800/900 (89%)]\tLoss: 1.886865\n\nevaluating...\nTest set: Average loss: 1.5504, Average CER: 0.481123 Average WER: 1.0061\n\nTrain Epoch: 22 [0/900 (0%)]\tLoss: 1.838190\nTrain Epoch: 22 [100/900 (11%)]\tLoss: 1.795507\nTrain Epoch: 22 [200/900 (22%)]\tLoss: 1.957183\nTrain Epoch: 22 [300/900 (33%)]\tLoss: 1.865445\nTrain Epoch: 22 [400/900 (44%)]\tLoss: 2.124315\nTrain Epoch: 22 [500/900 (56%)]\tLoss: 1.917001\nTrain Epoch: 22 [600/900 (67%)]\tLoss: 2.204135\nTrain Epoch: 22 [700/900 (78%)]\tLoss: 1.725441\nTrain Epoch: 22 [800/900 (89%)]\tLoss: 1.732441\n\nevaluating...\nTest set: Average loss: 1.5144, Average CER: 0.487038 Average WER: 1.0038\n\nTrain Epoch: 23 [0/900 (0%)]\tLoss: 1.875436\nTrain Epoch: 23 [100/900 (11%)]\tLoss: 2.069429\nTrain Epoch: 23 [200/900 (22%)]\tLoss: 1.645757\nTrain Epoch: 23 [300/900 (33%)]\tLoss: 1.820528\nTrain Epoch: 23 [400/900 (44%)]\tLoss: 1.923110\nTrain Epoch: 23 [500/900 (56%)]\tLoss: 1.693473\nTrain Epoch: 23 [600/900 (67%)]\tLoss: 1.812407\nTrain Epoch: 23 [700/900 (78%)]\tLoss: 2.005278\nTrain Epoch: 23 [800/900 (89%)]\tLoss: 1.721190\n\nevaluating...\nTest set: Average loss: 1.4589, Average CER: 0.459842 Average WER: 1.0138\n\nTrain Epoch: 24 [0/900 (0%)]\tLoss: 1.858705\nTrain Epoch: 24 [100/900 (11%)]\tLoss: 1.522231\nTrain Epoch: 24 [200/900 (22%)]\tLoss: 1.707723\nTrain Epoch: 24 [300/900 (33%)]\tLoss: 1.732652\nTrain Epoch: 24 [400/900 (44%)]\tLoss: 1.863308\nTrain Epoch: 24 [500/900 (56%)]\tLoss: 1.995616\nTrain Epoch: 24 [600/900 (67%)]\tLoss: 1.684825\nTrain Epoch: 24 [700/900 (78%)]\tLoss: 1.648924\nTrain Epoch: 24 [800/900 (89%)]\tLoss: 1.757933\n\nevaluating...\nTest set: Average loss: 1.4339, Average CER: 0.459194 Average WER: 1.0031\n\nTrain Epoch: 25 [0/900 (0%)]\tLoss: 1.348781\nTrain Epoch: 25 [100/900 (11%)]\tLoss: 1.724567\nTrain Epoch: 25 [200/900 (22%)]\tLoss: 2.155658\nTrain Epoch: 25 [300/900 (33%)]\tLoss: 1.873944\nTrain Epoch: 25 [400/900 (44%)]\tLoss: 1.810943\nTrain Epoch: 25 [500/900 (56%)]\tLoss: 1.887093\nTrain Epoch: 25 [600/900 (67%)]\tLoss: 1.656875\nTrain Epoch: 25 [700/900 (78%)]\tLoss: 1.621418\nTrain Epoch: 25 [800/900 (89%)]\tLoss: 1.788139\n\nevaluating...\nTest set: Average loss: 1.3815, Average CER: 0.430991 Average WER: 1.0070\n\nTrain Epoch: 26 [0/900 (0%)]\tLoss: 1.602218\nTrain Epoch: 26 [100/900 (11%)]\tLoss: 1.852275\nTrain Epoch: 26 [200/900 (22%)]\tLoss: 1.826671\nTrain Epoch: 26 [300/900 (33%)]\tLoss: 1.752720\nTrain Epoch: 26 [400/900 (44%)]\tLoss: 1.778238\nTrain Epoch: 26 [500/900 (56%)]\tLoss: 1.691484\nTrain Epoch: 26 [600/900 (67%)]\tLoss: 1.618342\nTrain Epoch: 26 [700/900 (78%)]\tLoss: 1.443797\nTrain Epoch: 26 [800/900 (89%)]\tLoss: 1.796798\n\nevaluating...\nTest set: Average loss: 1.3088, Average CER: 0.401169 Average WER: 0.9992\n\nTrain Epoch: 27 [0/900 (0%)]\tLoss: 2.305255\nTrain Epoch: 27 [100/900 (11%)]\tLoss: 1.979868\nTrain Epoch: 27 [200/900 (22%)]\tLoss: 2.015200\nTrain Epoch: 27 [300/900 (33%)]\tLoss: 1.522438\nTrain Epoch: 27 [400/900 (44%)]\tLoss: 1.875903\nTrain Epoch: 27 [500/900 (56%)]\tLoss: 1.340171\nTrain Epoch: 27 [600/900 (67%)]\tLoss: 1.647112\nTrain Epoch: 27 [700/900 (78%)]\tLoss: 1.777720\nTrain Epoch: 27 [800/900 (89%)]\tLoss: 1.723627\n\nevaluating...\nTest set: Average loss: 1.3177, Average CER: 0.410852 Average WER: 1.0025\n\nTrain Epoch: 28 [0/900 (0%)]\tLoss: 1.618301\nTrain Epoch: 28 [100/900 (11%)]\tLoss: 1.774397\nTrain Epoch: 28 [200/900 (22%)]\tLoss: 1.270332\nTrain Epoch: 28 [300/900 (33%)]\tLoss: 1.581091\nTrain Epoch: 28 [400/900 (44%)]\tLoss: 1.796545\nTrain Epoch: 28 [500/900 (56%)]\tLoss: 1.842694\nTrain Epoch: 28 [600/900 (67%)]\tLoss: 1.905335\nTrain Epoch: 28 [700/900 (78%)]\tLoss: 1.579352\nTrain Epoch: 28 [800/900 (89%)]\tLoss: 1.885922\n\nevaluating...\nTest set: Average loss: 1.2821, Average CER: 0.402049 Average WER: 1.0064\n\nTrain Epoch: 29 [0/900 (0%)]\tLoss: 1.706683\nTrain Epoch: 29 [100/900 (11%)]\tLoss: 1.269407\nTrain Epoch: 29 [200/900 (22%)]\tLoss: 1.733562\nTrain Epoch: 29 [300/900 (33%)]\tLoss: 1.995959\nTrain Epoch: 29 [400/900 (44%)]\tLoss: 1.732940\nTrain Epoch: 29 [500/900 (56%)]\tLoss: 1.831049\nTrain Epoch: 29 [600/900 (67%)]\tLoss: 2.019836\nTrain Epoch: 29 [700/900 (78%)]\tLoss: 1.469640\nTrain Epoch: 29 [800/900 (89%)]\tLoss: 1.649378\n\nevaluating...\nTest set: Average loss: 1.7225, Average CER: 0.518297 Average WER: 0.9932\n\nTrain Epoch: 30 [0/900 (0%)]\tLoss: 1.496737\nTrain Epoch: 30 [100/900 (11%)]\tLoss: 1.509173\nTrain Epoch: 30 [200/900 (22%)]\tLoss: 1.500144\nTrain Epoch: 30 [300/900 (33%)]\tLoss: 1.577368\nTrain Epoch: 30 [400/900 (44%)]\tLoss: 1.472853\nTrain Epoch: 30 [500/900 (56%)]\tLoss: 1.468623\nTrain Epoch: 30 [600/900 (67%)]\tLoss: 1.640058\nTrain Epoch: 30 [700/900 (78%)]\tLoss: 1.694431\nTrain Epoch: 30 [800/900 (89%)]\tLoss: 1.567720\n\nevaluating...\nTest set: Average loss: 1.4360, Average CER: 0.455565 Average WER: 1.0130\n\nTrain Epoch: 31 [0/900 (0%)]\tLoss: 1.477130\nTrain Epoch: 31 [100/900 (11%)]\tLoss: 1.622063\nTrain Epoch: 31 [200/900 (22%)]\tLoss: 1.777339\nTrain Epoch: 31 [300/900 (33%)]\tLoss: 1.567553\nTrain Epoch: 31 [400/900 (44%)]\tLoss: 1.261666\nTrain Epoch: 31 [500/900 (56%)]\tLoss: 1.471904\nTrain Epoch: 31 [600/900 (67%)]\tLoss: 1.550656\nTrain Epoch: 31 [700/900 (78%)]\tLoss: 1.283540\nTrain Epoch: 31 [800/900 (89%)]\tLoss: 1.386464\n\nevaluating...\nTest set: Average loss: 1.2480, Average CER: 0.394709 Average WER: 1.0072\n\nTrain Epoch: 32 [0/900 (0%)]\tLoss: 1.398203\nTrain Epoch: 32 [100/900 (11%)]\tLoss: 1.568894\nTrain Epoch: 32 [200/900 (22%)]\tLoss: 1.652448\nTrain Epoch: 32 [300/900 (33%)]\tLoss: 1.536855\nTrain Epoch: 32 [400/900 (44%)]\tLoss: 1.396231\nTrain Epoch: 32 [500/900 (56%)]\tLoss: 1.821484\nTrain Epoch: 32 [600/900 (67%)]\tLoss: 1.411644\nTrain Epoch: 32 [700/900 (78%)]\tLoss: 1.356974\nTrain Epoch: 32 [800/900 (89%)]\tLoss: 1.888158\n\nevaluating...\nTest set: Average loss: 1.1621, Average CER: 0.354598 Average WER: 0.9839\n\nTrain Epoch: 33 [0/900 (0%)]\tLoss: 1.705670\nTrain Epoch: 33 [100/900 (11%)]\tLoss: 1.467689\nTrain Epoch: 33 [200/900 (22%)]\tLoss: 1.552200\nTrain Epoch: 33 [300/900 (33%)]\tLoss: 1.549715\nTrain Epoch: 33 [400/900 (44%)]\tLoss: 1.184923\nTrain Epoch: 33 [500/900 (56%)]\tLoss: 1.508283\nTrain Epoch: 33 [600/900 (67%)]\tLoss: 1.394579\nTrain Epoch: 33 [700/900 (78%)]\tLoss: 1.167980\nTrain Epoch: 33 [800/900 (89%)]\tLoss: 1.964222\n\nevaluating...\nTest set: Average loss: 1.5431, Average CER: 0.467126 Average WER: 0.9867\n\nTrain Epoch: 34 [0/900 (0%)]\tLoss: 1.305834\nTrain Epoch: 34 [100/900 (11%)]\tLoss: 1.335294\nTrain Epoch: 34 [200/900 (22%)]\tLoss: 1.345389\nTrain Epoch: 34 [300/900 (33%)]\tLoss: 1.344410\nTrain Epoch: 34 [400/900 (44%)]\tLoss: 1.760174\nTrain Epoch: 34 [500/900 (56%)]\tLoss: 1.697563\nTrain Epoch: 34 [600/900 (67%)]\tLoss: 1.745520\nTrain Epoch: 34 [700/900 (78%)]\tLoss: 1.627993\nTrain Epoch: 34 [800/900 (89%)]\tLoss: 1.826163\n\nevaluating...\nTest set: Average loss: 1.1415, Average CER: 0.357375 Average WER: 0.9729\n\nTrain Epoch: 35 [0/900 (0%)]\tLoss: 1.598688\nTrain Epoch: 35 [100/900 (11%)]\tLoss: 1.407001\nTrain Epoch: 35 [200/900 (22%)]\tLoss: 1.389674\nTrain Epoch: 35 [300/900 (33%)]\tLoss: 1.409819\nTrain Epoch: 35 [400/900 (44%)]\tLoss: 1.481013\nTrain Epoch: 35 [500/900 (56%)]\tLoss: 1.379881\nTrain Epoch: 35 [600/900 (67%)]\tLoss: 1.520870\nTrain Epoch: 35 [700/900 (78%)]\tLoss: 1.524659\nTrain Epoch: 35 [800/900 (89%)]\tLoss: 1.403364\n\nevaluating...\nTest set: Average loss: 1.1411, Average CER: 0.349682 Average WER: 0.9720\n\nTrain Epoch: 36 [0/900 (0%)]\tLoss: 1.441041\nTrain Epoch: 36 [100/900 (11%)]\tLoss: 1.700502\nTrain Epoch: 36 [200/900 (22%)]\tLoss: 1.698648\nTrain Epoch: 36 [300/900 (33%)]\tLoss: 1.252588\nTrain Epoch: 36 [400/900 (44%)]\tLoss: 1.516813\nTrain Epoch: 36 [500/900 (56%)]\tLoss: 1.540052\nTrain Epoch: 36 [600/900 (67%)]\tLoss: 1.458424\nTrain Epoch: 36 [700/900 (78%)]\tLoss: 1.598029\nTrain Epoch: 36 [800/900 (89%)]\tLoss: 1.578847\n\nevaluating...\nTest set: Average loss: 1.1411, Average CER: 0.339136 Average WER: 0.9658\n\nTrain Epoch: 37 [0/900 (0%)]\tLoss: 1.328003\nTrain Epoch: 37 [100/900 (11%)]\tLoss: 1.586088\nTrain Epoch: 37 [200/900 (22%)]\tLoss: 1.334676\nTrain Epoch: 37 [300/900 (33%)]\tLoss: 1.292513\nTrain Epoch: 37 [400/900 (44%)]\tLoss: 1.462246\nTrain Epoch: 37 [500/900 (56%)]\tLoss: 1.440769\nTrain Epoch: 37 [600/900 (67%)]\tLoss: 1.728408\nTrain Epoch: 37 [700/900 (78%)]\tLoss: 1.094916\nTrain Epoch: 37 [800/900 (89%)]\tLoss: 1.463170\n\nevaluating...\nTest set: Average loss: 1.2190, Average CER: 0.393407 Average WER: 1.0012\n\nTrain Epoch: 38 [0/900 (0%)]\tLoss: 1.644980\nTrain Epoch: 38 [100/900 (11%)]\tLoss: 1.623795\nTrain Epoch: 38 [200/900 (22%)]\tLoss: 1.315784\nTrain Epoch: 38 [300/900 (33%)]\tLoss: 1.169903\nTrain Epoch: 38 [400/900 (44%)]\tLoss: 1.105755\nTrain Epoch: 38 [500/900 (56%)]\tLoss: 1.620692\nTrain Epoch: 38 [600/900 (67%)]\tLoss: 1.223107\nTrain Epoch: 38 [700/900 (78%)]\tLoss: 1.026900\nTrain Epoch: 38 [800/900 (89%)]\tLoss: 1.371592\n\nevaluating...\nTest set: Average loss: 1.1021, Average CER: 0.334558 Average WER: 0.9760\n\nTrain Epoch: 39 [0/900 (0%)]\tLoss: 1.389813\nTrain Epoch: 39 [100/900 (11%)]\tLoss: 1.719768\nTrain Epoch: 39 [200/900 (22%)]\tLoss: 0.860774\nTrain Epoch: 39 [300/900 (33%)]\tLoss: 1.600214\nTrain Epoch: 39 [400/900 (44%)]\tLoss: 1.254667\nTrain Epoch: 39 [500/900 (56%)]\tLoss: 1.346704\nTrain Epoch: 39 [600/900 (67%)]\tLoss: 1.303616\nTrain Epoch: 39 [700/900 (78%)]\tLoss: 1.070130\nTrain Epoch: 39 [800/900 (89%)]\tLoss: 1.536028\n\nevaluating...\nTest set: Average loss: 1.1079, Average CER: 0.331849 Average WER: 0.9780\n\nTrain Epoch: 40 [0/900 (0%)]\tLoss: 1.306080\nTrain Epoch: 40 [100/900 (11%)]\tLoss: 1.130359\nTrain Epoch: 40 [200/900 (22%)]\tLoss: 1.277138\nTrain Epoch: 40 [300/900 (33%)]\tLoss: 1.401568\nTrain Epoch: 40 [400/900 (44%)]\tLoss: 1.388760\nTrain Epoch: 40 [500/900 (56%)]\tLoss: 1.271054\nTrain Epoch: 40 [600/900 (67%)]\tLoss: 1.085459\nTrain Epoch: 40 [700/900 (78%)]\tLoss: 1.332488\nTrain Epoch: 40 [800/900 (89%)]\tLoss: 1.249119\n\nevaluating...\nTest set: Average loss: 1.1108, Average CER: 0.349824 Average WER: 0.9574\n\nTrain Epoch: 41 [0/900 (0%)]\tLoss: 1.357371\nTrain Epoch: 41 [100/900 (11%)]\tLoss: 1.195169\nTrain Epoch: 41 [200/900 (22%)]\tLoss: 1.458820\nTrain Epoch: 41 [300/900 (33%)]\tLoss: 1.440994\nTrain Epoch: 41 [400/900 (44%)]\tLoss: 1.634282\nTrain Epoch: 41 [500/900 (56%)]\tLoss: 1.702777\nTrain Epoch: 41 [600/900 (67%)]\tLoss: 1.078686\nTrain Epoch: 41 [700/900 (78%)]\tLoss: 1.569973\nTrain Epoch: 41 [800/900 (89%)]\tLoss: 2.244997\n\nevaluating...\nTest set: Average loss: 1.0883, Average CER: 0.320311 Average WER: 0.9769\n\nTrain Epoch: 42 [0/900 (0%)]\tLoss: 1.945981\nTrain Epoch: 42 [100/900 (11%)]\tLoss: 1.083511\nTrain Epoch: 42 [200/900 (22%)]\tLoss: 1.208156\nTrain Epoch: 42 [300/900 (33%)]\tLoss: 1.216740\nTrain Epoch: 42 [400/900 (44%)]\tLoss: 1.723415\nTrain Epoch: 42 [500/900 (56%)]\tLoss: 1.214407\nTrain Epoch: 42 [600/900 (67%)]\tLoss: 1.285242\nTrain Epoch: 42 [700/900 (78%)]\tLoss: 1.496367\nTrain Epoch: 42 [800/900 (89%)]\tLoss: 1.339756\n\nevaluating...\nTest set: Average loss: 1.0779, Average CER: 0.330952 Average WER: 0.9600\n\nTrain Epoch: 43 [0/900 (0%)]\tLoss: 2.003309\nTrain Epoch: 43 [100/900 (11%)]\tLoss: 1.650846\nTrain Epoch: 43 [200/900 (22%)]\tLoss: 1.477077\nTrain Epoch: 43 [300/900 (33%)]\tLoss: 1.405498\nTrain Epoch: 43 [400/900 (44%)]\tLoss: 1.131807\nTrain Epoch: 43 [500/900 (56%)]\tLoss: 1.873641\nTrain Epoch: 43 [600/900 (67%)]\tLoss: 1.439270\nTrain Epoch: 43 [700/900 (78%)]\tLoss: 1.195574\nTrain Epoch: 43 [800/900 (89%)]\tLoss: 1.219837\n\nevaluating...\nTest set: Average loss: 1.1036, Average CER: 0.343677 Average WER: 0.9808\n\nTrain Epoch: 44 [0/900 (0%)]\tLoss: 1.816038\nTrain Epoch: 44 [100/900 (11%)]\tLoss: 1.121382\nTrain Epoch: 44 [200/900 (22%)]\tLoss: 0.940517\nTrain Epoch: 44 [300/900 (33%)]\tLoss: 1.393806\nTrain Epoch: 44 [400/900 (44%)]\tLoss: 1.542386\nTrain Epoch: 44 [500/900 (56%)]\tLoss: 1.021142\nTrain Epoch: 44 [600/900 (67%)]\tLoss: 1.438230\nTrain Epoch: 44 [700/900 (78%)]\tLoss: 1.696005\nTrain Epoch: 44 [800/900 (89%)]\tLoss: 1.421007\n\nevaluating...\nTest set: Average loss: 1.0595, Average CER: 0.318846 Average WER: 0.9519\n\nTrain Epoch: 45 [0/900 (0%)]\tLoss: 1.236730\nTrain Epoch: 45 [100/900 (11%)]\tLoss: 1.212132\nTrain Epoch: 45 [200/900 (22%)]\tLoss: 1.025819\nTrain Epoch: 45 [300/900 (33%)]\tLoss: 1.545187\nTrain Epoch: 45 [400/900 (44%)]\tLoss: 1.069327\nTrain Epoch: 45 [500/900 (56%)]\tLoss: 1.464213\nTrain Epoch: 45 [600/900 (67%)]\tLoss: 1.322365\nTrain Epoch: 45 [700/900 (78%)]\tLoss: 1.339647\nTrain Epoch: 45 [800/900 (89%)]\tLoss: 1.383049\n\nevaluating...\nTest set: Average loss: 1.0738, Average CER: 0.319473 Average WER: 0.9326\n\nTrain Epoch: 46 [0/900 (0%)]\tLoss: 1.715645\nTrain Epoch: 46 [100/900 (11%)]\tLoss: 1.845909\nTrain Epoch: 46 [200/900 (22%)]\tLoss: 1.450273\nTrain Epoch: 46 [300/900 (33%)]\tLoss: 1.168821\nTrain Epoch: 46 [400/900 (44%)]\tLoss: 1.322910\nTrain Epoch: 46 [500/900 (56%)]\tLoss: 1.307490\nTrain Epoch: 46 [600/900 (67%)]\tLoss: 1.372376\nTrain Epoch: 46 [700/900 (78%)]\tLoss: 1.466408\nTrain Epoch: 46 [800/900 (89%)]\tLoss: 1.249597\n\nevaluating...\nTest set: Average loss: 1.0633, Average CER: 0.320120 Average WER: 0.9362\n\nTrain Epoch: 47 [0/900 (0%)]\tLoss: 1.325500\nTrain Epoch: 47 [100/900 (11%)]\tLoss: 1.007181\nTrain Epoch: 47 [200/900 (22%)]\tLoss: 1.369762\nTrain Epoch: 47 [300/900 (33%)]\tLoss: 1.155408\nTrain Epoch: 47 [400/900 (44%)]\tLoss: 1.520949\nTrain Epoch: 47 [500/900 (56%)]\tLoss: 1.078121\nTrain Epoch: 47 [600/900 (67%)]\tLoss: 1.145965\nTrain Epoch: 47 [700/900 (78%)]\tLoss: 1.254669\nTrain Epoch: 47 [800/900 (89%)]\tLoss: 1.016222\n\nevaluating...\nTest set: Average loss: 1.0589, Average CER: 0.324281 Average WER: 0.9457\n\nTrain Epoch: 48 [0/900 (0%)]\tLoss: 1.048665\nTrain Epoch: 48 [100/900 (11%)]\tLoss: 1.347053\nTrain Epoch: 48 [200/900 (22%)]\tLoss: 1.499543\nTrain Epoch: 48 [300/900 (33%)]\tLoss: 1.312958\nTrain Epoch: 48 [400/900 (44%)]\tLoss: 1.251560\nTrain Epoch: 48 [500/900 (56%)]\tLoss: 0.920080\nTrain Epoch: 48 [600/900 (67%)]\tLoss: 0.927581\nTrain Epoch: 48 [700/900 (78%)]\tLoss: 0.966946\nTrain Epoch: 48 [800/900 (89%)]\tLoss: 0.962643\n\nevaluating...\nTest set: Average loss: 1.0785, Average CER: 0.313763 Average WER: 0.9322\n\nTrain Epoch: 49 [0/900 (0%)]\tLoss: 1.369888\nTrain Epoch: 49 [100/900 (11%)]\tLoss: 1.130630\nTrain Epoch: 49 [200/900 (22%)]\tLoss: 1.251602\nTrain Epoch: 49 [300/900 (33%)]\tLoss: 1.114585\nTrain Epoch: 49 [400/900 (44%)]\tLoss: 1.112537\nTrain Epoch: 49 [500/900 (56%)]\tLoss: 1.356725\nTrain Epoch: 49 [600/900 (67%)]\tLoss: 1.189736\nTrain Epoch: 49 [700/900 (78%)]\tLoss: 1.155097\nTrain Epoch: 49 [800/900 (89%)]\tLoss: 0.983594\n\nevaluating...\nTest set: Average loss: 1.0956, Average CER: 0.328478 Average WER: 0.9544\n\nTrain Epoch: 50 [0/900 (0%)]\tLoss: 1.358353\nTrain Epoch: 50 [100/900 (11%)]\tLoss: 1.136450\nTrain Epoch: 50 [200/900 (22%)]\tLoss: 1.226535\nTrain Epoch: 50 [300/900 (33%)]\tLoss: 1.699105\nTrain Epoch: 50 [400/900 (44%)]\tLoss: 1.205375\nTrain Epoch: 50 [500/900 (56%)]\tLoss: 1.169842\nTrain Epoch: 50 [600/900 (67%)]\tLoss: 0.960235\nTrain Epoch: 50 [700/900 (78%)]\tLoss: 0.934455\nTrain Epoch: 50 [800/900 (89%)]\tLoss: 0.926142\n\nevaluating...\nTest set: Average loss: 1.0903, Average CER: 0.322622 Average WER: 0.9543\n\nTrain Epoch: 51 [0/900 (0%)]\tLoss: 0.989487\nTrain Epoch: 51 [100/900 (11%)]\tLoss: 1.341913\nTrain Epoch: 51 [200/900 (22%)]\tLoss: 1.408670\nTrain Epoch: 51 [300/900 (33%)]\tLoss: 0.848233\nTrain Epoch: 51 [400/900 (44%)]\tLoss: 0.938321\nTrain Epoch: 51 [500/900 (56%)]\tLoss: 1.197334\nTrain Epoch: 51 [600/900 (67%)]\tLoss: 1.339103\nTrain Epoch: 51 [700/900 (78%)]\tLoss: 1.740889\nTrain Epoch: 51 [800/900 (89%)]\tLoss: 1.201564\n\nevaluating...\nTest set: Average loss: 1.0748, Average CER: 0.320106 Average WER: 0.9382\n\nTrain Epoch: 52 [0/900 (0%)]\tLoss: 0.969602\nTrain Epoch: 52 [100/900 (11%)]\tLoss: 0.912874\nTrain Epoch: 52 [200/900 (22%)]\tLoss: 1.156084\nTrain Epoch: 52 [300/900 (33%)]\tLoss: 1.013860\nTrain Epoch: 52 [400/900 (44%)]\tLoss: 1.029172\nTrain Epoch: 52 [500/900 (56%)]\tLoss: 1.171638\nTrain Epoch: 52 [600/900 (67%)]\tLoss: 1.508407\nTrain Epoch: 52 [700/900 (78%)]\tLoss: 1.091632\nTrain Epoch: 52 [800/900 (89%)]\tLoss: 1.206810\n\nevaluating...\nTest set: Average loss: 1.0700, Average CER: 0.299969 Average WER: 0.9232\n\nTrain Epoch: 53 [0/900 (0%)]\tLoss: 0.577793\nTrain Epoch: 53 [100/900 (11%)]\tLoss: 0.929739\nTrain Epoch: 53 [200/900 (22%)]\tLoss: 0.920296\nTrain Epoch: 53 [300/900 (33%)]\tLoss: 1.062817\nTrain Epoch: 53 [400/900 (44%)]\tLoss: 1.086107\nTrain Epoch: 53 [500/900 (56%)]\tLoss: 1.180834\nTrain Epoch: 53 [600/900 (67%)]\tLoss: 1.014544\nTrain Epoch: 53 [700/900 (78%)]\tLoss: 1.150731\nTrain Epoch: 53 [800/900 (89%)]\tLoss: 1.525056\n\nevaluating...\nTest set: Average loss: 1.0895, Average CER: 0.335257 Average WER: 0.9714\n\nTrain Epoch: 54 [0/900 (0%)]\tLoss: 1.027698\nTrain Epoch: 54 [100/900 (11%)]\tLoss: 1.215216\nTrain Epoch: 54 [200/900 (22%)]\tLoss: 1.364927\nTrain Epoch: 54 [300/900 (33%)]\tLoss: 1.264429\nTrain Epoch: 54 [400/900 (44%)]\tLoss: 0.654506\nTrain Epoch: 54 [500/900 (56%)]\tLoss: 1.298189\nTrain Epoch: 54 [600/900 (67%)]\tLoss: 1.009469\nTrain Epoch: 54 [700/900 (78%)]\tLoss: 1.172986\nTrain Epoch: 54 [800/900 (89%)]\tLoss: 1.212853\n\nevaluating...\nTest set: Average loss: 1.0872, Average CER: 0.318630 Average WER: 0.9360\n\nTrain Epoch: 55 [0/900 (0%)]\tLoss: 1.071075\nTrain Epoch: 55 [100/900 (11%)]\tLoss: 1.403885\nTrain Epoch: 55 [200/900 (22%)]\tLoss: 0.829506\nTrain Epoch: 55 [300/900 (33%)]\tLoss: 1.122301\nTrain Epoch: 55 [400/900 (44%)]\tLoss: 1.346066\nTrain Epoch: 55 [500/900 (56%)]\tLoss: 1.155402\nTrain Epoch: 55 [600/900 (67%)]\tLoss: 1.474880\nTrain Epoch: 55 [700/900 (78%)]\tLoss: 0.830915\nTrain Epoch: 55 [800/900 (89%)]\tLoss: 0.700764\n\nevaluating...\nTest set: Average loss: 1.0539, Average CER: 0.311605 Average WER: 0.9407\n\nTrain Epoch: 56 [0/900 (0%)]\tLoss: 1.101735\nTrain Epoch: 56 [100/900 (11%)]\tLoss: 1.049328\nTrain Epoch: 56 [200/900 (22%)]\tLoss: 1.067459\nTrain Epoch: 56 [300/900 (33%)]\tLoss: 1.082699\nTrain Epoch: 56 [400/900 (44%)]\tLoss: 0.860965\nTrain Epoch: 56 [500/900 (56%)]\tLoss: 1.325163\nTrain Epoch: 56 [600/900 (67%)]\tLoss: 1.192652\nTrain Epoch: 56 [700/900 (78%)]\tLoss: 0.943009\nTrain Epoch: 56 [800/900 (89%)]\tLoss: 0.918364\n\nevaluating...\nTest set: Average loss: 1.0476, Average CER: 0.296523 Average WER: 0.9068\n\nTrain Epoch: 57 [0/900 (0%)]\tLoss: 1.305384\nTrain Epoch: 57 [100/900 (11%)]\tLoss: 0.692941\nTrain Epoch: 57 [200/900 (22%)]\tLoss: 0.898964\nTrain Epoch: 57 [300/900 (33%)]\tLoss: 1.060871\nTrain Epoch: 57 [400/900 (44%)]\tLoss: 1.184234\nTrain Epoch: 57 [500/900 (56%)]\tLoss: 1.287213\nTrain Epoch: 57 [600/900 (67%)]\tLoss: 1.534544\nTrain Epoch: 57 [700/900 (78%)]\tLoss: 0.947479\nTrain Epoch: 57 [800/900 (89%)]\tLoss: 0.950058\n\nevaluating...\nTest set: Average loss: 1.0683, Average CER: 0.309480 Average WER: 0.9394\n\nTrain Epoch: 58 [0/900 (0%)]\tLoss: 1.106748\nTrain Epoch: 58 [100/900 (11%)]\tLoss: 1.089266\nTrain Epoch: 58 [200/900 (22%)]\tLoss: 0.764398\nTrain Epoch: 58 [300/900 (33%)]\tLoss: 1.037748\nTrain Epoch: 58 [400/900 (44%)]\tLoss: 1.099091\nTrain Epoch: 58 [500/900 (56%)]\tLoss: 1.036405\nTrain Epoch: 58 [600/900 (67%)]\tLoss: 0.931296\nTrain Epoch: 58 [700/900 (78%)]\tLoss: 1.102892\nTrain Epoch: 58 [800/900 (89%)]\tLoss: 1.037853\n\nevaluating...\nTest set: Average loss: 1.1138, Average CER: 0.335059 Average WER: 0.9615\n\nTrain Epoch: 59 [0/900 (0%)]\tLoss: 1.220847\nTrain Epoch: 59 [100/900 (11%)]\tLoss: 1.053462\nTrain Epoch: 59 [200/900 (22%)]\tLoss: 1.122566\nTrain Epoch: 59 [300/900 (33%)]\tLoss: 1.242081\nTrain Epoch: 59 [400/900 (44%)]\tLoss: 1.183319\nTrain Epoch: 59 [500/900 (56%)]\tLoss: 1.021507\nTrain Epoch: 59 [600/900 (67%)]\tLoss: 1.147314\nTrain Epoch: 59 [700/900 (78%)]\tLoss: 1.043764\nTrain Epoch: 59 [800/900 (89%)]\tLoss: 1.210206\n\nevaluating...\nTest set: Average loss: 1.0784, Average CER: 0.307905 Average WER: 0.9434\n\nTrain Epoch: 60 [0/900 (0%)]\tLoss: 0.952738\nTrain Epoch: 60 [100/900 (11%)]\tLoss: 1.289971\nTrain Epoch: 60 [200/900 (22%)]\tLoss: 1.063882\nTrain Epoch: 60 [300/900 (33%)]\tLoss: 0.734593\nTrain Epoch: 60 [400/900 (44%)]\tLoss: 1.438202\nTrain Epoch: 60 [500/900 (56%)]\tLoss: 1.152414\nTrain Epoch: 60 [600/900 (67%)]\tLoss: 1.099448\nTrain Epoch: 60 [700/900 (78%)]\tLoss: 1.540278\nTrain Epoch: 60 [800/900 (89%)]\tLoss: 1.006453\n\nevaluating...\nTest set: Average loss: 1.0576, Average CER: 0.308849 Average WER: 0.9325\n\nTrain Epoch: 61 [0/900 (0%)]\tLoss: 1.185251\nTrain Epoch: 61 [100/900 (11%)]\tLoss: 0.806211\nTrain Epoch: 61 [200/900 (22%)]\tLoss: 1.248669\nTrain Epoch: 61 [300/900 (33%)]\tLoss: 1.028406\nTrain Epoch: 61 [400/900 (44%)]\tLoss: 1.202610\nTrain Epoch: 61 [500/900 (56%)]\tLoss: 1.075795\nTrain Epoch: 61 [600/900 (67%)]\tLoss: 0.769789\nTrain Epoch: 61 [700/900 (78%)]\tLoss: 0.839886\nTrain Epoch: 61 [800/900 (89%)]\tLoss: 1.066235\n\nevaluating...\nTest set: Average loss: 1.0866, Average CER: 0.309270 Average WER: 0.9327\n\nTrain Epoch: 62 [0/900 (0%)]\tLoss: 1.182989\nTrain Epoch: 62 [100/900 (11%)]\tLoss: 1.015456\nTrain Epoch: 62 [200/900 (22%)]\tLoss: 1.148974\nTrain Epoch: 62 [300/900 (33%)]\tLoss: 0.836610\nTrain Epoch: 62 [400/900 (44%)]\tLoss: 0.882206\nTrain Epoch: 62 [500/900 (56%)]\tLoss: 1.176411\nTrain Epoch: 62 [600/900 (67%)]\tLoss: 1.553244\nTrain Epoch: 62 [700/900 (78%)]\tLoss: 1.296501\nTrain Epoch: 62 [800/900 (89%)]\tLoss: 0.943867\n\nevaluating...\nTest set: Average loss: 1.3504, Average CER: 0.401067 Average WER: 0.9589\n\nTrain Epoch: 63 [0/900 (0%)]\tLoss: 0.883953\nTrain Epoch: 63 [100/900 (11%)]\tLoss: 1.269142\nTrain Epoch: 63 [200/900 (22%)]\tLoss: 1.151643\nTrain Epoch: 63 [300/900 (33%)]\tLoss: 1.148979\nTrain Epoch: 63 [400/900 (44%)]\tLoss: 1.250426\nTrain Epoch: 63 [500/900 (56%)]\tLoss: 0.830400\nTrain Epoch: 63 [600/900 (67%)]\tLoss: 0.997171\nTrain Epoch: 63 [700/900 (78%)]\tLoss: 0.961095\nTrain Epoch: 63 [800/900 (89%)]\tLoss: 0.980426\n\nevaluating...\nTest set: Average loss: 1.2026, Average CER: 0.362378 Average WER: 0.9673\n\nTrain Epoch: 64 [0/900 (0%)]\tLoss: 1.427778\nTrain Epoch: 64 [100/900 (11%)]\tLoss: 1.211611\nTrain Epoch: 64 [200/900 (22%)]\tLoss: 0.717873\nTrain Epoch: 64 [300/900 (33%)]\tLoss: 1.175960\nTrain Epoch: 64 [400/900 (44%)]\tLoss: 0.770869\nTrain Epoch: 64 [500/900 (56%)]\tLoss: 0.754435\nTrain Epoch: 64 [600/900 (67%)]\tLoss: 0.915040\nTrain Epoch: 64 [700/900 (78%)]\tLoss: 0.859396\nTrain Epoch: 64 [800/900 (89%)]\tLoss: 0.997290\n\nevaluating...\nTest set: Average loss: 1.0608, Average CER: 0.298201 Average WER: 0.9212\n\nTrain Epoch: 65 [0/900 (0%)]\tLoss: 1.099135\nTrain Epoch: 65 [100/900 (11%)]\tLoss: 1.366583\nTrain Epoch: 65 [200/900 (22%)]\tLoss: 1.604050\nTrain Epoch: 65 [300/900 (33%)]\tLoss: 0.845787\nTrain Epoch: 65 [400/900 (44%)]\tLoss: 1.155505\nTrain Epoch: 65 [500/900 (56%)]\tLoss: 0.984544\nTrain Epoch: 65 [600/900 (67%)]\tLoss: 1.451610\nTrain Epoch: 65 [700/900 (78%)]\tLoss: 1.198740\nTrain Epoch: 65 [800/900 (89%)]\tLoss: 0.899179\n\nevaluating...\nTest set: Average loss: 1.0688, Average CER: 0.301159 Average WER: 0.9167\n\nTrain Epoch: 66 [0/900 (0%)]\tLoss: 0.634074\nTrain Epoch: 66 [100/900 (11%)]\tLoss: 1.088879\nTrain Epoch: 66 [200/900 (22%)]\tLoss: 1.101955\nTrain Epoch: 66 [300/900 (33%)]\tLoss: 1.202803\nTrain Epoch: 66 [400/900 (44%)]\tLoss: 1.115385\nTrain Epoch: 66 [500/900 (56%)]\tLoss: 1.092969\nTrain Epoch: 66 [600/900 (67%)]\tLoss: 0.691531\nTrain Epoch: 66 [700/900 (78%)]\tLoss: 0.917667\nTrain Epoch: 66 [800/900 (89%)]\tLoss: 1.145086\n\nevaluating...\nTest set: Average loss: 1.1631, Average CER: 0.335206 Average WER: 0.9511\n\nTrain Epoch: 67 [0/900 (0%)]\tLoss: 1.054465\nTrain Epoch: 67 [100/900 (11%)]\tLoss: 1.166014\nTrain Epoch: 67 [200/900 (22%)]\tLoss: 1.262514\nTrain Epoch: 67 [300/900 (33%)]\tLoss: 1.769963\nTrain Epoch: 67 [400/900 (44%)]\tLoss: 0.869678\nTrain Epoch: 67 [500/900 (56%)]\tLoss: 1.047139\nTrain Epoch: 67 [600/900 (67%)]\tLoss: 1.059908\nTrain Epoch: 67 [700/900 (78%)]\tLoss: 0.882782\nTrain Epoch: 67 [800/900 (89%)]\tLoss: 1.198637\n\nevaluating...\nTest set: Average loss: 1.0670, Average CER: 0.288716 Average WER: 0.9179\n\nTrain Epoch: 68 [0/900 (0%)]\tLoss: 0.925663\nTrain Epoch: 68 [100/900 (11%)]\tLoss: 1.286954\nTrain Epoch: 68 [200/900 (22%)]\tLoss: 0.942776\nTrain Epoch: 68 [300/900 (33%)]\tLoss: 0.828344\nTrain Epoch: 68 [400/900 (44%)]\tLoss: 0.922662\nTrain Epoch: 68 [500/900 (56%)]\tLoss: 1.197742\nTrain Epoch: 68 [600/900 (67%)]\tLoss: 0.867500\nTrain Epoch: 68 [700/900 (78%)]\tLoss: 1.165271\nTrain Epoch: 68 [800/900 (89%)]\tLoss: 0.830697\n\nevaluating...\nTest set: Average loss: 1.1163, Average CER: 0.321144 Average WER: 0.9438\n\nTrain Epoch: 69 [0/900 (0%)]\tLoss: 1.238520\nTrain Epoch: 69 [100/900 (11%)]\tLoss: 0.797189\nTrain Epoch: 69 [200/900 (22%)]\tLoss: 1.123569\nTrain Epoch: 69 [300/900 (33%)]\tLoss: 0.757719\nTrain Epoch: 69 [400/900 (44%)]\tLoss: 1.092886\nTrain Epoch: 69 [500/900 (56%)]\tLoss: 1.482968\nTrain Epoch: 69 [600/900 (67%)]\tLoss: 0.889159\nTrain Epoch: 69 [700/900 (78%)]\tLoss: 1.126372\nTrain Epoch: 69 [800/900 (89%)]\tLoss: 1.186399\n\nevaluating...\nTest set: Average loss: 1.0819, Average CER: 0.292756 Average WER: 0.9302\n\nTrain Epoch: 70 [0/900 (0%)]\tLoss: 0.741490\nTrain Epoch: 70 [100/900 (11%)]\tLoss: 0.721969\nTrain Epoch: 70 [200/900 (22%)]\tLoss: 1.129530\nTrain Epoch: 70 [300/900 (33%)]\tLoss: 1.100388\nTrain Epoch: 70 [400/900 (44%)]\tLoss: 1.287444\nTrain Epoch: 70 [500/900 (56%)]\tLoss: 1.185076\nTrain Epoch: 70 [600/900 (67%)]\tLoss: 0.929367\nTrain Epoch: 70 [700/900 (78%)]\tLoss: 1.209198\nTrain Epoch: 70 [800/900 (89%)]\tLoss: 1.566216\n\nevaluating...\nTest set: Average loss: 1.0901, Average CER: 0.296347 Average WER: 0.9188\n\nTrain Epoch: 71 [0/900 (0%)]\tLoss: 0.917299\nTrain Epoch: 71 [100/900 (11%)]\tLoss: 0.922005\nTrain Epoch: 71 [200/900 (22%)]\tLoss: 1.176076\nTrain Epoch: 71 [300/900 (33%)]\tLoss: 1.151742\nTrain Epoch: 71 [400/900 (44%)]\tLoss: 0.727312\nTrain Epoch: 71 [500/900 (56%)]\tLoss: 1.153434\nTrain Epoch: 71 [600/900 (67%)]\tLoss: 0.707222\nTrain Epoch: 71 [700/900 (78%)]\tLoss: 1.096540\nTrain Epoch: 71 [800/900 (89%)]\tLoss: 0.822561\n\nevaluating...\nTest set: Average loss: 1.0678, Average CER: 0.298747 Average WER: 0.9183\n\nTrain Epoch: 72 [0/900 (0%)]\tLoss: 1.156197\nTrain Epoch: 72 [100/900 (11%)]\tLoss: 0.750200\nTrain Epoch: 72 [200/900 (22%)]\tLoss: 0.924943\nTrain Epoch: 72 [300/900 (33%)]\tLoss: 0.741651\nTrain Epoch: 72 [400/900 (44%)]\tLoss: 1.722459\nTrain Epoch: 72 [500/900 (56%)]\tLoss: 0.881865\nTrain Epoch: 72 [600/900 (67%)]\tLoss: 0.863921\nTrain Epoch: 72 [700/900 (78%)]\tLoss: 0.930653\nTrain Epoch: 72 [800/900 (89%)]\tLoss: 0.988410\n\nevaluating...\nTest set: Average loss: 1.0799, Average CER: 0.295044 Average WER: 0.9379\n\nTrain Epoch: 73 [0/900 (0%)]\tLoss: 1.463597\nTrain Epoch: 73 [100/900 (11%)]\tLoss: 1.215383\nTrain Epoch: 73 [200/900 (22%)]\tLoss: 0.670812\nTrain Epoch: 73 [300/900 (33%)]\tLoss: 0.962116\nTrain Epoch: 73 [400/900 (44%)]\tLoss: 0.991554\nTrain Epoch: 73 [500/900 (56%)]\tLoss: 0.801032\nTrain Epoch: 73 [600/900 (67%)]\tLoss: 0.660018\nTrain Epoch: 73 [700/900 (78%)]\tLoss: 1.200469\nTrain Epoch: 73 [800/900 (89%)]\tLoss: 1.036675\n\nevaluating...\nTest set: Average loss: 1.0734, Average CER: 0.296592 Average WER: 0.9216\n\nTrain Epoch: 74 [0/900 (0%)]\tLoss: 1.362995\nTrain Epoch: 74 [100/900 (11%)]\tLoss: 1.013113\nTrain Epoch: 74 [200/900 (22%)]\tLoss: 0.648513\nTrain Epoch: 74 [300/900 (33%)]\tLoss: 0.932583\nTrain Epoch: 74 [400/900 (44%)]\tLoss: 0.777534\nTrain Epoch: 74 [500/900 (56%)]\tLoss: 1.076445\nTrain Epoch: 74 [600/900 (67%)]\tLoss: 1.291445\nTrain Epoch: 74 [700/900 (78%)]\tLoss: 0.910316\nTrain Epoch: 74 [800/900 (89%)]\tLoss: 0.897173\n\nevaluating...\nTest set: Average loss: 1.0863, Average CER: 0.287438 Average WER: 0.9105\n\nTrain Epoch: 75 [0/900 (0%)]\tLoss: 0.630097\nTrain Epoch: 75 [100/900 (11%)]\tLoss: 1.055203\nTrain Epoch: 75 [200/900 (22%)]\tLoss: 1.071897\nTrain Epoch: 75 [300/900 (33%)]\tLoss: 0.928641\nTrain Epoch: 75 [400/900 (44%)]\tLoss: 1.154154\nTrain Epoch: 75 [500/900 (56%)]\tLoss: 1.042014\nTrain Epoch: 75 [600/900 (67%)]\tLoss: 1.018411\nTrain Epoch: 75 [700/900 (78%)]\tLoss: 0.984652\nTrain Epoch: 75 [800/900 (89%)]\tLoss: 0.554814\n\nevaluating...\nTest set: Average loss: 1.0781, Average CER: 0.297978 Average WER: 0.9352\n\nTrain Epoch: 76 [0/900 (0%)]\tLoss: 0.823349\nTrain Epoch: 76 [100/900 (11%)]\tLoss: 0.967653\nTrain Epoch: 76 [200/900 (22%)]\tLoss: 0.786512\nTrain Epoch: 76 [300/900 (33%)]\tLoss: 1.230842\nTrain Epoch: 76 [400/900 (44%)]\tLoss: 0.984728\nTrain Epoch: 76 [500/900 (56%)]\tLoss: 0.797318\nTrain Epoch: 76 [600/900 (67%)]\tLoss: 1.021960\nTrain Epoch: 76 [700/900 (78%)]\tLoss: 1.505468\nTrain Epoch: 76 [800/900 (89%)]\tLoss: 0.770285\n\nevaluating...\nTest set: Average loss: 1.0781, Average CER: 0.290205 Average WER: 0.8979\n\nTrain Epoch: 77 [0/900 (0%)]\tLoss: 0.697780\nTrain Epoch: 77 [100/900 (11%)]\tLoss: 1.559773\nTrain Epoch: 77 [200/900 (22%)]\tLoss: 1.087899\nTrain Epoch: 77 [300/900 (33%)]\tLoss: 1.077738\nTrain Epoch: 77 [400/900 (44%)]\tLoss: 0.805120\nTrain Epoch: 77 [500/900 (56%)]\tLoss: 0.797932\nTrain Epoch: 77 [600/900 (67%)]\tLoss: 0.776449\nTrain Epoch: 77 [700/900 (78%)]\tLoss: 1.066824\nTrain Epoch: 77 [800/900 (89%)]\tLoss: 0.945580\n\nevaluating...\nTest set: Average loss: 1.0950, Average CER: 0.295384 Average WER: 0.9329\n\nTrain Epoch: 78 [0/900 (0%)]\tLoss: 1.160218\nTrain Epoch: 78 [100/900 (11%)]\tLoss: 0.799512\nTrain Epoch: 78 [200/900 (22%)]\tLoss: 0.902307\nTrain Epoch: 78 [300/900 (33%)]\tLoss: 1.026212\nTrain Epoch: 78 [400/900 (44%)]\tLoss: 1.074874\nTrain Epoch: 78 [500/900 (56%)]\tLoss: 0.944646\nTrain Epoch: 78 [600/900 (67%)]\tLoss: 0.817157\nTrain Epoch: 78 [700/900 (78%)]\tLoss: 0.954698\nTrain Epoch: 78 [800/900 (89%)]\tLoss: 0.925727\n\nevaluating...\nTest set: Average loss: 1.0699, Average CER: 0.291378 Average WER: 0.9178\n\nTrain Epoch: 79 [0/900 (0%)]\tLoss: 0.636205\nTrain Epoch: 79 [100/900 (11%)]\tLoss: 0.783081\nTrain Epoch: 79 [200/900 (22%)]\tLoss: 0.931646\nTrain Epoch: 79 [300/900 (33%)]\tLoss: 1.014270\nTrain Epoch: 79 [400/900 (44%)]\tLoss: 0.974724\nTrain Epoch: 79 [500/900 (56%)]\tLoss: 0.693341\nTrain Epoch: 79 [600/900 (67%)]\tLoss: 0.857522\nTrain Epoch: 79 [700/900 (78%)]\tLoss: 0.917492\nTrain Epoch: 79 [800/900 (89%)]\tLoss: 1.293066\n\nevaluating...\nTest set: Average loss: 1.0769, Average CER: 0.296215 Average WER: 0.9248\n\nTrain Epoch: 80 [0/900 (0%)]\tLoss: 0.929916\nTrain Epoch: 80 [100/900 (11%)]\tLoss: 1.157361\nTrain Epoch: 80 [200/900 (22%)]\tLoss: 0.970364\nTrain Epoch: 80 [300/900 (33%)]\tLoss: 0.734377\nTrain Epoch: 80 [400/900 (44%)]\tLoss: 0.947551\nTrain Epoch: 80 [500/900 (56%)]\tLoss: 1.031640\nTrain Epoch: 80 [600/900 (67%)]\tLoss: 1.280464\nTrain Epoch: 80 [700/900 (78%)]\tLoss: 0.805764\nTrain Epoch: 80 [800/900 (89%)]\tLoss: 0.990090\n\nevaluating...\nTest set: Average loss: 1.0781, Average CER: 0.299523 Average WER: 0.9150\n\nTrain Epoch: 81 [0/900 (0%)]\tLoss: 0.777631\nTrain Epoch: 81 [100/900 (11%)]\tLoss: 0.790437\nTrain Epoch: 81 [200/900 (22%)]\tLoss: 0.731107\nTrain Epoch: 81 [300/900 (33%)]\tLoss: 0.696008\nTrain Epoch: 81 [400/900 (44%)]\tLoss: 0.847718\nTrain Epoch: 81 [500/900 (56%)]\tLoss: 0.877094\nTrain Epoch: 81 [600/900 (67%)]\tLoss: 1.236513\nTrain Epoch: 81 [700/900 (78%)]\tLoss: 0.944705\nTrain Epoch: 81 [800/900 (89%)]\tLoss: 0.664942\n\nevaluating...\nTest set: Average loss: 1.0888, Average CER: 0.288952 Average WER: 0.9523\n\nTrain Epoch: 82 [0/900 (0%)]\tLoss: 1.032132\nTrain Epoch: 82 [100/900 (11%)]\tLoss: 0.788119\nTrain Epoch: 82 [200/900 (22%)]\tLoss: 0.671826\nTrain Epoch: 82 [300/900 (33%)]\tLoss: 0.735316\nTrain Epoch: 82 [400/900 (44%)]\tLoss: 0.632106\nTrain Epoch: 82 [500/900 (56%)]\tLoss: 1.012875\nTrain Epoch: 82 [600/900 (67%)]\tLoss: 0.795567\nTrain Epoch: 82 [700/900 (78%)]\tLoss: 0.771281\nTrain Epoch: 82 [800/900 (89%)]\tLoss: 0.857653\n\nevaluating...\nTest set: Average loss: 1.0750, Average CER: 0.290317 Average WER: 0.9125\n\nTrain Epoch: 83 [0/900 (0%)]\tLoss: 1.051652\nTrain Epoch: 83 [100/900 (11%)]\tLoss: 1.336389\nTrain Epoch: 83 [200/900 (22%)]\tLoss: 0.942312\nTrain Epoch: 83 [300/900 (33%)]\tLoss: 1.139462\nTrain Epoch: 83 [400/900 (44%)]\tLoss: 0.910292\nTrain Epoch: 83 [500/900 (56%)]\tLoss: 1.357286\nTrain Epoch: 83 [600/900 (67%)]\tLoss: 0.630291\nTrain Epoch: 83 [700/900 (78%)]\tLoss: 0.654800\nTrain Epoch: 83 [800/900 (89%)]\tLoss: 0.923937\n\nevaluating...\nTest set: Average loss: 1.0766, Average CER: 0.286149 Average WER: 0.9197\n\nTrain Epoch: 84 [0/900 (0%)]\tLoss: 0.722251\nTrain Epoch: 84 [100/900 (11%)]\tLoss: 0.879443\nTrain Epoch: 84 [200/900 (22%)]\tLoss: 0.747031\nTrain Epoch: 84 [300/900 (33%)]\tLoss: 0.638455\nTrain Epoch: 84 [400/900 (44%)]\tLoss: 0.837967\nTrain Epoch: 84 [500/900 (56%)]\tLoss: 0.952460\nTrain Epoch: 84 [600/900 (67%)]\tLoss: 0.567734\nTrain Epoch: 84 [700/900 (78%)]\tLoss: 1.161192\nTrain Epoch: 84 [800/900 (89%)]\tLoss: 0.856974\n\nevaluating...\nTest set: Average loss: 1.0737, Average CER: 0.297386 Average WER: 0.9409\n\nTrain Epoch: 85 [0/900 (0%)]\tLoss: 0.813570\nTrain Epoch: 85 [100/900 (11%)]\tLoss: 0.833271\nTrain Epoch: 85 [200/900 (22%)]\tLoss: 1.012518\nTrain Epoch: 85 [300/900 (33%)]\tLoss: 0.839205\nTrain Epoch: 85 [400/900 (44%)]\tLoss: 1.038281\nTrain Epoch: 85 [500/900 (56%)]\tLoss: 0.934745\nTrain Epoch: 85 [600/900 (67%)]\tLoss: 1.055796\nTrain Epoch: 85 [700/900 (78%)]\tLoss: 0.851944\nTrain Epoch: 85 [800/900 (89%)]\tLoss: 0.556568\n\nevaluating...\nTest set: Average loss: 1.0769, Average CER: 0.292886 Average WER: 0.9364\n\nTrain Epoch: 86 [0/900 (0%)]\tLoss: 0.567209\nTrain Epoch: 86 [100/900 (11%)]\tLoss: 0.831660\nTrain Epoch: 86 [200/900 (22%)]\tLoss: 0.997512\nTrain Epoch: 86 [300/900 (33%)]\tLoss: 0.760338\nTrain Epoch: 86 [400/900 (44%)]\tLoss: 1.080304\nTrain Epoch: 86 [500/900 (56%)]\tLoss: 1.096713\nTrain Epoch: 86 [600/900 (67%)]\tLoss: 1.118553\nTrain Epoch: 86 [700/900 (78%)]\tLoss: 0.823623\nTrain Epoch: 86 [800/900 (89%)]\tLoss: 0.994015\n\nevaluating...\nTest set: Average loss: 1.0796, Average CER: 0.291172 Average WER: 0.9280\n\nTrain Epoch: 87 [0/900 (0%)]\tLoss: 1.274658\nTrain Epoch: 87 [100/900 (11%)]\tLoss: 0.971340\nTrain Epoch: 87 [200/900 (22%)]\tLoss: 1.355665\nTrain Epoch: 87 [300/900 (33%)]\tLoss: 0.538347\nTrain Epoch: 87 [400/900 (44%)]\tLoss: 0.630496\nTrain Epoch: 87 [500/900 (56%)]\tLoss: 0.824483\nTrain Epoch: 87 [600/900 (67%)]\tLoss: 1.040385\nTrain Epoch: 87 [700/900 (78%)]\tLoss: 0.971460\nTrain Epoch: 87 [800/900 (89%)]\tLoss: 1.054598\n\nevaluating...\nTest set: Average loss: 1.0877, Average CER: 0.298437 Average WER: 0.9344\n\nTrain Epoch: 88 [0/900 (0%)]\tLoss: 0.818854\nTrain Epoch: 88 [100/900 (11%)]\tLoss: 0.669929\nTrain Epoch: 88 [200/900 (22%)]\tLoss: 0.815124\nTrain Epoch: 88 [300/900 (33%)]\tLoss: 0.690347\nTrain Epoch: 88 [400/900 (44%)]\tLoss: 1.067567\nTrain Epoch: 88 [500/900 (56%)]\tLoss: 0.621963\nTrain Epoch: 88 [600/900 (67%)]\tLoss: 0.834245\nTrain Epoch: 88 [700/900 (78%)]\tLoss: 1.190350\nTrain Epoch: 88 [800/900 (89%)]\tLoss: 0.820371\n\nevaluating...\nTest set: Average loss: 1.0763, Average CER: 0.289072 Average WER: 0.9164\n\nTrain Epoch: 89 [0/900 (0%)]\tLoss: 0.859644\nTrain Epoch: 89 [100/900 (11%)]\tLoss: 0.825212\nTrain Epoch: 89 [200/900 (22%)]\tLoss: 0.963245\nTrain Epoch: 89 [300/900 (33%)]\tLoss: 0.867146\nTrain Epoch: 89 [400/900 (44%)]\tLoss: 0.709565\nTrain Epoch: 89 [500/900 (56%)]\tLoss: 0.706255\nTrain Epoch: 89 [600/900 (67%)]\tLoss: 0.983528\nTrain Epoch: 89 [700/900 (78%)]\tLoss: 0.690992\nTrain Epoch: 89 [800/900 (89%)]\tLoss: 0.850962\n\nevaluating...\nTest set: Average loss: 1.0815, Average CER: 0.291084 Average WER: 0.9303\n\nTrain Epoch: 90 [0/900 (0%)]\tLoss: 1.058666\nTrain Epoch: 90 [100/900 (11%)]\tLoss: 0.880291\nTrain Epoch: 90 [200/900 (22%)]\tLoss: 0.767382\nTrain Epoch: 90 [300/900 (33%)]\tLoss: 0.893383\nTrain Epoch: 90 [400/900 (44%)]\tLoss: 0.892380\nTrain Epoch: 90 [500/900 (56%)]\tLoss: 0.851880\nTrain Epoch: 90 [600/900 (67%)]\tLoss: 0.866806\nTrain Epoch: 90 [700/900 (78%)]\tLoss: 0.611642\nTrain Epoch: 90 [800/900 (89%)]\tLoss: 1.235705\n\nevaluating...\nTest set: Average loss: 1.0811, Average CER: 0.292086 Average WER: 0.9204\n\n","output_type":"stream"}]},{"cell_type":"code","source":"use_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cpu\")\n\nmodel = torch.load('/kaggle/working/model.pt')\n\n\"\"\"model = SpeechRecognitionModel1(34)\nmodel.load_state_dict(check_model)\"\"\"\n\n#924 file, 961 964 992 994\n\nmodel.to(device)\npredict(model, '/kaggle/input/upd-speech/abnormal_voice/961.wav', device)","metadata":{"execution":{"iopub.status.busy":"2023-04-26T16:55:39.546151Z","iopub.execute_input":"2023-04-26T16:55:39.546613Z","iopub.status.idle":"2023-04-26T16:55:39.777499Z","shell.execute_reply.started":"2023-04-26T16:55:39.546574Z","shell.execute_reply":"2023-04-26T16:55:39.776495Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"'тапки не стоит остовлят на улице'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}