{"cells":[{"cell_type":"code","execution_count":1,"id":"cd03f93d","metadata":{"execution":{"iopub.execute_input":"2023-04-22T15:36:09.735870Z","iopub.status.busy":"2023-04-22T15:36:09.735387Z","iopub.status.idle":"2023-04-22T15:36:12.109130Z","shell.execute_reply":"2023-04-22T15:36:12.107903Z","shell.execute_reply.started":"2023-04-22T15:36:09.735823Z"},"trusted":true},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.utils.data as data\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchaudio\n","import numpy as np"]},{"cell_type":"code","execution_count":55,"id":"7d9d740a","metadata":{"execution":{"iopub.execute_input":"2023-04-22T16:03:36.788130Z","iopub.status.busy":"2023-04-22T16:03:36.787732Z","iopub.status.idle":"2023-04-22T16:03:36.823871Z","shell.execute_reply":"2023-04-22T16:03:36.822715Z","shell.execute_reply.started":"2023-04-22T16:03:36.788097Z"},"trusted":true},"outputs":[],"source":["def avg_wer(wer_scores, combined_ref_len):\n","    return float(sum(wer_scores)) / float(combined_ref_len)\n","\n","\n","def _levenshtein_distance(ref, hyp):\n","    \"\"\"Levenshtein distance is a string metric for measuring the difference\n","    between two sequences. Informally, the levenshtein disctance is defined as\n","    the minimum number of single-character edits (substitutions, insertions or\n","    deletions) required to change one word into the other. We can naturally\n","    extend the edits to word level when calculate levenshtein disctance for\n","    two sentences.\n","    \"\"\"\n","    m = len(ref)\n","    n = len(hyp)\n","\n","    # special case\n","    if ref == hyp:\n","        return 0\n","    if m == 0:\n","        return n\n","    if n == 0:\n","        return m\n","\n","    if m < n:\n","        ref, hyp = hyp, ref\n","        m, n = n, m\n","\n","    # use O(min(m, n)) space\n","    distance = np.zeros((2, n + 1), dtype=np.int32)\n","\n","    # initialize distance matrix\n","    for j in range(0,n + 1):\n","        distance[0][j] = j\n","\n","    # calculate levenshtein distance\n","    for i in range(1, m + 1):\n","        prev_row_idx = (i - 1) % 2\n","        cur_row_idx = i % 2\n","        distance[cur_row_idx][0] = i\n","        for j in range(1, n + 1):\n","            if ref[i - 1] == hyp[j - 1]:\n","                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n","            else:\n","                s_num = distance[prev_row_idx][j - 1] + 1\n","                i_num = distance[cur_row_idx][j - 1] + 1\n","                d_num = distance[prev_row_idx][j] + 1\n","                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n","\n","    return distance[m % 2][n]\n","\n","\n","def word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n","    \"\"\"Compute the levenshtein distance between reference sequence and\n","    hypothesis sequence in word-level.\n","    :param reference: The reference sentence.\n","    :type reference: basestring\n","    :param hypothesis: The hypothesis sentence.\n","    :type hypothesis: basestring\n","    :param ignore_case: Whether case-sensitive or not.\n","    :type ignore_case: bool\n","    :param delimiter: Delimiter of input sentences.\n","    :type delimiter: char\n","    :return: Levenshtein distance and word number of reference sentence.\n","    :rtype: list\n","    \"\"\"\n","    if ignore_case == True:\n","        reference = reference.lower()\n","        hypothesis = hypothesis.lower()\n","\n","    ref_words = reference.split(delimiter)\n","    hyp_words = hypothesis.split(delimiter)\n","\n","    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n","    return float(edit_distance), len(ref_words)\n","\n","\n","def char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n","    \"\"\"Compute the levenshtein distance between reference sequence and\n","    hypothesis sequence in char-level.\n","    :param reference: The reference sentence.\n","    :type reference: basestring\n","    :param hypothesis: The hypothesis sentence.\n","    :type hypothesis: basestring\n","    :param ignore_case: Whether case-sensitive or not.\n","    :type ignore_case: bool\n","    :param remove_space: Whether remove internal space characters\n","    :type remove_space: bool\n","    :return: Levenshtein distance and length of reference sentence.\n","    :rtype: list\n","    \"\"\"\n","    if ignore_case == True:\n","        reference = reference.lower()\n","        hypothesis = hypothesis.lower()\n","\n","    join_char = ' '\n","    if remove_space == True:\n","        join_char = ''\n","\n","    reference = join_char.join(filter(None, reference.split(' ')))\n","    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n","\n","    edit_distance = _levenshtein_distance(reference, hypothesis)\n","    return float(edit_distance), len(reference)\n","\n","\n","def wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n","    \"\"\"Calculate word error rate (WER). WER compares reference text and\n","    hypothesis text in word-level. WER is defined as:\n","    .. math::\n","        WER = (Sw + Dw + Iw) / Nw\n","    where\n","    .. code-block:: text\n","        Sw is the number of words subsituted,\n","        Dw is the number of words deleted,\n","        Iw is the number of words inserted,\n","        Nw is the number of words in the reference\n","    We can use levenshtein distance to calculate WER. Please draw an attention\n","    that empty items will be removed when splitting sentences by delimiter.\n","    :param reference: The reference sentence.\n","    :type reference: basestring\n","    :param hypothesis: The hypothesis sentence.\n","    :type hypothesis: basestring\n","    :param ignore_case: Whether case-sensitive or not.\n","    :type ignore_case: bool\n","    :param delimiter: Delimiter of input sentences.\n","    :type delimiter: char\n","    :return: Word error rate.\n","    :rtype: float\n","    :raises ValueError: If word number of reference is zero.\n","    \"\"\"\n","    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n","                                         delimiter)\n","\n","    if ref_len == 0:\n","        raise ValueError(\"Reference's word number should be greater than 0.\")\n","\n","    wer = float(edit_distance) / ref_len\n","    return wer\n","\n","\n","def cer(reference, hypothesis, ignore_case=False, remove_space=False):\n","    \"\"\"Calculate charactor error rate (CER). CER compares reference text and\n","    hypothesis text in char-level. CER is defined as:\n","    .. math::\n","        CER = (Sc + Dc + Ic) / Nc\n","    where\n","    .. code-block:: text\n","        Sc is the number of characters substituted,\n","        Dc is the number of characters deleted,\n","        Ic is the number of characters inserted\n","        Nc is the number of characters in the reference\n","    We can use levenshtein distance to calculate CER. Chinese input should be\n","    encoded to unicode. Please draw an attention that the leading and tailing\n","    space characters will be truncated and multiple consecutive space\n","    characters in a sentence will be replaced by one space character.\n","    :param reference: The reference sentence.\n","    :type reference: basestring\n","    :param hypothesis: The hypothesis sentence.\n","    :type hypothesis: basestring\n","    :param ignore_case: Whether case-sensitive or not.\n","    :type ignore_case: bool\n","    :param remove_space: Whether remove internal space characters\n","    :type remove_space: bool\n","    :return: Character error rate.\n","    :rtype: float\n","    :raises ValueError: If the reference length is zero.\n","    \"\"\"\n","    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n","                                         remove_space)\n","\n","    if ref_len == 0:\n","        raise ValueError(\"Length of reference should be greater than 0.\")\n","\n","    cer = float(edit_distance) / ref_len\n","    return cer\n","\n","class TextTransform:\n","    \"\"\"Maps characters to integers and vice versa\"\"\"\n","    def __init__(self):\n","        self.char_map = {\"а\": 0, \"б\": 1, \"в\": 2, \"г\": 3, \"д\": 4, \"е\": 5, \"ё\": 6, \"ж\": 7, \"з\": 8, \"и\": 9, \"й\": 10,\n","                  \"к\": 11, \"л\": 12, \"м\": 13, \"н\": 14, \"о\": 15, \"п\": 16, \"р\": 17, \"с\": 18, \"т\": 19, \"у\": 20,\n","                  \"ф\": 21, \"ч\": 22, \"ц\": 23, \"ш\": 24, \"щ\": 25, \"ъ\": 26, \"ы\": 27, \"ь\": 28, \"э\": 29, \"ю\": 30,\n","                  \"я\": 31, \"х\": 32, \" \": 33}\n","\n","        self.index_map = {}\n","        for key, value in self.char_map.items():\n","            self.index_map[value] = key\n","\n","    def text_to_int(self, text):\n","        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n","        int_sequence = []\n","        for c in text:\n","            if c != '':\n","                ch = self.char_map[c]\n","            int_sequence.append(ch)\n","        return int_sequence\n","\n","    def int_to_text(self, labels):\n","        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n","        string = []\n","        for i in labels:\n","            string.append(self.index_map[i])\n","        return ''.join(string)\n","\n","train_audio_transforms = nn.Sequential(\n","    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n","    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n","    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",")\n","\n","valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n","\n","text_transform = TextTransform()\n","\n","def data_processing(data, data_type=\"train\"):\n","    spectrograms = []\n","    labels = []\n","    input_lengths = []\n","    label_lengths = []\n","    for (waveform, utterance) in data:\n","        if data_type == 'train':\n","            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n","        elif data_type == 'valid':\n","            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n","        else:\n","            raise Exception('data_type should be train or valid')\n","        spectrograms.append(spec)\n","        label = torch.Tensor(text_transform.text_to_int(utterance))\n","        labels.append(label)\n","        input_lengths.append(spec.shape[0]//4)\n","        label_lengths.append(len(label))\n","    \n","    spectrograms1 = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n","            \n","    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n","\n","    return spectrograms1, labels, input_lengths, label_lengths\n","\n","\n","def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n","    arg_maxes = torch.argmax(output, dim=2)\n","    decodes = []\n","    targets = []\n","    for i, args in enumerate(arg_maxes):\n","        decode = []\n","        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n","        for j, index in enumerate(args):\n","            if index != blank_label:\n","                if collapse_repeated and j != 0 and index == args[j -1]:\n","                    continue\n","                decode.append(index.item())\n","        decodes.append(text_transform.int_to_text(decode))\n","    return decodes, targets"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2023-04-22T16:01:59.318957Z","iopub.status.busy":"2023-04-22T16:01:59.318569Z","iopub.status.idle":"2023-04-22T16:01:59.337059Z","shell.execute_reply":"2023-04-22T16:01:59.335966Z","shell.execute_reply.started":"2023-04-22T16:01:59.318898Z"},"trusted":true},"outputs":[],"source":["class CNNLayerNorm(nn.Module):\n","    \"\"\"Layer normalization built for cnns input\"\"\"\n","    def __init__(self, n_feats):\n","        super(CNNLayerNorm, self).__init__()\n","        self.layer_norm = nn.LayerNorm(n_feats)\n","\n","    def forward(self, x):\n","        # x (batch, channel, feature, time)\n","        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n","        x = self.layer_norm(x)\n","        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n","\n","\n","class ResidualCNN(nn.Module):\n","    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n","        except with layer norm instead of batch norm\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n","        super(ResidualCNN, self).__init__()\n","\n","        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n","        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n","        self.dropout1 = nn.Dropout(dropout)\n","        self.dropout2 = nn.Dropout(dropout)\n","        self.layer_norm1 = CNNLayerNorm(n_feats)\n","        self.layer_norm2 = CNNLayerNorm(n_feats)\n","\n","    def forward(self, x):\n","        residual = x  # (batch, channel, feature, time)\n","        x = self.layer_norm1(x)\n","        x = F.gelu(x)\n","        x = self.dropout1(x)\n","        x = self.cnn1(x)\n","        x = self.layer_norm2(x)\n","        x = F.gelu(x)\n","        x = self.dropout2(x)\n","        x = self.cnn2(x)\n","        x += residual\n","        return x # (batch, channel, feature, time)\n","\n","\n","class BidirectionalGRU(nn.Module):\n","\n","    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n","        super(BidirectionalGRU, self).__init__()\n","\n","        self.BiGRU = nn.GRU(\n","            input_size=rnn_dim, hidden_size=hidden_size,\n","            num_layers=1, batch_first=batch_first, bidirectional=True)\n","        self.layer_norm = nn.LayerNorm(rnn_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        x = self.layer_norm(x)\n","        x = F.gelu(x)\n","        x, _ = self.BiGRU(x)\n","        x = self.dropout(x)\n","        return x\n","\n","\n","class SpeechRecognitionModel(nn.Module):\n","    \n","    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n","        super(SpeechRecognitionModel, self).__init__()\n","        n_feats = n_feats//2\n","        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n","\n","        # n residual cnn layers with filter size of 32\n","        self.rescnn_layers = nn.Sequential(*[\n","            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n","            for _ in range(n_cnn_layers)\n","        ])\n","        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n","        self.birnn_layers = nn.Sequential(*[\n","            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n","                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n","            for i in range(n_rnn_layers)\n","        ])\n","        self.classifier = nn.Sequential(\n","            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(rnn_dim, n_class)\n","        )\n","\n","    def forward(self, x):\n","        x = self.cnn(x)\n","        x = self.rescnn_layers(x)\n","        sizes = x.size()\n","        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n","        x = x.transpose(1, 2) # (batch, time, feature)\n","        x = self.fully_connected(x)\n","        x = self.birnn_layers(x)\n","        x = self.classifier(x)\n","        return x"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2023-04-22T16:02:00.231377Z","iopub.status.busy":"2023-04-22T16:02:00.230458Z","iopub.status.idle":"2023-04-22T16:02:02.073930Z","shell.execute_reply":"2023-04-22T16:02:02.072790Z","shell.execute_reply.started":"2023-04-22T16:02:00.231326Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["100\n","200\n","300\n","400\n","500\n","600\n","700\n","800\n","900\n","1000\n"]}],"source":["import pandas as pd\n","import librosa\n","\n","file = pd.read_excel('/kaggle/input/rus-speech/russian_speech.xlsx')\n","y = [sentence for sentence in file['Русская речь']]\n","\n","dir_name = \"/kaggle/input/upd-speech/abnormal_voice/\"\n","files_in_dir = os.listdir(dir_name)\n","\n","X = []\n","i = 1\n","\n","for e in range(1, 1001):\n","    file_name = f'{e}.wav'\n","    sampl = librosa.load(dir_name + file_name, sr=16000)[0]\n","    sampl = sampl[np.newaxis, :]\n","    X.append(torch.Tensor(sampl))\n","    if i % 100 == 0:\n","        print(i)\n","    i += 1"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2023-04-22T16:02:02.076285Z","iopub.status.busy":"2023-04-22T16:02:02.075903Z","iopub.status.idle":"2023-04-22T16:02:02.085788Z","shell.execute_reply":"2023-04-22T16:02:02.084448Z","shell.execute_reply.started":"2023-04-22T16:02:02.076249Z"},"trusted":true},"outputs":[{"data":{"text/plain":["torch.Size([1, 79872])"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["X[0].shape"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2023-04-22T16:02:03.401883Z","iopub.status.busy":"2023-04-22T16:02:03.400961Z","iopub.status.idle":"2023-04-22T16:02:03.416583Z","shell.execute_reply":"2023-04-22T16:02:03.415481Z","shell.execute_reply.started":"2023-04-22T16:02:03.401832Z"},"trusted":true},"outputs":[],"source":["char_map = {\"а\": 0, \"б\": 1, \"в\": 2, \"г\": 3, \"д\": 4, \"е\": 5, \"ё\": 6, \"ж\": 7, \"з\": 8, \"и\": 9, \"й\": 10,\n","            \"к\": 11, \"л\": 12, \"м\": 13, \"н\": 14, \"о\": 15, \"п\": 16, \"р\": 17, \"с\": 18, \"т\": 19, \"у\": 20,\n","            \"ф\": 21, \"ч\": 22, \"ц\": 23, \"ш\": 24, \"щ\": 25, \"ъ\": 26, \"ы\": 27, \"ь\": 28, \"э\": 29, \"ю\": 30,\n","            \"я\": 31, \"х\": 32, \" \": 33}\n","\n","def remove_characters(sentence):\n","    sentence = sentence.lower()\n","    sentence = ''.join(filter(lambda x: x in char_map, sentence))\n","    return sentence\n","\n","y = list(map(remove_characters, y))"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2023-04-22T16:02:03.579569Z","iopub.status.busy":"2023-04-22T16:02:03.579054Z","iopub.status.idle":"2023-04-22T16:02:03.586845Z","shell.execute_reply":"2023-04-22T16:02:03.585681Z","shell.execute_reply.started":"2023-04-22T16:02:03.579538Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2023-04-22T16:02:03.742887Z","iopub.status.busy":"2023-04-22T16:02:03.742203Z","iopub.status.idle":"2023-04-22T16:02:03.749657Z","shell.execute_reply":"2023-04-22T16:02:03.748380Z","shell.execute_reply.started":"2023-04-22T16:02:03.742853Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset\n","\n","class AudioDataset(Dataset):\n","    def __init__(self, audio_list, text_list):\n","        self.audio_list = audio_list\n","        self.text_list = text_list\n","        \n","    def __len__(self):\n","        return len(self.text_list)\n","    \n","    def __getitem__(self, index):\n","        audio = self.audio_list[index]\n","        text = self.text_list[index]\n","        return audio, text"]},{"cell_type":"code","execution_count":95,"metadata":{"execution":{"iopub.execute_input":"2023-04-22T16:45:21.958850Z","iopub.status.busy":"2023-04-22T16:45:21.958436Z","iopub.status.idle":"2023-04-22T16:45:21.978050Z","shell.execute_reply":"2023-04-22T16:45:21.976009Z","shell.execute_reply.started":"2023-04-22T16:45:21.958815Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'nn.Linear(512, 128),\\n            nn.GELU(),\\n            nn.Dropout(0.35),'"]},"execution_count":95,"metadata":{},"output_type":"execute_result"}],"source":["class SpeechRecognitionModel1(nn.Module):\n","    def __init__(self, num_classes):\n","        super(SpeechRecognitionModel1, self).__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(1, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n","            nn.BatchNorm2d(64),\n","            nn.GELU(),\n","            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)),\n","            nn.Conv2d(64, 128, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n","            nn.BatchNorm2d(128),\n","            nn.GELU(),\n","            nn.Conv2d(128, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n","            nn.BatchNorm2d(64),\n","            nn.GELU(),\n","            nn.Conv2d(64, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n","            nn.GELU(),\n","            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)),\n","        )\n","        \n","        self.rnn = nn.GRU(input_size=2048, \n","                    hidden_size=256, \n","                    num_layers=1, \n","                    batch_first=True, \n","                    bidirectional=True)\n","        \n","        self.fc = nn.Sequential(\n","            nn.Linear(512, 128),\n","            nn.GELU(),\n","            nn.Linear(128, num_classes),\n","        )\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = x.permute(0, 3, 1, 2)\n","        x = x.view(x.size(0), x.size(1), -1)\n","        x, _ = self.rnn(x)\n","        x = self.fc(x)\n","        x = self.softmax(x)\n","        return x\n","    \n","\"\"\"nn.Linear(512, 128),\n","            nn.GELU(),\n","            nn.Dropout(0.35),\"\"\""]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2023-04-22T16:02:12.700111Z","iopub.status.busy":"2023-04-22T16:02:12.699663Z","iopub.status.idle":"2023-04-22T16:02:12.723713Z","shell.execute_reply":"2023-04-22T16:02:12.722610Z","shell.execute_reply.started":"2023-04-22T16:02:12.700076Z"},"trusted":true},"outputs":[],"source":["class IterMeter(object):\n","    \"\"\"keeps track of total iterations\"\"\"\n","    def __init__(self):\n","        self.val = 0\n","\n","    def step(self):\n","        self.val += 1\n","\n","    def get(self):\n","        return self.val\n","\n","\n","def train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter):\n","    model.train()\n","    data_len = len(train_loader.dataset)\n","    for batch_idx, _data in enumerate(train_loader):\n","        spectrograms, labels, input_lengths, label_lengths = _data \n","        spectrograms, labels = spectrograms.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        output = model(spectrograms)  # (batch, time, n_class)\n","        output = F.log_softmax(output, dim=2)\n","        output = output.transpose(0, 1) # (time, batch, n_class)\n","\n","        loss = criterion(output, labels, input_lengths, label_lengths)\n","        loss.backward()\n","\n","        optimizer.step()\n","        scheduler.step()\n","        iter_meter.step()\n","        if batch_idx % 10 == 0 or batch_idx == data_len:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(spectrograms), data_len,\n","                100. * batch_idx / len(train_loader), loss.item()))\n","\n","\n","def test(model, device, test_loader, criterion, epoch, iter_meter):\n","    print('\\nevaluating...')\n","    model.eval()\n","    test_loss = 0\n","    test_cer, test_wer = [], []\n","    with torch.no_grad():\n","        for i, _data in enumerate(test_loader):\n","            spectrograms, labels, input_lengths, label_lengths = _data \n","            spectrograms, labels = spectrograms.to(device), labels.to(device)\n","\n","            output = model(spectrograms)  # (batch, time, n_class)\n","            output = F.log_softmax(output, dim=2)\n","            output = output.transpose(0, 1) # (time, batch, n_class)\n","\n","            loss = criterion(output, labels, input_lengths, label_lengths)\n","            test_loss += loss.item() / len(test_loader)\n","\n","            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n","            for j in range(len(decoded_preds)):\n","                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n","                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n","\n","\n","    avg_cer = sum(test_cer)/len(test_cer)\n","    avg_wer = sum(test_wer)/len(test_wer)\n","\n","    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n","\n","\n","def main(learning_rate=5e-4, batch_size=20, epochs=10):\n","\n","    hparams = {\n","        \"n_cnn_layers\": 2,\n","        \"n_rnn_layers\": 2,\n","        \"rnn_dim\": 256,\n","        \"n_class\": 34,\n","        \"n_feats\": 128,\n","        \"stride\":2,\n","        \"dropout\": 0.1,\n","        \"learning_rate\": learning_rate,\n","        \"batch_size\": batch_size,\n","        \"epochs\": epochs\n","    }\n","\n","    use_cuda = torch.cuda.is_available()\n","    torch.manual_seed(7)\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    train_dataset = AudioDataset(X_train, y_train)\n","    test_dataset = AudioDataset(X_test, y_test)\n","\n","    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n","    train_loader = data.DataLoader(dataset=train_dataset,\n","                                batch_size=hparams['batch_size'],\n","                                shuffle=True,\n","                                collate_fn=lambda x: data_processing(x, 'train'),\n","                                **kwargs)\n","    test_loader = data.DataLoader(dataset=test_dataset,\n","                                batch_size=hparams['batch_size'],\n","                                shuffle=False,\n","                                collate_fn=lambda x: data_processing(x, 'valid'),\n","                                **kwargs)\n","\n","    model = SpeechRecognitionModel1(34).to(device)\n","    \"\"\"model = SpeechRecognitionModel(\n","        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n","        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n","        ).to(device)\"\"\"\n","\n","    print(model)\n","    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n","\n","    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n","    criterion = nn.CTCLoss(blank=28).to(device)\n","    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n","                                            steps_per_epoch=int(len(train_loader)),\n","                                            epochs=hparams['epochs'],\n","                                            anneal_strategy='linear')\n","    \n","    iter_meter = IterMeter()\n","    for epoch in range(1, epochs + 1):\n","        train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\n","        test(model, device, test_loader, criterion, epoch, iter_meter)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-22T16:45:25.817296Z","iopub.status.busy":"2023-04-22T16:45:25.816591Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["SpeechRecognitionModel1(\n","  (conv): Sequential(\n","    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): GELU(approximate='none')\n","    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n","    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (6): GELU(approximate='none')\n","    (7): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (9): GELU(approximate='none')\n","    (10): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): GELU(approximate='none')\n","    (12): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (rnn): GRU(2048, 256, batch_first=True, bidirectional=True)\n","  (fc): Sequential(\n","    (0): Linear(in_features=512, out_features=128, bias=True)\n","    (1): GELU(approximate='none')\n","    (2): Linear(in_features=128, out_features=34, bias=True)\n","  )\n","  (softmax): LogSoftmax(dim=1)\n",")\n","Num Model Parameters 3797794\n","Train Epoch: 1 [0/900 (0%)]\tLoss: 12.265035\n","Train Epoch: 1 [100/900 (11%)]\tLoss: 10.244518\n","Train Epoch: 1 [200/900 (22%)]\tLoss: 11.821872\n","Train Epoch: 1 [300/900 (33%)]\tLoss: 10.533934\n","Train Epoch: 1 [400/900 (44%)]\tLoss: 10.265449\n","Train Epoch: 1 [500/900 (56%)]\tLoss: 11.833995\n","Train Epoch: 1 [600/900 (67%)]\tLoss: 11.082906\n","Train Epoch: 1 [700/900 (78%)]\tLoss: 10.221188\n","Train Epoch: 1 [800/900 (89%)]\tLoss: 9.064540\n","\n","evaluating...\n","Test set: Average loss: 10.3092, Average CER: 0.969988 Average WER: 1.0000\n","\n","Train Epoch: 2 [0/900 (0%)]\tLoss: 10.992615\n","Train Epoch: 2 [100/900 (11%)]\tLoss: 9.801940\n","Train Epoch: 2 [200/900 (22%)]\tLoss: 8.970467\n","Train Epoch: 2 [300/900 (33%)]\tLoss: 9.284268\n","Train Epoch: 2 [400/900 (44%)]\tLoss: 9.647020\n","Train Epoch: 2 [500/900 (56%)]\tLoss: 9.139844\n","Train Epoch: 2 [600/900 (67%)]\tLoss: 9.780423\n","Train Epoch: 2 [700/900 (78%)]\tLoss: 10.659213\n","Train Epoch: 2 [800/900 (89%)]\tLoss: 10.004518\n","\n","evaluating...\n","Test set: Average loss: 6.9685, Average CER: 1.004104 Average WER: 1.0000\n","\n","Train Epoch: 3 [0/900 (0%)]\tLoss: 7.725401\n","Train Epoch: 3 [100/900 (11%)]\tLoss: 6.403322\n","Train Epoch: 3 [200/900 (22%)]\tLoss: 8.779566\n","Train Epoch: 3 [300/900 (33%)]\tLoss: 4.879209\n","Train Epoch: 3 [400/900 (44%)]\tLoss: 5.869485\n","Train Epoch: 3 [500/900 (56%)]\tLoss: 4.790915\n","Train Epoch: 3 [600/900 (67%)]\tLoss: 5.565995\n","Train Epoch: 3 [700/900 (78%)]\tLoss: 4.063441\n","Train Epoch: 3 [800/900 (89%)]\tLoss: 4.167707\n","\n","evaluating...\n","Test set: Average loss: 4.4105, Average CER: 0.959700 Average WER: 1.0000\n","\n","Train Epoch: 4 [0/900 (0%)]\tLoss: 4.502885\n","Train Epoch: 4 [100/900 (11%)]\tLoss: 3.741678\n","Train Epoch: 4 [200/900 (22%)]\tLoss: 4.036948\n","Train Epoch: 4 [300/900 (33%)]\tLoss: 4.256277\n","Train Epoch: 4 [400/900 (44%)]\tLoss: 3.782094\n","Train Epoch: 4 [500/900 (56%)]\tLoss: 4.006182\n","Train Epoch: 4 [600/900 (67%)]\tLoss: 3.646770\n","Train Epoch: 4 [700/900 (78%)]\tLoss: 3.907647\n","Train Epoch: 4 [800/900 (89%)]\tLoss: 3.554085\n","\n","evaluating...\n","Test set: Average loss: 3.7663, Average CER: 0.982116 Average WER: 1.0752\n","\n","Train Epoch: 5 [0/900 (0%)]\tLoss: 3.796948\n","Train Epoch: 5 [100/900 (11%)]\tLoss: 3.826939\n","Train Epoch: 5 [200/900 (22%)]\tLoss: 3.491627\n","Train Epoch: 5 [300/900 (33%)]\tLoss: 3.664868\n","Train Epoch: 5 [400/900 (44%)]\tLoss: 3.596797\n","Train Epoch: 5 [500/900 (56%)]\tLoss: 3.175581\n","Train Epoch: 5 [600/900 (67%)]\tLoss: 3.347099\n","Train Epoch: 5 [700/900 (78%)]\tLoss: 3.341534\n","Train Epoch: 5 [800/900 (89%)]\tLoss: 3.162967\n","\n","evaluating...\n","Test set: Average loss: 3.3431, Average CER: 0.981304 Average WER: 1.0000\n","\n","Train Epoch: 6 [0/900 (0%)]\tLoss: 3.611168\n","Train Epoch: 6 [100/900 (11%)]\tLoss: 3.302932\n","Train Epoch: 6 [200/900 (22%)]\tLoss: 3.558591\n","Train Epoch: 6 [300/900 (33%)]\tLoss: 3.485109\n","Train Epoch: 6 [400/900 (44%)]\tLoss: 3.342442\n","Train Epoch: 6 [500/900 (56%)]\tLoss: 3.104277\n","Train Epoch: 6 [600/900 (67%)]\tLoss: 3.690970\n","Train Epoch: 6 [700/900 (78%)]\tLoss: 3.232396\n","Train Epoch: 6 [800/900 (89%)]\tLoss: 3.304963\n","\n","evaluating...\n","Test set: Average loss: 3.2349, Average CER: 0.970931 Average WER: 1.0000\n","\n","Train Epoch: 7 [0/900 (0%)]\tLoss: 3.245757\n","Train Epoch: 7 [100/900 (11%)]\tLoss: 3.341102\n","Train Epoch: 7 [200/900 (22%)]\tLoss: 3.068076\n","Train Epoch: 7 [300/900 (33%)]\tLoss: 3.152831\n","Train Epoch: 7 [400/900 (44%)]\tLoss: 3.063792\n","Train Epoch: 7 [500/900 (56%)]\tLoss: 3.081379\n","Train Epoch: 7 [600/900 (67%)]\tLoss: 3.315295\n","Train Epoch: 7 [700/900 (78%)]\tLoss: 3.256223\n","Train Epoch: 7 [800/900 (89%)]\tLoss: 3.284388\n","\n","evaluating...\n","Test set: Average loss: 3.2191, Average CER: 0.964825 Average WER: 1.0000\n","\n","Train Epoch: 8 [0/900 (0%)]\tLoss: 3.151129\n","Train Epoch: 8 [100/900 (11%)]\tLoss: 3.090006\n","Train Epoch: 8 [200/900 (22%)]\tLoss: 3.290111\n","Train Epoch: 8 [300/900 (33%)]\tLoss: 3.184175\n","Train Epoch: 8 [400/900 (44%)]\tLoss: 2.901906\n","Train Epoch: 8 [500/900 (56%)]\tLoss: 3.365335\n","Train Epoch: 8 [600/900 (67%)]\tLoss: 3.239249\n","Train Epoch: 8 [700/900 (78%)]\tLoss: 3.215264\n","Train Epoch: 8 [800/900 (89%)]\tLoss: 3.246654\n","\n","evaluating...\n","Test set: Average loss: 3.1832, Average CER: 0.953121 Average WER: 1.0000\n","\n","Train Epoch: 9 [0/900 (0%)]\tLoss: 3.129723\n","Train Epoch: 9 [100/900 (11%)]\tLoss: 3.032275\n","Train Epoch: 9 [200/900 (22%)]\tLoss: 2.898948\n","Train Epoch: 9 [300/900 (33%)]\tLoss: 3.056439\n","Train Epoch: 9 [400/900 (44%)]\tLoss: 3.061276\n","Train Epoch: 9 [500/900 (56%)]\tLoss: 3.220918\n","Train Epoch: 9 [600/900 (67%)]\tLoss: 3.261778\n","Train Epoch: 9 [700/900 (78%)]\tLoss: 3.088760\n","Train Epoch: 9 [800/900 (89%)]\tLoss: 3.064336\n","\n","evaluating...\n","Test set: Average loss: 3.1641, Average CER: 0.936408 Average WER: 1.0000\n","\n","Train Epoch: 10 [0/900 (0%)]\tLoss: 2.886799\n","Train Epoch: 10 [100/900 (11%)]\tLoss: 3.109587\n","Train Epoch: 10 [200/900 (22%)]\tLoss: 3.115734\n","Train Epoch: 10 [300/900 (33%)]\tLoss: 3.072966\n","Train Epoch: 10 [400/900 (44%)]\tLoss: 3.229708\n","Train Epoch: 10 [500/900 (56%)]\tLoss: 3.140054\n","Train Epoch: 10 [600/900 (67%)]\tLoss: 3.057136\n","Train Epoch: 10 [700/900 (78%)]\tLoss: 3.115516\n","Train Epoch: 10 [800/900 (89%)]\tLoss: 3.136168\n","\n","evaluating...\n","Test set: Average loss: 3.1435, Average CER: 0.951363 Average WER: 1.0000\n","\n","Train Epoch: 11 [0/900 (0%)]\tLoss: 3.202473\n","Train Epoch: 11 [100/900 (11%)]\tLoss: 3.190946\n","Train Epoch: 11 [200/900 (22%)]\tLoss: 3.171634\n","Train Epoch: 11 [300/900 (33%)]\tLoss: 2.997189\n","Train Epoch: 11 [400/900 (44%)]\tLoss: 3.066371\n","Train Epoch: 11 [500/900 (56%)]\tLoss: 3.081600\n","Train Epoch: 11 [600/900 (67%)]\tLoss: 3.071889\n","Train Epoch: 11 [700/900 (78%)]\tLoss: 3.178649\n","Train Epoch: 11 [800/900 (89%)]\tLoss: 2.915858\n","\n","evaluating...\n","Test set: Average loss: 3.1374, Average CER: 0.932921 Average WER: 1.0000\n","\n","Train Epoch: 12 [0/900 (0%)]\tLoss: 3.018284\n","Train Epoch: 12 [100/900 (11%)]\tLoss: 3.150153\n","Train Epoch: 12 [200/900 (22%)]\tLoss: 2.964222\n","Train Epoch: 12 [300/900 (33%)]\tLoss: 3.098699\n","Train Epoch: 12 [400/900 (44%)]\tLoss: 3.041979\n","Train Epoch: 12 [500/900 (56%)]\tLoss: 3.153472\n","Train Epoch: 12 [600/900 (67%)]\tLoss: 3.020569\n","Train Epoch: 12 [700/900 (78%)]\tLoss: 3.029241\n","Train Epoch: 12 [800/900 (89%)]\tLoss: 3.143673\n","\n","evaluating...\n","Test set: Average loss: 3.1059, Average CER: 0.932819 Average WER: 1.0000\n","\n","Train Epoch: 13 [0/900 (0%)]\tLoss: 3.000185\n","Train Epoch: 13 [100/900 (11%)]\tLoss: 3.059890\n","Train Epoch: 13 [200/900 (22%)]\tLoss: 3.053439\n","Train Epoch: 13 [300/900 (33%)]\tLoss: 3.124644\n","Train Epoch: 13 [400/900 (44%)]\tLoss: 3.103388\n","Train Epoch: 13 [500/900 (56%)]\tLoss: 2.948405\n","Train Epoch: 13 [600/900 (67%)]\tLoss: 3.108810\n","Train Epoch: 13 [700/900 (78%)]\tLoss: 3.164974\n","Train Epoch: 13 [800/900 (89%)]\tLoss: 2.902054\n","\n","evaluating...\n","Test set: Average loss: 3.0671, Average CER: 0.930423 Average WER: 1.0000\n","\n","Train Epoch: 14 [0/900 (0%)]\tLoss: 3.051732\n","Train Epoch: 14 [100/900 (11%)]\tLoss: 3.096223\n","Train Epoch: 14 [200/900 (22%)]\tLoss: 3.126546\n","Train Epoch: 14 [300/900 (33%)]\tLoss: 3.035084\n","Train Epoch: 14 [400/900 (44%)]\tLoss: 3.018119\n","Train Epoch: 14 [500/900 (56%)]\tLoss: 3.046373\n","Train Epoch: 14 [600/900 (67%)]\tLoss: 2.945817\n","Train Epoch: 14 [700/900 (78%)]\tLoss: 2.778517\n","Train Epoch: 14 [800/900 (89%)]\tLoss: 3.054949\n","\n","evaluating...\n","Test set: Average loss: 3.0965, Average CER: 0.933503 Average WER: 1.0000\n","\n","Train Epoch: 15 [0/900 (0%)]\tLoss: 3.221055\n","Train Epoch: 15 [100/900 (11%)]\tLoss: 3.082696\n","Train Epoch: 15 [200/900 (22%)]\tLoss: 2.943979\n","Train Epoch: 15 [300/900 (33%)]\tLoss: 3.126443\n","Train Epoch: 15 [400/900 (44%)]\tLoss: 2.977577\n","Train Epoch: 15 [500/900 (56%)]\tLoss: 3.119735\n","Train Epoch: 15 [600/900 (67%)]\tLoss: 3.021311\n","Train Epoch: 15 [700/900 (78%)]\tLoss: 2.922657\n","Train Epoch: 15 [800/900 (89%)]\tLoss: 2.960299\n","\n","evaluating...\n","Test set: Average loss: 2.9955, Average CER: 0.936374 Average WER: 1.0000\n","\n","Train Epoch: 16 [0/900 (0%)]\tLoss: 3.001853\n","Train Epoch: 16 [100/900 (11%)]\tLoss: 2.978594\n","Train Epoch: 16 [200/900 (22%)]\tLoss: 2.941176\n","Train Epoch: 16 [300/900 (33%)]\tLoss: 2.986175\n","Train Epoch: 16 [400/900 (44%)]\tLoss: 2.913072\n","Train Epoch: 16 [500/900 (56%)]\tLoss: 2.993183\n","Train Epoch: 16 [600/900 (67%)]\tLoss: 3.048349\n","Train Epoch: 16 [700/900 (78%)]\tLoss: 2.974499\n","Train Epoch: 16 [800/900 (89%)]\tLoss: 2.876100\n","\n","evaluating...\n","Test set: Average loss: 2.9557, Average CER: 0.933325 Average WER: 1.0000\n","\n","Train Epoch: 17 [0/900 (0%)]\tLoss: 2.935081\n","Train Epoch: 17 [100/900 (11%)]\tLoss: 3.090155\n","Train Epoch: 17 [200/900 (22%)]\tLoss: 3.009092\n","Train Epoch: 17 [300/900 (33%)]\tLoss: 2.810921\n","Train Epoch: 17 [400/900 (44%)]\tLoss: 2.906806\n","Train Epoch: 17 [500/900 (56%)]\tLoss: 2.921199\n","Train Epoch: 17 [600/900 (67%)]\tLoss: 3.108717\n","Train Epoch: 17 [700/900 (78%)]\tLoss: 2.822756\n","Train Epoch: 17 [800/900 (89%)]\tLoss: 3.090810\n","\n","evaluating...\n","Test set: Average loss: 2.8918, Average CER: 0.938694 Average WER: 1.0000\n","\n","Train Epoch: 18 [0/900 (0%)]\tLoss: 2.910877\n","Train Epoch: 18 [100/900 (11%)]\tLoss: 2.941802\n","Train Epoch: 18 [200/900 (22%)]\tLoss: 3.079222\n","Train Epoch: 18 [300/900 (33%)]\tLoss: 3.003516\n","Train Epoch: 18 [400/900 (44%)]\tLoss: 2.841828\n","Train Epoch: 18 [500/900 (56%)]\tLoss: 3.108747\n","Train Epoch: 18 [600/900 (67%)]\tLoss: 2.763873\n","Train Epoch: 18 [700/900 (78%)]\tLoss: 2.805729\n","Train Epoch: 18 [800/900 (89%)]\tLoss: 2.986123\n","\n","evaluating...\n","Test set: Average loss: 2.8159, Average CER: 0.951750 Average WER: 1.0000\n","\n","Train Epoch: 19 [0/900 (0%)]\tLoss: 2.771736\n","Train Epoch: 19 [100/900 (11%)]\tLoss: 2.929471\n","Train Epoch: 19 [200/900 (22%)]\tLoss: 2.804426\n","Train Epoch: 19 [300/900 (33%)]\tLoss: 2.995668\n","Train Epoch: 19 [400/900 (44%)]\tLoss: 2.941821\n","Train Epoch: 19 [500/900 (56%)]\tLoss: 2.815310\n","Train Epoch: 19 [600/900 (67%)]\tLoss: 3.005075\n","Train Epoch: 19 [700/900 (78%)]\tLoss: 2.880907\n","Train Epoch: 19 [800/900 (89%)]\tLoss: 3.004189\n","\n","evaluating...\n","Test set: Average loss: 2.7209, Average CER: 0.942951 Average WER: 1.0000\n","\n","Train Epoch: 20 [0/900 (0%)]\tLoss: 2.746949\n","Train Epoch: 20 [100/900 (11%)]\tLoss: 2.725300\n","Train Epoch: 20 [200/900 (22%)]\tLoss: 2.821354\n","Train Epoch: 20 [300/900 (33%)]\tLoss: 2.944612\n","Train Epoch: 20 [400/900 (44%)]\tLoss: 2.761673\n","Train Epoch: 20 [500/900 (56%)]\tLoss: 2.822039\n","Train Epoch: 20 [600/900 (67%)]\tLoss: 2.828492\n","Train Epoch: 20 [700/900 (78%)]\tLoss: 3.048596\n","Train Epoch: 20 [800/900 (89%)]\tLoss: 3.150243\n","\n","evaluating...\n","Test set: Average loss: 2.6097, Average CER: 0.902813 Average WER: 1.0100\n","\n","Train Epoch: 21 [0/900 (0%)]\tLoss: 2.608045\n","Train Epoch: 21 [100/900 (11%)]\tLoss: 2.662219\n","Train Epoch: 21 [200/900 (22%)]\tLoss: 3.210115\n","Train Epoch: 21 [300/900 (33%)]\tLoss: 2.813777\n","Train Epoch: 21 [400/900 (44%)]\tLoss: 2.630284\n","Train Epoch: 21 [500/900 (56%)]\tLoss: 2.662005\n","Train Epoch: 21 [600/900 (67%)]\tLoss: 2.432672\n","Train Epoch: 21 [700/900 (78%)]\tLoss: 2.565946\n","Train Epoch: 21 [800/900 (89%)]\tLoss: 2.578023\n","\n","evaluating...\n","Test set: Average loss: 2.5096, Average CER: 0.823909 Average WER: 1.0113\n","\n","Train Epoch: 22 [0/900 (0%)]\tLoss: 2.610991\n","Train Epoch: 22 [100/900 (11%)]\tLoss: 2.608662\n","Train Epoch: 22 [200/900 (22%)]\tLoss: 2.467092\n","Train Epoch: 22 [300/900 (33%)]\tLoss: 2.751757\n","Train Epoch: 22 [400/900 (44%)]\tLoss: 2.716553\n","Train Epoch: 22 [500/900 (56%)]\tLoss: 2.436863\n","Train Epoch: 22 [600/900 (67%)]\tLoss: 2.406029\n","Train Epoch: 22 [700/900 (78%)]\tLoss: 2.576356\n","Train Epoch: 22 [800/900 (89%)]\tLoss: 2.667389\n","\n","evaluating...\n","Test set: Average loss: 2.4167, Average CER: 0.768875 Average WER: 1.0125\n","\n","Train Epoch: 23 [0/900 (0%)]\tLoss: 2.631514\n","Train Epoch: 23 [100/900 (11%)]\tLoss: 2.565188\n","Train Epoch: 23 [200/900 (22%)]\tLoss: 2.484240\n","Train Epoch: 23 [300/900 (33%)]\tLoss: 2.466734\n","Train Epoch: 23 [400/900 (44%)]\tLoss: 2.482130\n","Train Epoch: 23 [500/900 (56%)]\tLoss: 2.745614\n","Train Epoch: 23 [600/900 (67%)]\tLoss: 2.382291\n","Train Epoch: 23 [700/900 (78%)]\tLoss: 2.286316\n","Train Epoch: 23 [800/900 (89%)]\tLoss: 2.378684\n","\n","evaluating...\n","Test set: Average loss: 2.2553, Average CER: 0.709848 Average WER: 1.0300\n","\n","Train Epoch: 24 [0/900 (0%)]\tLoss: 2.291166\n","Train Epoch: 24 [100/900 (11%)]\tLoss: 2.646219\n","Train Epoch: 24 [200/900 (22%)]\tLoss: 2.359732\n","Train Epoch: 24 [300/900 (33%)]\tLoss: 2.377091\n","Train Epoch: 24 [400/900 (44%)]\tLoss: 2.564656\n","Train Epoch: 24 [500/900 (56%)]\tLoss: 2.453891\n","Train Epoch: 24 [600/900 (67%)]\tLoss: 2.346116\n","Train Epoch: 24 [700/900 (78%)]\tLoss: 2.310172\n","Train Epoch: 24 [800/900 (89%)]\tLoss: 2.419559\n","\n","evaluating...\n","Test set: Average loss: 2.1430, Average CER: 0.694197 Average WER: 1.0130\n","\n","Train Epoch: 25 [0/900 (0%)]\tLoss: 2.102622\n","Train Epoch: 25 [100/900 (11%)]\tLoss: 2.329289\n","Train Epoch: 25 [200/900 (22%)]\tLoss: 2.364152\n","Train Epoch: 25 [300/900 (33%)]\tLoss: 2.386322\n","Train Epoch: 25 [400/900 (44%)]\tLoss: 2.363431\n","Train Epoch: 25 [500/900 (56%)]\tLoss: 2.588372\n","Train Epoch: 25 [600/900 (67%)]\tLoss: 2.261183\n","Train Epoch: 25 [700/900 (78%)]\tLoss: 2.313560\n","Train Epoch: 25 [800/900 (89%)]\tLoss: 2.066676\n","\n","evaluating...\n","Test set: Average loss: 2.0470, Average CER: 0.678109 Average WER: 1.0130\n","\n","Train Epoch: 26 [0/900 (0%)]\tLoss: 2.240425\n","Train Epoch: 26 [100/900 (11%)]\tLoss: 2.529582\n","Train Epoch: 26 [200/900 (22%)]\tLoss: 2.321407\n","Train Epoch: 26 [300/900 (33%)]\tLoss: 2.341588\n","Train Epoch: 26 [400/900 (44%)]\tLoss: 2.349994\n","Train Epoch: 26 [500/900 (56%)]\tLoss: 2.362286\n","Train Epoch: 26 [600/900 (67%)]\tLoss: 2.212876\n","Train Epoch: 26 [700/900 (78%)]\tLoss: 2.405272\n","Train Epoch: 26 [800/900 (89%)]\tLoss: 2.096692\n","\n","evaluating...\n","Test set: Average loss: 1.9823, Average CER: 0.675699 Average WER: 1.0080\n","\n","Train Epoch: 27 [0/900 (0%)]\tLoss: 2.353556\n","Train Epoch: 27 [100/900 (11%)]\tLoss: 2.394422\n","Train Epoch: 27 [200/900 (22%)]\tLoss: 2.248969\n","Train Epoch: 27 [300/900 (33%)]\tLoss: 2.485780\n","Train Epoch: 27 [400/900 (44%)]\tLoss: 2.159736\n","Train Epoch: 27 [500/900 (56%)]\tLoss: 2.207346\n","Train Epoch: 27 [600/900 (67%)]\tLoss: 2.240075\n","Train Epoch: 27 [700/900 (78%)]\tLoss: 2.239267\n","Train Epoch: 27 [800/900 (89%)]\tLoss: 1.988915\n","\n","evaluating...\n","Test set: Average loss: 1.9062, Average CER: 0.633392 Average WER: 1.0550\n","\n","Train Epoch: 28 [0/900 (0%)]\tLoss: 2.108279\n","Train Epoch: 28 [100/900 (11%)]\tLoss: 2.336700\n","Train Epoch: 28 [200/900 (22%)]\tLoss: 2.216344\n","Train Epoch: 28 [300/900 (33%)]\tLoss: 1.912202\n","Train Epoch: 28 [400/900 (44%)]\tLoss: 2.332389\n","Train Epoch: 28 [500/900 (56%)]\tLoss: 1.828599\n","Train Epoch: 28 [600/900 (67%)]\tLoss: 2.031823\n","Train Epoch: 28 [700/900 (78%)]\tLoss: 2.199732\n","Train Epoch: 28 [800/900 (89%)]\tLoss: 2.040156\n","\n","evaluating...\n","Test set: Average loss: 1.8633, Average CER: 0.635010 Average WER: 1.0658\n","\n","Train Epoch: 29 [0/900 (0%)]\tLoss: 2.109662\n","Train Epoch: 29 [100/900 (11%)]\tLoss: 2.093581\n","Train Epoch: 29 [200/900 (22%)]\tLoss: 2.090437\n","Train Epoch: 29 [300/900 (33%)]\tLoss: 2.198176\n","Train Epoch: 29 [400/900 (44%)]\tLoss: 2.083127\n","Train Epoch: 29 [500/900 (56%)]\tLoss: 2.055619\n","Train Epoch: 29 [600/900 (67%)]\tLoss: 1.820006\n","Train Epoch: 29 [700/900 (78%)]\tLoss: 2.288908\n","Train Epoch: 29 [800/900 (89%)]\tLoss: 1.917813\n","\n","evaluating...\n","Test set: Average loss: 1.7580, Average CER: 0.632666 Average WER: 1.0200\n","\n","Train Epoch: 30 [0/900 (0%)]\tLoss: 1.967250\n","Train Epoch: 30 [100/900 (11%)]\tLoss: 1.937811\n","Train Epoch: 30 [200/900 (22%)]\tLoss: 2.033335\n","Train Epoch: 30 [300/900 (33%)]\tLoss: 2.237518\n","Train Epoch: 30 [400/900 (44%)]\tLoss: 2.040182\n","Train Epoch: 30 [500/900 (56%)]\tLoss: 2.220152\n","Train Epoch: 30 [600/900 (67%)]\tLoss: 2.462484\n","Train Epoch: 30 [700/900 (78%)]\tLoss: 2.374565\n","Train Epoch: 30 [800/900 (89%)]\tLoss: 1.872848\n","\n","evaluating...\n","Test set: Average loss: 1.7572, Average CER: 0.611215 Average WER: 1.0339\n","\n","Train Epoch: 31 [0/900 (0%)]\tLoss: 2.196247\n","Train Epoch: 31 [100/900 (11%)]\tLoss: 2.219199\n","Train Epoch: 31 [200/900 (22%)]\tLoss: 1.588713\n","Train Epoch: 31 [300/900 (33%)]\tLoss: 2.002120\n","Train Epoch: 31 [400/900 (44%)]\tLoss: 2.092585\n","Train Epoch: 31 [500/900 (56%)]\tLoss: 1.917501\n","Train Epoch: 31 [600/900 (67%)]\tLoss: 1.873656\n","Train Epoch: 31 [700/900 (78%)]\tLoss: 1.815979\n","Train Epoch: 31 [800/900 (89%)]\tLoss: 1.858022\n","\n","evaluating...\n","Test set: Average loss: 1.7488, Average CER: 0.590596 Average WER: 1.0759\n","\n","Train Epoch: 32 [0/900 (0%)]\tLoss: 2.162105\n","Train Epoch: 32 [100/900 (11%)]\tLoss: 1.783811\n","Train Epoch: 32 [200/900 (22%)]\tLoss: 2.005081\n","Train Epoch: 32 [300/900 (33%)]\tLoss: 1.684342\n","Train Epoch: 32 [400/900 (44%)]\tLoss: 2.160098\n","Train Epoch: 32 [500/900 (56%)]\tLoss: 1.763211\n","Train Epoch: 32 [600/900 (67%)]\tLoss: 1.640723\n","Train Epoch: 32 [700/900 (78%)]\tLoss: 1.671304\n","Train Epoch: 32 [800/900 (89%)]\tLoss: 1.935285\n","\n","evaluating...\n","Test set: Average loss: 1.6240, Average CER: 0.560049 Average WER: 1.0280\n","\n","Train Epoch: 33 [0/900 (0%)]\tLoss: 1.910085\n","Train Epoch: 33 [100/900 (11%)]\tLoss: 1.870178\n","Train Epoch: 33 [200/900 (22%)]\tLoss: 2.221246\n","Train Epoch: 33 [300/900 (33%)]\tLoss: 1.993613\n","Train Epoch: 33 [400/900 (44%)]\tLoss: 1.795780\n","Train Epoch: 33 [500/900 (56%)]\tLoss: 1.985579\n","Train Epoch: 33 [600/900 (67%)]\tLoss: 1.790260\n","Train Epoch: 33 [700/900 (78%)]\tLoss: 1.947778\n","Train Epoch: 33 [800/900 (89%)]\tLoss: 1.884674\n","\n","evaluating...\n","Test set: Average loss: 1.5809, Average CER: 0.533708 Average WER: 1.0150\n","\n","Train Epoch: 34 [0/900 (0%)]\tLoss: 2.088810\n","Train Epoch: 34 [100/900 (11%)]\tLoss: 1.756961\n","Train Epoch: 34 [200/900 (22%)]\tLoss: 2.067070\n","Train Epoch: 34 [300/900 (33%)]\tLoss: 1.922287\n","Train Epoch: 34 [400/900 (44%)]\tLoss: 1.839278\n","Train Epoch: 34 [500/900 (56%)]\tLoss: 1.675933\n","Train Epoch: 34 [600/900 (67%)]\tLoss: 1.788797\n","Train Epoch: 34 [700/900 (78%)]\tLoss: 1.494745\n","Train Epoch: 34 [800/900 (89%)]\tLoss: 1.831713\n","\n","evaluating...\n","Test set: Average loss: 1.5374, Average CER: 0.499652 Average WER: 1.0266\n","\n","Train Epoch: 35 [0/900 (0%)]\tLoss: 1.390717\n","Train Epoch: 35 [100/900 (11%)]\tLoss: 1.730216\n","Train Epoch: 35 [200/900 (22%)]\tLoss: 1.904276\n","Train Epoch: 35 [300/900 (33%)]\tLoss: 1.752010\n","Train Epoch: 35 [400/900 (44%)]\tLoss: 1.592380\n","Train Epoch: 35 [500/900 (56%)]\tLoss: 1.857775\n","Train Epoch: 35 [600/900 (67%)]\tLoss: 1.553799\n","Train Epoch: 35 [700/900 (78%)]\tLoss: 1.963440\n","Train Epoch: 35 [800/900 (89%)]\tLoss: 1.839545\n","\n","evaluating...\n","Test set: Average loss: 1.4923, Average CER: 0.500908 Average WER: 1.0185\n","\n","Train Epoch: 36 [0/900 (0%)]\tLoss: 1.916065\n","Train Epoch: 36 [100/900 (11%)]\tLoss: 2.058777\n","Train Epoch: 36 [200/900 (22%)]\tLoss: 1.582274\n","Train Epoch: 36 [300/900 (33%)]\tLoss: 1.933758\n","Train Epoch: 36 [400/900 (44%)]\tLoss: 1.709020\n","Train Epoch: 36 [500/900 (56%)]\tLoss: 1.824163\n","Train Epoch: 36 [600/900 (67%)]\tLoss: 2.319005\n","Train Epoch: 36 [700/900 (78%)]\tLoss: 1.440336\n","Train Epoch: 36 [800/900 (89%)]\tLoss: 1.550661\n","\n","evaluating...\n","Test set: Average loss: 1.4681, Average CER: 0.503204 Average WER: 0.9955\n","\n","Train Epoch: 37 [0/900 (0%)]\tLoss: 1.754516\n","Train Epoch: 37 [100/900 (11%)]\tLoss: 1.652270\n","Train Epoch: 37 [200/900 (22%)]\tLoss: 1.864281\n","Train Epoch: 37 [300/900 (33%)]\tLoss: 1.580336\n","Train Epoch: 37 [400/900 (44%)]\tLoss: 1.585723\n","Train Epoch: 37 [500/900 (56%)]\tLoss: 2.072208\n","Train Epoch: 37 [600/900 (67%)]\tLoss: 1.752981\n","Train Epoch: 37 [700/900 (78%)]\tLoss: 1.912268\n","Train Epoch: 37 [800/900 (89%)]\tLoss: 1.698004\n","\n","evaluating...\n","Test set: Average loss: 1.4915, Average CER: 0.483366 Average WER: 1.0130\n","\n","Train Epoch: 38 [0/900 (0%)]\tLoss: 2.106539\n","Train Epoch: 38 [100/900 (11%)]\tLoss: 1.317045\n","Train Epoch: 38 [200/900 (22%)]\tLoss: 1.516330\n","Train Epoch: 38 [300/900 (33%)]\tLoss: 1.765861\n","Train Epoch: 38 [400/900 (44%)]\tLoss: 1.801247\n","Train Epoch: 38 [500/900 (56%)]\tLoss: 1.759953\n","Train Epoch: 38 [600/900 (67%)]\tLoss: 1.644308\n","Train Epoch: 38 [700/900 (78%)]\tLoss: 1.643094\n","Train Epoch: 38 [800/900 (89%)]\tLoss: 1.666919\n","\n","evaluating...\n","Test set: Average loss: 1.5315, Average CER: 0.503211 Average WER: 1.0000\n","\n","Train Epoch: 39 [0/900 (0%)]\tLoss: 1.688685\n","Train Epoch: 39 [100/900 (11%)]\tLoss: 1.753976\n","Train Epoch: 39 [200/900 (22%)]\tLoss: 1.579998\n","Train Epoch: 39 [300/900 (33%)]\tLoss: 1.417755\n","Train Epoch: 39 [400/900 (44%)]\tLoss: 1.467198\n","Train Epoch: 39 [500/900 (56%)]\tLoss: 1.521881\n","Train Epoch: 39 [600/900 (67%)]\tLoss: 1.503117\n","Train Epoch: 39 [700/900 (78%)]\tLoss: 1.997003\n","Train Epoch: 39 [800/900 (89%)]\tLoss: 1.471203\n","\n","evaluating...\n","Test set: Average loss: 1.3593, Average CER: 0.451164 Average WER: 0.9867\n","\n","Train Epoch: 40 [0/900 (0%)]\tLoss: 1.531756\n","Train Epoch: 40 [100/900 (11%)]\tLoss: 1.575238\n","Train Epoch: 40 [200/900 (22%)]\tLoss: 1.824357\n","Train Epoch: 40 [300/900 (33%)]\tLoss: 2.042999\n","Train Epoch: 40 [400/900 (44%)]\tLoss: 1.625505\n","Train Epoch: 40 [500/900 (56%)]\tLoss: 1.724047\n","Train Epoch: 40 [600/900 (67%)]\tLoss: 1.776281\n","Train Epoch: 40 [700/900 (78%)]\tLoss: 1.921481\n","Train Epoch: 40 [800/900 (89%)]\tLoss: 1.624811\n","\n","evaluating...\n","Test set: Average loss: 1.4135, Average CER: 0.476226 Average WER: 1.0043\n","\n","Train Epoch: 41 [0/900 (0%)]\tLoss: 1.633929\n","Train Epoch: 41 [100/900 (11%)]\tLoss: 1.710540\n","Train Epoch: 41 [200/900 (22%)]\tLoss: 1.554755\n","Train Epoch: 41 [300/900 (33%)]\tLoss: 1.687256\n","Train Epoch: 41 [400/900 (44%)]\tLoss: 1.759653\n","Train Epoch: 41 [500/900 (56%)]\tLoss: 1.570320\n","Train Epoch: 41 [600/900 (67%)]\tLoss: 1.551298\n","Train Epoch: 41 [700/900 (78%)]\tLoss: 1.688686\n","Train Epoch: 41 [800/900 (89%)]\tLoss: 1.370081\n","\n","evaluating...\n","Test set: Average loss: 1.3228, Average CER: 0.434645 Average WER: 0.9926\n","\n","Train Epoch: 42 [0/900 (0%)]\tLoss: 1.317203\n","Train Epoch: 42 [100/900 (11%)]\tLoss: 1.540430\n","Train Epoch: 42 [200/900 (22%)]\tLoss: 1.722732\n","Train Epoch: 42 [300/900 (33%)]\tLoss: 1.405532\n","Train Epoch: 42 [400/900 (44%)]\tLoss: 1.650701\n","Train Epoch: 42 [500/900 (56%)]\tLoss: 1.543269\n","Train Epoch: 42 [600/900 (67%)]\tLoss: 1.765677\n","Train Epoch: 42 [700/900 (78%)]\tLoss: 1.822602\n","Train Epoch: 42 [800/900 (89%)]\tLoss: 1.403384\n","\n","evaluating...\n","Test set: Average loss: 1.3206, Average CER: 0.452359 Average WER: 0.9988\n","\n","Train Epoch: 43 [0/900 (0%)]\tLoss: 1.732221\n","Train Epoch: 43 [100/900 (11%)]\tLoss: 1.272180\n","Train Epoch: 43 [200/900 (22%)]\tLoss: 1.502257\n","Train Epoch: 43 [300/900 (33%)]\tLoss: 1.729516\n","Train Epoch: 43 [400/900 (44%)]\tLoss: 1.414172\n","Train Epoch: 43 [500/900 (56%)]\tLoss: 1.547133\n","Train Epoch: 43 [600/900 (67%)]\tLoss: 1.297640\n","Train Epoch: 43 [700/900 (78%)]\tLoss: 1.627046\n","Train Epoch: 43 [800/900 (89%)]\tLoss: 1.948868\n","\n","evaluating...\n","Test set: Average loss: 1.3721, Average CER: 0.468541 Average WER: 1.0211\n","\n","Train Epoch: 44 [0/900 (0%)]\tLoss: 1.514993\n","Train Epoch: 44 [100/900 (11%)]\tLoss: 1.863253\n","Train Epoch: 44 [200/900 (22%)]\tLoss: 1.112131\n","Train Epoch: 44 [300/900 (33%)]\tLoss: 1.634426\n","Train Epoch: 44 [400/900 (44%)]\tLoss: 1.881111\n","Train Epoch: 44 [500/900 (56%)]\tLoss: 1.341874\n","Train Epoch: 44 [600/900 (67%)]\tLoss: 1.650381\n","Train Epoch: 44 [700/900 (78%)]\tLoss: 1.546451\n","Train Epoch: 44 [800/900 (89%)]\tLoss: 1.300454\n","\n","evaluating...\n","Test set: Average loss: 1.2585, Average CER: 0.421897 Average WER: 0.9806\n","\n","Train Epoch: 45 [0/900 (0%)]\tLoss: 1.107208\n","Train Epoch: 45 [100/900 (11%)]\tLoss: 1.489727\n","Train Epoch: 45 [200/900 (22%)]\tLoss: 1.597253\n","Train Epoch: 45 [300/900 (33%)]\tLoss: 1.486328\n","Train Epoch: 45 [400/900 (44%)]\tLoss: 1.161384\n","Train Epoch: 45 [500/900 (56%)]\tLoss: 1.566893\n","Train Epoch: 45 [600/900 (67%)]\tLoss: 1.460381\n","Train Epoch: 45 [700/900 (78%)]\tLoss: 1.678871\n","Train Epoch: 45 [800/900 (89%)]\tLoss: 1.405288\n","\n","evaluating...\n","Test set: Average loss: 1.2923, Average CER: 0.437674 Average WER: 0.9953\n","\n","Train Epoch: 46 [0/900 (0%)]\tLoss: 1.457556\n","Train Epoch: 46 [100/900 (11%)]\tLoss: 1.133775\n","Train Epoch: 46 [200/900 (22%)]\tLoss: 1.439649\n","Train Epoch: 46 [300/900 (33%)]\tLoss: 1.462561\n","Train Epoch: 46 [400/900 (44%)]\tLoss: 1.710590\n","Train Epoch: 46 [500/900 (56%)]\tLoss: 1.076447\n","Train Epoch: 46 [600/900 (67%)]\tLoss: 1.789453\n","Train Epoch: 46 [700/900 (78%)]\tLoss: 1.568888\n","Train Epoch: 46 [800/900 (89%)]\tLoss: 1.865966\n","\n","evaluating...\n","Test set: Average loss: 1.3738, Average CER: 0.480459 Average WER: 0.9981\n","\n","Train Epoch: 47 [0/900 (0%)]\tLoss: 1.228687\n","Train Epoch: 47 [100/900 (11%)]\tLoss: 1.307627\n","Train Epoch: 47 [200/900 (22%)]\tLoss: 1.482950\n","Train Epoch: 47 [300/900 (33%)]\tLoss: 1.400796\n","Train Epoch: 47 [400/900 (44%)]\tLoss: 1.347612\n","Train Epoch: 47 [500/900 (56%)]\tLoss: 1.598197\n","Train Epoch: 47 [600/900 (67%)]\tLoss: 1.867463\n","Train Epoch: 47 [700/900 (78%)]\tLoss: 1.331607\n","Train Epoch: 47 [800/900 (89%)]\tLoss: 1.433845\n","\n","evaluating...\n","Test set: Average loss: 1.3904, Average CER: 0.438890 Average WER: 0.9965\n","\n","Train Epoch: 48 [0/900 (0%)]\tLoss: 1.475392\n","Train Epoch: 48 [100/900 (11%)]\tLoss: 1.054130\n","Train Epoch: 48 [200/900 (22%)]\tLoss: 1.331267\n","Train Epoch: 48 [300/900 (33%)]\tLoss: 1.375809\n","Train Epoch: 48 [400/900 (44%)]\tLoss: 1.598564\n","Train Epoch: 48 [500/900 (56%)]\tLoss: 2.065625\n","Train Epoch: 48 [600/900 (67%)]\tLoss: 1.232936\n","Train Epoch: 48 [700/900 (78%)]\tLoss: 1.455502\n","Train Epoch: 48 [800/900 (89%)]\tLoss: 1.950575\n","\n","evaluating...\n","Test set: Average loss: 1.2615, Average CER: 0.416165 Average WER: 0.9792\n","\n","Train Epoch: 49 [0/900 (0%)]\tLoss: 1.284290\n","Train Epoch: 49 [100/900 (11%)]\tLoss: 1.326158\n","Train Epoch: 49 [200/900 (22%)]\tLoss: 1.482091\n","Train Epoch: 49 [300/900 (33%)]\tLoss: 1.218943\n","Train Epoch: 49 [400/900 (44%)]\tLoss: 1.402127\n","Train Epoch: 49 [500/900 (56%)]\tLoss: 1.397301\n","Train Epoch: 49 [600/900 (67%)]\tLoss: 1.320870\n","Train Epoch: 49 [700/900 (78%)]\tLoss: 1.747689\n","Train Epoch: 49 [800/900 (89%)]\tLoss: 1.222783\n","\n","evaluating...\n","Test set: Average loss: 1.1893, Average CER: 0.413037 Average WER: 0.9694\n","\n","Train Epoch: 50 [0/900 (0%)]\tLoss: 1.146069\n","Train Epoch: 50 [100/900 (11%)]\tLoss: 1.553010\n","Train Epoch: 50 [200/900 (22%)]\tLoss: 1.607005\n","Train Epoch: 50 [300/900 (33%)]\tLoss: 1.304571\n","Train Epoch: 50 [400/900 (44%)]\tLoss: 1.298211\n","Train Epoch: 50 [500/900 (56%)]\tLoss: 1.238926\n","Train Epoch: 50 [600/900 (67%)]\tLoss: 1.232546\n","Train Epoch: 50 [700/900 (78%)]\tLoss: 1.316372\n","Train Epoch: 50 [800/900 (89%)]\tLoss: 1.100294\n","\n","evaluating...\n","Test set: Average loss: 1.2635, Average CER: 0.432648 Average WER: 0.9916\n","\n","Train Epoch: 51 [0/900 (0%)]\tLoss: 1.484043\n","Train Epoch: 51 [100/900 (11%)]\tLoss: 1.213411\n","Train Epoch: 51 [200/900 (22%)]\tLoss: 1.193165\n","Train Epoch: 51 [300/900 (33%)]\tLoss: 1.470401\n","Train Epoch: 51 [400/900 (44%)]\tLoss: 1.271742\n","Train Epoch: 51 [500/900 (56%)]\tLoss: 1.080245\n","Train Epoch: 51 [600/900 (67%)]\tLoss: 1.188292\n","Train Epoch: 51 [700/900 (78%)]\tLoss: 1.690286\n","Train Epoch: 51 [800/900 (89%)]\tLoss: 1.268836\n","\n","evaluating...\n","Test set: Average loss: 1.2032, Average CER: 0.399772 Average WER: 1.0269\n","\n","Train Epoch: 52 [0/900 (0%)]\tLoss: 1.352790\n","Train Epoch: 52 [100/900 (11%)]\tLoss: 0.905117\n","Train Epoch: 52 [200/900 (22%)]\tLoss: 1.534792\n","Train Epoch: 52 [300/900 (33%)]\tLoss: 1.799215\n","Train Epoch: 52 [400/900 (44%)]\tLoss: 1.565438\n","Train Epoch: 52 [500/900 (56%)]\tLoss: 1.297954\n","Train Epoch: 52 [600/900 (67%)]\tLoss: 1.141541\n","Train Epoch: 52 [700/900 (78%)]\tLoss: 1.124598\n","Train Epoch: 52 [800/900 (89%)]\tLoss: 1.008712\n","\n","evaluating...\n","Test set: Average loss: 1.7258, Average CER: 0.470636 Average WER: 1.0562\n","\n","Train Epoch: 53 [0/900 (0%)]\tLoss: 1.155520\n","Train Epoch: 53 [100/900 (11%)]\tLoss: 1.110592\n","Train Epoch: 53 [200/900 (22%)]\tLoss: 1.380550\n","Train Epoch: 53 [300/900 (33%)]\tLoss: 1.757936\n","Train Epoch: 53 [400/900 (44%)]\tLoss: 1.301104\n","Train Epoch: 53 [500/900 (56%)]\tLoss: 0.972149\n","Train Epoch: 53 [600/900 (67%)]\tLoss: 1.673833\n","Train Epoch: 53 [700/900 (78%)]\tLoss: 1.519580\n","Train Epoch: 53 [800/900 (89%)]\tLoss: 1.226171\n","\n","evaluating...\n","Test set: Average loss: 1.1810, Average CER: 0.400625 Average WER: 0.9781\n","\n","Train Epoch: 54 [0/900 (0%)]\tLoss: 0.763562\n","Train Epoch: 54 [100/900 (11%)]\tLoss: 1.471031\n","Train Epoch: 54 [200/900 (22%)]\tLoss: 1.043593\n","Train Epoch: 54 [300/900 (33%)]\tLoss: 1.532991\n","Train Epoch: 54 [400/900 (44%)]\tLoss: 1.017541\n","Train Epoch: 54 [500/900 (56%)]\tLoss: 1.069088\n","Train Epoch: 54 [600/900 (67%)]\tLoss: 1.533902\n","Train Epoch: 54 [700/900 (78%)]\tLoss: 1.479963\n","Train Epoch: 54 [800/900 (89%)]\tLoss: 1.731120\n","\n","evaluating...\n","Test set: Average loss: 1.1783, Average CER: 0.401097 Average WER: 1.0039\n","\n","Train Epoch: 55 [0/900 (0%)]\tLoss: 1.112583\n","Train Epoch: 55 [100/900 (11%)]\tLoss: 1.342447\n","Train Epoch: 55 [200/900 (22%)]\tLoss: 1.115394\n","Train Epoch: 55 [300/900 (33%)]\tLoss: 1.049162\n","Train Epoch: 55 [400/900 (44%)]\tLoss: 1.240727\n","Train Epoch: 55 [500/900 (56%)]\tLoss: 0.977744\n","Train Epoch: 55 [600/900 (67%)]\tLoss: 0.908275\n","Train Epoch: 55 [700/900 (78%)]\tLoss: 1.119112\n","Train Epoch: 55 [800/900 (89%)]\tLoss: 1.548663\n","\n","evaluating...\n","Test set: Average loss: 1.2288, Average CER: 0.407477 Average WER: 0.9972\n","\n","Train Epoch: 56 [0/900 (0%)]\tLoss: 1.265797\n","Train Epoch: 56 [100/900 (11%)]\tLoss: 1.249886\n","Train Epoch: 56 [200/900 (22%)]\tLoss: 1.175366\n","Train Epoch: 56 [300/900 (33%)]\tLoss: 1.030118\n","Train Epoch: 56 [400/900 (44%)]\tLoss: 0.952911\n","Train Epoch: 56 [500/900 (56%)]\tLoss: 1.682769\n","Train Epoch: 56 [600/900 (67%)]\tLoss: 1.350101\n","Train Epoch: 56 [700/900 (78%)]\tLoss: 1.101409\n","Train Epoch: 56 [800/900 (89%)]\tLoss: 1.620572\n","\n","evaluating...\n","Test set: Average loss: 1.1921, Average CER: 0.410532 Average WER: 0.9720\n","\n","Train Epoch: 57 [0/900 (0%)]\tLoss: 1.265234\n","Train Epoch: 57 [100/900 (11%)]\tLoss: 1.271111\n","Train Epoch: 57 [200/900 (22%)]\tLoss: 1.104763\n","Train Epoch: 57 [300/900 (33%)]\tLoss: 0.954480\n","Train Epoch: 57 [400/900 (44%)]\tLoss: 1.189750\n","Train Epoch: 57 [500/900 (56%)]\tLoss: 1.389891\n","Train Epoch: 57 [600/900 (67%)]\tLoss: 1.188257\n","Train Epoch: 57 [700/900 (78%)]\tLoss: 1.285105\n","Train Epoch: 57 [800/900 (89%)]\tLoss: 1.513577\n","\n","evaluating...\n","Test set: Average loss: 1.2494, Average CER: 0.418624 Average WER: 1.0125\n","\n","Train Epoch: 58 [0/900 (0%)]\tLoss: 1.357547\n","Train Epoch: 58 [100/900 (11%)]\tLoss: 1.183542\n","Train Epoch: 58 [200/900 (22%)]\tLoss: 0.951040\n","Train Epoch: 58 [300/900 (33%)]\tLoss: 1.366977\n","Train Epoch: 58 [400/900 (44%)]\tLoss: 1.091257\n","Train Epoch: 58 [500/900 (56%)]\tLoss: 1.382639\n","Train Epoch: 58 [600/900 (67%)]\tLoss: 0.857783\n","Train Epoch: 58 [700/900 (78%)]\tLoss: 0.816988\n","Train Epoch: 58 [800/900 (89%)]\tLoss: 1.251335\n","\n","evaluating...\n","Test set: Average loss: 1.1886, Average CER: 0.390984 Average WER: 0.9894\n","\n","Train Epoch: 59 [0/900 (0%)]\tLoss: 1.348723\n","Train Epoch: 59 [100/900 (11%)]\tLoss: 1.371485\n","Train Epoch: 59 [200/900 (22%)]\tLoss: 1.319544\n","Train Epoch: 59 [300/900 (33%)]\tLoss: 0.952240\n","Train Epoch: 59 [400/900 (44%)]\tLoss: 0.897017\n","Train Epoch: 59 [500/900 (56%)]\tLoss: 1.101937\n","Train Epoch: 59 [600/900 (67%)]\tLoss: 0.941533\n","Train Epoch: 59 [700/900 (78%)]\tLoss: 1.106851\n","Train Epoch: 59 [800/900 (89%)]\tLoss: 1.101245\n","\n","evaluating...\n","Test set: Average loss: 1.3822, Average CER: 0.464460 Average WER: 1.0188\n","\n","Train Epoch: 60 [0/900 (0%)]\tLoss: 1.624721\n","Train Epoch: 60 [100/900 (11%)]\tLoss: 1.168710\n","Train Epoch: 60 [200/900 (22%)]\tLoss: 0.996585\n","Train Epoch: 60 [300/900 (33%)]\tLoss: 1.037576\n","Train Epoch: 60 [400/900 (44%)]\tLoss: 1.274178\n","Train Epoch: 60 [500/900 (56%)]\tLoss: 1.101150\n","Train Epoch: 60 [600/900 (67%)]\tLoss: 1.068669\n","Train Epoch: 60 [700/900 (78%)]\tLoss: 1.449985\n","Train Epoch: 60 [800/900 (89%)]\tLoss: 0.842761\n","\n","evaluating...\n","Test set: Average loss: 1.9987, Average CER: 0.537014 Average WER: 1.3685\n","\n","Train Epoch: 61 [0/900 (0%)]\tLoss: 1.227245\n","Train Epoch: 61 [100/900 (11%)]\tLoss: 1.054043\n","Train Epoch: 61 [200/900 (22%)]\tLoss: 1.204990\n","Train Epoch: 61 [300/900 (33%)]\tLoss: 1.080253\n","Train Epoch: 61 [400/900 (44%)]\tLoss: 1.576092\n","Train Epoch: 61 [500/900 (56%)]\tLoss: 0.892769\n","Train Epoch: 61 [600/900 (67%)]\tLoss: 1.378065\n","Train Epoch: 61 [700/900 (78%)]\tLoss: 1.333869\n","Train Epoch: 61 [800/900 (89%)]\tLoss: 1.550505\n","\n","evaluating...\n","Test set: Average loss: 1.2322, Average CER: 0.405067 Average WER: 1.0139\n","\n","Train Epoch: 62 [0/900 (0%)]\tLoss: 1.235753\n","Train Epoch: 62 [100/900 (11%)]\tLoss: 1.069341\n","Train Epoch: 62 [200/900 (22%)]\tLoss: 1.451875\n","Train Epoch: 62 [300/900 (33%)]\tLoss: 2.125185\n","Train Epoch: 62 [400/900 (44%)]\tLoss: 1.346889\n","Train Epoch: 62 [500/900 (56%)]\tLoss: 0.938645\n","Train Epoch: 62 [600/900 (67%)]\tLoss: 1.193987\n","Train Epoch: 62 [700/900 (78%)]\tLoss: 0.965129\n","Train Epoch: 62 [800/900 (89%)]\tLoss: 1.352288\n","\n","evaluating...\n","Test set: Average loss: 1.2340, Average CER: 0.402984 Average WER: 0.9750\n","\n","Train Epoch: 63 [0/900 (0%)]\tLoss: 1.083710\n","Train Epoch: 63 [100/900 (11%)]\tLoss: 1.159354\n","Train Epoch: 63 [200/900 (22%)]\tLoss: 1.124886\n","Train Epoch: 63 [300/900 (33%)]\tLoss: 1.134385\n","Train Epoch: 63 [400/900 (44%)]\tLoss: 0.936359\n","Train Epoch: 63 [500/900 (56%)]\tLoss: 1.208503\n","Train Epoch: 63 [600/900 (67%)]\tLoss: 1.605597\n","Train Epoch: 63 [700/900 (78%)]\tLoss: 1.179458\n","Train Epoch: 63 [800/900 (89%)]\tLoss: 1.443682\n","\n","evaluating...\n","Test set: Average loss: 1.3873, Average CER: 0.413364 Average WER: 1.0155\n","\n","Train Epoch: 64 [0/900 (0%)]\tLoss: 1.061034\n","Train Epoch: 64 [100/900 (11%)]\tLoss: 1.069804\n","Train Epoch: 64 [200/900 (22%)]\tLoss: 1.001573\n","Train Epoch: 64 [300/900 (33%)]\tLoss: 1.112454\n","Train Epoch: 64 [400/900 (44%)]\tLoss: 1.347985\n","Train Epoch: 64 [500/900 (56%)]\tLoss: 1.173460\n","Train Epoch: 64 [600/900 (67%)]\tLoss: 1.229845\n","Train Epoch: 64 [700/900 (78%)]\tLoss: 1.152527\n","Train Epoch: 64 [800/900 (89%)]\tLoss: 1.271095\n","\n","evaluating...\n","Test set: Average loss: 1.4555, Average CER: 0.453219 Average WER: 1.0936\n","\n","Train Epoch: 65 [0/900 (0%)]\tLoss: 1.477939\n","Train Epoch: 65 [100/900 (11%)]\tLoss: 1.126784\n","Train Epoch: 65 [200/900 (22%)]\tLoss: 1.065218\n","Train Epoch: 65 [300/900 (33%)]\tLoss: 0.939635\n","Train Epoch: 65 [400/900 (44%)]\tLoss: 1.223470\n","Train Epoch: 65 [500/900 (56%)]\tLoss: 0.970024\n","Train Epoch: 65 [600/900 (67%)]\tLoss: 0.994590\n","Train Epoch: 65 [700/900 (78%)]\tLoss: 2.165526\n","Train Epoch: 65 [800/900 (89%)]\tLoss: 0.671628\n","\n","evaluating...\n","Test set: Average loss: 1.2167, Average CER: 0.444031 Average WER: 0.9604\n","\n","Train Epoch: 66 [0/900 (0%)]\tLoss: 1.473999\n","Train Epoch: 66 [100/900 (11%)]\tLoss: 0.691069\n","Train Epoch: 66 [200/900 (22%)]\tLoss: 1.054640\n","Train Epoch: 66 [300/900 (33%)]\tLoss: 1.174435\n","Train Epoch: 66 [400/900 (44%)]\tLoss: 0.864154\n","Train Epoch: 66 [500/900 (56%)]\tLoss: 0.845725\n","Train Epoch: 66 [600/900 (67%)]\tLoss: 0.782875\n","Train Epoch: 66 [700/900 (78%)]\tLoss: 1.102994\n","Train Epoch: 66 [800/900 (89%)]\tLoss: 1.520479\n","\n","evaluating...\n","Test set: Average loss: 1.2805, Average CER: 0.424832 Average WER: 0.9903\n","\n","Train Epoch: 67 [0/900 (0%)]\tLoss: 0.924014\n","Train Epoch: 67 [100/900 (11%)]\tLoss: 0.970627\n","Train Epoch: 67 [200/900 (22%)]\tLoss: 1.104498\n","Train Epoch: 67 [300/900 (33%)]\tLoss: 1.063579\n","Train Epoch: 67 [400/900 (44%)]\tLoss: 1.343070\n","Train Epoch: 67 [500/900 (56%)]\tLoss: 1.106193\n","Train Epoch: 67 [600/900 (67%)]\tLoss: 1.518673\n","Train Epoch: 67 [700/900 (78%)]\tLoss: 1.022026\n","Train Epoch: 67 [800/900 (89%)]\tLoss: 0.814971\n","\n","evaluating...\n","Test set: Average loss: 1.2959, Average CER: 0.408802 Average WER: 0.9839\n","\n","Train Epoch: 68 [0/900 (0%)]\tLoss: 1.073084\n","Train Epoch: 68 [100/900 (11%)]\tLoss: 1.282732\n","Train Epoch: 68 [200/900 (22%)]\tLoss: 1.108121\n","Train Epoch: 68 [300/900 (33%)]\tLoss: 1.166482\n","Train Epoch: 68 [400/900 (44%)]\tLoss: 1.044248\n","Train Epoch: 68 [500/900 (56%)]\tLoss: 0.936904\n","Train Epoch: 68 [600/900 (67%)]\tLoss: 1.067589\n","Train Epoch: 68 [700/900 (78%)]\tLoss: 1.086594\n","Train Epoch: 68 [800/900 (89%)]\tLoss: 1.058046\n","\n","evaluating...\n","Test set: Average loss: 5.3496, Average CER: 0.997291 Average WER: 2.8398\n","\n","Train Epoch: 69 [0/900 (0%)]\tLoss: 1.747595\n","Train Epoch: 69 [100/900 (11%)]\tLoss: 1.317579\n","Train Epoch: 69 [200/900 (22%)]\tLoss: 0.894942\n","Train Epoch: 69 [300/900 (33%)]\tLoss: 0.936859\n","Train Epoch: 69 [400/900 (44%)]\tLoss: 1.300910\n","Train Epoch: 69 [500/900 (56%)]\tLoss: 1.368961\n","Train Epoch: 69 [600/900 (67%)]\tLoss: 1.218530\n","Train Epoch: 69 [700/900 (78%)]\tLoss: 1.184807\n","Train Epoch: 69 [800/900 (89%)]\tLoss: 0.849977\n","\n","evaluating...\n","Test set: Average loss: 2.4053, Average CER: 0.626513 Average WER: 1.1525\n","\n","Train Epoch: 70 [0/900 (0%)]\tLoss: 1.308257\n","Train Epoch: 70 [100/900 (11%)]\tLoss: 1.089220\n","Train Epoch: 70 [200/900 (22%)]\tLoss: 1.047602\n","Train Epoch: 70 [300/900 (33%)]\tLoss: 0.608076\n","Train Epoch: 70 [400/900 (44%)]\tLoss: 1.084463\n","Train Epoch: 70 [500/900 (56%)]\tLoss: 0.658263\n","Train Epoch: 70 [600/900 (67%)]\tLoss: 1.033349\n","Train Epoch: 70 [700/900 (78%)]\tLoss: 1.215292\n","Train Epoch: 70 [800/900 (89%)]\tLoss: 0.882183\n","\n","evaluating...\n","Test set: Average loss: 1.2421, Average CER: 0.399613 Average WER: 0.9779\n","\n","Train Epoch: 71 [0/900 (0%)]\tLoss: 0.966343\n","Train Epoch: 71 [100/900 (11%)]\tLoss: 1.323889\n","Train Epoch: 71 [200/900 (22%)]\tLoss: 0.921159\n","Train Epoch: 71 [300/900 (33%)]\tLoss: 0.877191\n","Train Epoch: 71 [400/900 (44%)]\tLoss: 1.131194\n","Train Epoch: 71 [500/900 (56%)]\tLoss: 1.427375\n","Train Epoch: 71 [600/900 (67%)]\tLoss: 0.747873\n","Train Epoch: 71 [700/900 (78%)]\tLoss: 1.445028\n","Train Epoch: 71 [800/900 (89%)]\tLoss: 0.919640\n","\n","evaluating...\n","Test set: Average loss: 3.9702, Average CER: 0.634441 Average WER: 1.4141\n","\n","Train Epoch: 72 [0/900 (0%)]\tLoss: 0.601131\n","Train Epoch: 72 [100/900 (11%)]\tLoss: 0.814670\n","Train Epoch: 72 [200/900 (22%)]\tLoss: 0.640469\n","Train Epoch: 72 [300/900 (33%)]\tLoss: 1.429281\n","Train Epoch: 72 [400/900 (44%)]\tLoss: 1.231449\n","Train Epoch: 72 [500/900 (56%)]\tLoss: 0.638213\n","Train Epoch: 72 [600/900 (67%)]\tLoss: 1.004336\n","Train Epoch: 72 [700/900 (78%)]\tLoss: 0.905483\n","Train Epoch: 72 [800/900 (89%)]\tLoss: 1.271759\n","\n","evaluating...\n","Test set: Average loss: 1.5640, Average CER: 0.479393 Average WER: 1.1830\n","\n","Train Epoch: 73 [0/900 (0%)]\tLoss: 0.853826\n","Train Epoch: 73 [100/900 (11%)]\tLoss: 0.892940\n","Train Epoch: 73 [200/900 (22%)]\tLoss: 0.811434\n","Train Epoch: 73 [300/900 (33%)]\tLoss: 1.051618\n","Train Epoch: 73 [400/900 (44%)]\tLoss: 1.062401\n","Train Epoch: 73 [500/900 (56%)]\tLoss: 0.723729\n","Train Epoch: 73 [600/900 (67%)]\tLoss: 0.935519\n","Train Epoch: 73 [700/900 (78%)]\tLoss: 0.726007\n","Train Epoch: 73 [800/900 (89%)]\tLoss: 0.854894\n","\n","evaluating...\n","Test set: Average loss: 1.2634, Average CER: 0.453904 Average WER: 0.9832\n","\n","Train Epoch: 74 [0/900 (0%)]\tLoss: 0.769540\n","Train Epoch: 74 [100/900 (11%)]\tLoss: 0.874025\n","Train Epoch: 74 [200/900 (22%)]\tLoss: 0.981487\n","Train Epoch: 74 [300/900 (33%)]\tLoss: 1.044871\n","Train Epoch: 74 [400/900 (44%)]\tLoss: 0.864935\n","Train Epoch: 74 [500/900 (56%)]\tLoss: 1.296017\n","Train Epoch: 74 [600/900 (67%)]\tLoss: 0.887107\n","Train Epoch: 74 [700/900 (78%)]\tLoss: 1.015552\n","Train Epoch: 74 [800/900 (89%)]\tLoss: 1.004553\n","\n","evaluating...\n","Test set: Average loss: 1.3110, Average CER: 0.409590 Average WER: 1.0575\n","\n","Train Epoch: 75 [0/900 (0%)]\tLoss: 0.860148\n","Train Epoch: 75 [100/900 (11%)]\tLoss: 0.937051\n","Train Epoch: 75 [200/900 (22%)]\tLoss: 1.238938\n","Train Epoch: 75 [300/900 (33%)]\tLoss: 0.781489\n","Train Epoch: 75 [400/900 (44%)]\tLoss: 1.052446\n","Train Epoch: 75 [500/900 (56%)]\tLoss: 0.975196\n","Train Epoch: 75 [600/900 (67%)]\tLoss: 0.998201\n","Train Epoch: 75 [700/900 (78%)]\tLoss: 1.306786\n","Train Epoch: 75 [800/900 (89%)]\tLoss: 1.344807\n","\n","evaluating...\n","Test set: Average loss: 1.4621, Average CER: 0.428101 Average WER: 1.0420\n","\n","Train Epoch: 76 [0/900 (0%)]\tLoss: 0.787812\n","Train Epoch: 76 [100/900 (11%)]\tLoss: 0.846726\n","Train Epoch: 76 [200/900 (22%)]\tLoss: 1.260051\n","Train Epoch: 76 [300/900 (33%)]\tLoss: 0.759485\n","Train Epoch: 76 [400/900 (44%)]\tLoss: 1.098270\n","Train Epoch: 76 [500/900 (56%)]\tLoss: 0.979449\n","Train Epoch: 76 [600/900 (67%)]\tLoss: 0.851974\n","Train Epoch: 76 [700/900 (78%)]\tLoss: 0.960798\n","Train Epoch: 76 [800/900 (89%)]\tLoss: 0.883852\n","\n","evaluating...\n","Test set: Average loss: 1.2256, Average CER: 0.415441 Average WER: 0.9908\n","\n","Train Epoch: 77 [0/900 (0%)]\tLoss: 1.327292\n","Train Epoch: 77 [100/900 (11%)]\tLoss: 0.785746\n","Train Epoch: 77 [200/900 (22%)]\tLoss: 1.277256\n","Train Epoch: 77 [300/900 (33%)]\tLoss: 0.941661\n","Train Epoch: 77 [400/900 (44%)]\tLoss: 1.048439\n","Train Epoch: 77 [500/900 (56%)]\tLoss: 0.872276\n","Train Epoch: 77 [600/900 (67%)]\tLoss: 1.168471\n","Train Epoch: 77 [700/900 (78%)]\tLoss: 1.093051\n","Train Epoch: 77 [800/900 (89%)]\tLoss: 0.923384\n","\n","evaluating...\n","Test set: Average loss: 1.2454, Average CER: 0.383399 Average WER: 0.9961\n","\n","Train Epoch: 78 [0/900 (0%)]\tLoss: 1.233149\n","Train Epoch: 78 [100/900 (11%)]\tLoss: 0.852931\n","Train Epoch: 78 [200/900 (22%)]\tLoss: 1.313658\n","Train Epoch: 78 [300/900 (33%)]\tLoss: 0.899297\n","Train Epoch: 78 [400/900 (44%)]\tLoss: 0.943677\n","Train Epoch: 78 [500/900 (56%)]\tLoss: 0.853710\n","Train Epoch: 78 [600/900 (67%)]\tLoss: 0.669194\n","Train Epoch: 78 [700/900 (78%)]\tLoss: 0.922533\n","Train Epoch: 78 [800/900 (89%)]\tLoss: 1.062617\n","\n","evaluating...\n","Test set: Average loss: 1.2997, Average CER: 0.420496 Average WER: 1.0219\n","\n","Train Epoch: 79 [0/900 (0%)]\tLoss: 0.938635\n","Train Epoch: 79 [100/900 (11%)]\tLoss: 1.263950\n","Train Epoch: 79 [200/900 (22%)]\tLoss: 0.701300\n","Train Epoch: 79 [300/900 (33%)]\tLoss: 1.069888\n","Train Epoch: 79 [400/900 (44%)]\tLoss: 0.826848\n","Train Epoch: 79 [500/900 (56%)]\tLoss: 0.954243\n","Train Epoch: 79 [600/900 (67%)]\tLoss: 0.879752\n","Train Epoch: 79 [700/900 (78%)]\tLoss: 1.329785\n","Train Epoch: 79 [800/900 (89%)]\tLoss: 0.831389\n","\n","evaluating...\n","Test set: Average loss: 1.2940, Average CER: 0.395561 Average WER: 1.0362\n","\n","Train Epoch: 80 [0/900 (0%)]\tLoss: 1.081804\n","Train Epoch: 80 [100/900 (11%)]\tLoss: 0.536700\n","Train Epoch: 80 [200/900 (22%)]\tLoss: 0.653555\n","Train Epoch: 80 [300/900 (33%)]\tLoss: 0.837178\n","Train Epoch: 80 [400/900 (44%)]\tLoss: 0.936910\n","Train Epoch: 80 [500/900 (56%)]\tLoss: 0.907897\n","Train Epoch: 80 [600/900 (67%)]\tLoss: 0.959795\n","Train Epoch: 80 [700/900 (78%)]\tLoss: 0.712738\n","Train Epoch: 80 [800/900 (89%)]\tLoss: 0.848739\n","\n","evaluating...\n","Test set: Average loss: 1.3165, Average CER: 0.402608 Average WER: 0.9938\n","\n","Train Epoch: 81 [0/900 (0%)]\tLoss: 1.570715\n","Train Epoch: 81 [100/900 (11%)]\tLoss: 0.571869\n","Train Epoch: 81 [200/900 (22%)]\tLoss: 1.040450\n","Train Epoch: 81 [300/900 (33%)]\tLoss: 0.999265\n","Train Epoch: 81 [400/900 (44%)]\tLoss: 0.557211\n","Train Epoch: 81 [500/900 (56%)]\tLoss: 0.790824\n","Train Epoch: 81 [600/900 (67%)]\tLoss: 1.370510\n","Train Epoch: 81 [700/900 (78%)]\tLoss: 0.757442\n","Train Epoch: 81 [800/900 (89%)]\tLoss: 0.751406\n","\n","evaluating...\n","Test set: Average loss: 1.6370, Average CER: 0.488638 Average WER: 1.1082\n","\n","Train Epoch: 82 [0/900 (0%)]\tLoss: 1.153054\n","Train Epoch: 82 [100/900 (11%)]\tLoss: 0.793503\n","Train Epoch: 82 [200/900 (22%)]\tLoss: 0.905900\n","Train Epoch: 82 [300/900 (33%)]\tLoss: 1.121682\n","Train Epoch: 82 [400/900 (44%)]\tLoss: 0.770862\n","Train Epoch: 82 [500/900 (56%)]\tLoss: 0.983387\n","Train Epoch: 82 [600/900 (67%)]\tLoss: 1.339888\n","Train Epoch: 82 [700/900 (78%)]\tLoss: 0.843518\n","Train Epoch: 82 [800/900 (89%)]\tLoss: 0.980535\n","\n","evaluating...\n","Test set: Average loss: 1.3823, Average CER: 0.412513 Average WER: 1.0163\n","\n","Train Epoch: 83 [0/900 (0%)]\tLoss: 0.614526\n","Train Epoch: 83 [100/900 (11%)]\tLoss: 0.842972\n","Train Epoch: 83 [200/900 (22%)]\tLoss: 0.799703\n","Train Epoch: 83 [300/900 (33%)]\tLoss: 0.705535\n","Train Epoch: 83 [400/900 (44%)]\tLoss: 1.203232\n","Train Epoch: 83 [500/900 (56%)]\tLoss: 1.122957\n","Train Epoch: 83 [600/900 (67%)]\tLoss: 0.768445\n","Train Epoch: 83 [700/900 (78%)]\tLoss: 0.811372\n","Train Epoch: 83 [800/900 (89%)]\tLoss: 0.772009\n","\n","evaluating...\n","Test set: Average loss: 1.3597, Average CER: 0.410203 Average WER: 1.0746\n","\n","Train Epoch: 84 [0/900 (0%)]\tLoss: 0.853496\n","Train Epoch: 84 [100/900 (11%)]\tLoss: 0.737002\n","Train Epoch: 84 [200/900 (22%)]\tLoss: 1.346192\n","Train Epoch: 84 [300/900 (33%)]\tLoss: 0.766521\n","Train Epoch: 84 [400/900 (44%)]\tLoss: 0.894798\n","Train Epoch: 84 [500/900 (56%)]\tLoss: 0.535962\n","Train Epoch: 84 [600/900 (67%)]\tLoss: 0.555185\n","Train Epoch: 84 [700/900 (78%)]\tLoss: 1.024196\n","Train Epoch: 84 [800/900 (89%)]\tLoss: 0.609094\n","\n","evaluating...\n","Test set: Average loss: 1.2982, Average CER: 0.402796 Average WER: 1.0279\n","\n","Train Epoch: 85 [0/900 (0%)]\tLoss: 0.825907\n","Train Epoch: 85 [100/900 (11%)]\tLoss: 0.726969\n","Train Epoch: 85 [200/900 (22%)]\tLoss: 0.678564\n","Train Epoch: 85 [300/900 (33%)]\tLoss: 0.978951\n","Train Epoch: 85 [400/900 (44%)]\tLoss: 0.988832\n","Train Epoch: 85 [500/900 (56%)]\tLoss: 0.911962\n","Train Epoch: 85 [600/900 (67%)]\tLoss: 0.851835\n","Train Epoch: 85 [700/900 (78%)]\tLoss: 1.109952\n","Train Epoch: 85 [800/900 (89%)]\tLoss: 1.069236\n","\n","evaluating...\n","Test set: Average loss: 1.3257, Average CER: 0.405531 Average WER: 1.0444\n","\n","Train Epoch: 86 [0/900 (0%)]\tLoss: 0.708472\n","Train Epoch: 86 [100/900 (11%)]\tLoss: 0.893804\n","Train Epoch: 86 [200/900 (22%)]\tLoss: 0.531663\n","Train Epoch: 86 [300/900 (33%)]\tLoss: 1.005206\n","Train Epoch: 86 [400/900 (44%)]\tLoss: 0.914171\n","Train Epoch: 86 [500/900 (56%)]\tLoss: 1.248437\n","Train Epoch: 86 [600/900 (67%)]\tLoss: 0.811403\n","Train Epoch: 86 [700/900 (78%)]\tLoss: 0.583125\n","Train Epoch: 86 [800/900 (89%)]\tLoss: 0.744491\n","\n","evaluating...\n","Test set: Average loss: 1.4134, Average CER: 0.413374 Average WER: 1.0070\n","\n","Train Epoch: 87 [0/900 (0%)]\tLoss: 1.475010\n","Train Epoch: 87 [100/900 (11%)]\tLoss: 0.564661\n","Train Epoch: 87 [200/900 (22%)]\tLoss: 0.786857\n","Train Epoch: 87 [300/900 (33%)]\tLoss: 0.761147\n","Train Epoch: 87 [400/900 (44%)]\tLoss: 0.808252\n","Train Epoch: 87 [500/900 (56%)]\tLoss: 0.892402\n","Train Epoch: 87 [600/900 (67%)]\tLoss: 0.816011\n","Train Epoch: 87 [700/900 (78%)]\tLoss: 0.947820\n","Train Epoch: 87 [800/900 (89%)]\tLoss: 0.745488\n","\n","evaluating...\n","Test set: Average loss: 2.0399, Average CER: 0.553405 Average WER: 1.1704\n","\n","Train Epoch: 88 [0/900 (0%)]\tLoss: 1.044998\n","Train Epoch: 88 [100/900 (11%)]\tLoss: 0.770936\n","Train Epoch: 88 [200/900 (22%)]\tLoss: 1.034652\n","Train Epoch: 88 [300/900 (33%)]\tLoss: 0.793143\n","Train Epoch: 88 [400/900 (44%)]\tLoss: 0.588065\n","Train Epoch: 88 [500/900 (56%)]\tLoss: 1.025825\n","Train Epoch: 88 [600/900 (67%)]\tLoss: 0.909695\n","Train Epoch: 88 [700/900 (78%)]\tLoss: 0.741560\n","Train Epoch: 88 [800/900 (89%)]\tLoss: 1.155248\n","\n","evaluating...\n","Test set: Average loss: 1.3501, Average CER: 0.395062 Average WER: 1.0311\n","\n","Train Epoch: 89 [0/900 (0%)]\tLoss: 1.059784\n","Train Epoch: 89 [100/900 (11%)]\tLoss: 1.059235\n","Train Epoch: 89 [200/900 (22%)]\tLoss: 0.753352\n","Train Epoch: 89 [300/900 (33%)]\tLoss: 0.725590\n","Train Epoch: 89 [400/900 (44%)]\tLoss: 0.752449\n","Train Epoch: 89 [500/900 (56%)]\tLoss: 0.663540\n","Train Epoch: 89 [600/900 (67%)]\tLoss: 1.349541\n","Train Epoch: 89 [700/900 (78%)]\tLoss: 0.730145\n","Train Epoch: 89 [800/900 (89%)]\tLoss: 0.836052\n","\n","evaluating...\n","Test set: Average loss: 1.3164, Average CER: 0.393186 Average WER: 1.0269\n","\n","Train Epoch: 90 [0/900 (0%)]\tLoss: 0.942578\n","Train Epoch: 90 [100/900 (11%)]\tLoss: 0.930072\n","Train Epoch: 90 [200/900 (22%)]\tLoss: 0.712065\n","Train Epoch: 90 [300/900 (33%)]\tLoss: 1.120054\n","Train Epoch: 90 [400/900 (44%)]\tLoss: 0.721006\n","Train Epoch: 90 [500/900 (56%)]\tLoss: 0.973104\n","Train Epoch: 90 [600/900 (67%)]\tLoss: 0.982894\n","Train Epoch: 90 [700/900 (78%)]\tLoss: 0.708680\n","Train Epoch: 90 [800/900 (89%)]\tLoss: 0.528039\n","\n","evaluating...\n","Test set: Average loss: 1.2619, Average CER: 0.379942 Average WER: 0.9730\n","\n","Train Epoch: 91 [0/900 (0%)]\tLoss: 0.456524\n","Train Epoch: 91 [100/900 (11%)]\tLoss: 0.880009\n","Train Epoch: 91 [200/900 (22%)]\tLoss: 0.707780\n","Train Epoch: 91 [300/900 (33%)]\tLoss: 0.717909\n","Train Epoch: 91 [400/900 (44%)]\tLoss: 0.885920\n","Train Epoch: 91 [500/900 (56%)]\tLoss: 0.612270\n","Train Epoch: 91 [600/900 (67%)]\tLoss: 0.606835\n","Train Epoch: 91 [700/900 (78%)]\tLoss: 1.045942\n","Train Epoch: 91 [800/900 (89%)]\tLoss: 0.976008\n","\n","evaluating...\n","Test set: Average loss: 1.3510, Average CER: 0.393884 Average WER: 1.0219\n","\n","Train Epoch: 92 [0/900 (0%)]\tLoss: 0.895954\n","Train Epoch: 92 [100/900 (11%)]\tLoss: 0.906852\n","Train Epoch: 92 [200/900 (22%)]\tLoss: 0.567153\n","Train Epoch: 92 [300/900 (33%)]\tLoss: 0.692880\n","Train Epoch: 92 [400/900 (44%)]\tLoss: 0.643882\n","Train Epoch: 92 [500/900 (56%)]\tLoss: 0.904253\n","Train Epoch: 92 [600/900 (67%)]\tLoss: 0.770847\n","Train Epoch: 92 [700/900 (78%)]\tLoss: 0.835325\n","Train Epoch: 92 [800/900 (89%)]\tLoss: 0.712172\n","\n","evaluating...\n","Test set: Average loss: 2.1141, Average CER: 0.554449 Average WER: 1.0754\n","\n","Train Epoch: 93 [0/900 (0%)]\tLoss: 0.926638\n","Train Epoch: 93 [100/900 (11%)]\tLoss: 0.880600\n","Train Epoch: 93 [200/900 (22%)]\tLoss: 1.152972\n","Train Epoch: 93 [300/900 (33%)]\tLoss: 0.804346\n","Train Epoch: 93 [400/900 (44%)]\tLoss: 0.628073\n","Train Epoch: 93 [500/900 (56%)]\tLoss: 1.010371\n","Train Epoch: 93 [600/900 (67%)]\tLoss: 0.817418\n","Train Epoch: 93 [700/900 (78%)]\tLoss: 0.959035\n","Train Epoch: 93 [800/900 (89%)]\tLoss: 0.620958\n","\n","evaluating...\n","Test set: Average loss: 1.4197, Average CER: 0.418814 Average WER: 1.0116\n","\n","Train Epoch: 94 [0/900 (0%)]\tLoss: 1.059978\n","Train Epoch: 94 [100/900 (11%)]\tLoss: 0.872992\n","Train Epoch: 94 [200/900 (22%)]\tLoss: 0.691430\n","Train Epoch: 94 [300/900 (33%)]\tLoss: 0.892771\n","Train Epoch: 94 [400/900 (44%)]\tLoss: 0.575934\n","Train Epoch: 94 [500/900 (56%)]\tLoss: 0.785601\n","Train Epoch: 94 [600/900 (67%)]\tLoss: 0.556567\n","Train Epoch: 94 [700/900 (78%)]\tLoss: 0.669540\n","Train Epoch: 94 [800/900 (89%)]\tLoss: 0.507939\n","\n","evaluating...\n","Test set: Average loss: 1.3611, Average CER: 0.383793 Average WER: 0.9845\n","\n","Train Epoch: 95 [0/900 (0%)]\tLoss: 0.721959\n","Train Epoch: 95 [100/900 (11%)]\tLoss: 0.487509\n","Train Epoch: 95 [200/900 (22%)]\tLoss: 0.760898\n","Train Epoch: 95 [300/900 (33%)]\tLoss: 0.910528\n","Train Epoch: 95 [400/900 (44%)]\tLoss: 0.755159\n","Train Epoch: 95 [500/900 (56%)]\tLoss: 0.387654\n","Train Epoch: 95 [600/900 (67%)]\tLoss: 0.961613\n","Train Epoch: 95 [700/900 (78%)]\tLoss: 1.173818\n","Train Epoch: 95 [800/900 (89%)]\tLoss: 1.012960\n","\n","evaluating...\n","Test set: Average loss: 1.5582, Average CER: 0.481986 Average WER: 1.1001\n","\n","Train Epoch: 96 [0/900 (0%)]\tLoss: 0.632735\n","Train Epoch: 96 [100/900 (11%)]\tLoss: 0.897097\n","Train Epoch: 96 [200/900 (22%)]\tLoss: 0.886500\n","Train Epoch: 96 [300/900 (33%)]\tLoss: 0.673228\n","Train Epoch: 96 [400/900 (44%)]\tLoss: 1.250131\n","Train Epoch: 96 [500/900 (56%)]\tLoss: 0.800995\n","Train Epoch: 96 [600/900 (67%)]\tLoss: 0.636494\n","Train Epoch: 96 [700/900 (78%)]\tLoss: 0.619771\n","Train Epoch: 96 [800/900 (89%)]\tLoss: 0.792947\n","\n","evaluating...\n","Test set: Average loss: 1.3445, Average CER: 0.385356 Average WER: 1.0017\n","\n","Train Epoch: 97 [0/900 (0%)]\tLoss: 0.591543\n","Train Epoch: 97 [100/900 (11%)]\tLoss: 0.865737\n","Train Epoch: 97 [200/900 (22%)]\tLoss: 0.485373\n","Train Epoch: 97 [300/900 (33%)]\tLoss: 0.866054\n","Train Epoch: 97 [400/900 (44%)]\tLoss: 1.143053\n","Train Epoch: 97 [500/900 (56%)]\tLoss: 0.683634\n","Train Epoch: 97 [600/900 (67%)]\tLoss: 1.087814\n","Train Epoch: 97 [700/900 (78%)]\tLoss: 0.839989\n","Train Epoch: 97 [800/900 (89%)]\tLoss: 0.699910\n","\n","evaluating...\n","Test set: Average loss: 1.3532, Average CER: 0.436820 Average WER: 0.9845\n","\n","Train Epoch: 98 [0/900 (0%)]\tLoss: 0.773053\n","Train Epoch: 98 [100/900 (11%)]\tLoss: 0.821437\n","Train Epoch: 98 [200/900 (22%)]\tLoss: 0.969368\n","Train Epoch: 98 [300/900 (33%)]\tLoss: 0.580391\n","Train Epoch: 98 [400/900 (44%)]\tLoss: 0.815431\n","Train Epoch: 98 [500/900 (56%)]\tLoss: 0.854268\n","Train Epoch: 98 [600/900 (67%)]\tLoss: 0.799509\n","Train Epoch: 98 [700/900 (78%)]\tLoss: 1.101131\n","Train Epoch: 98 [800/900 (89%)]\tLoss: 0.996903\n","\n","evaluating...\n","Test set: Average loss: 1.3552, Average CER: 0.382118 Average WER: 0.9930\n","\n","Train Epoch: 99 [0/900 (0%)]\tLoss: 1.126696\n","Train Epoch: 99 [100/900 (11%)]\tLoss: 0.606105\n","Train Epoch: 99 [200/900 (22%)]\tLoss: 1.068051\n","Train Epoch: 99 [300/900 (33%)]\tLoss: 0.856731\n","Train Epoch: 99 [400/900 (44%)]\tLoss: 0.710282\n","Train Epoch: 99 [500/900 (56%)]\tLoss: 0.822399\n","Train Epoch: 99 [600/900 (67%)]\tLoss: 1.605337\n","Train Epoch: 99 [700/900 (78%)]\tLoss: 0.692865\n","Train Epoch: 99 [800/900 (89%)]\tLoss: 0.809825\n","\n","evaluating...\n","Test set: Average loss: 1.4547, Average CER: 0.416664 Average WER: 1.0639\n","\n","Train Epoch: 100 [0/900 (0%)]\tLoss: 0.850309\n","Train Epoch: 100 [100/900 (11%)]\tLoss: 0.643456\n","Train Epoch: 100 [200/900 (22%)]\tLoss: 0.735172\n"]}],"source":["learning_rate = 0.001\n","batch_size = 10\n","epochs = 250\n","\n","main(learning_rate, batch_size, epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":5}
